# =============================================================================
# LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
# =============================================================================
# ì´ íŒŒì¼ì€ ê³µê³µë„ì„œê´€ ì´ìš©ì ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” 
# Streamlit ê¸°ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
# 
# ì£¼ìš” ê¸°ëŠ¥:
# - ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„
# - ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ ë¶„ì„  
# - ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„
# - ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„
# - ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„
# - ê³µí†µ ì‹¬í™” ë¶„ì„
# - AI ê¸°ë°˜ ì „ëµ ì¸ì‚¬ì´íŠ¸ ì œê³µ
# =============================================================================

# =============================================================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì„í¬íŠ¸
# =============================================================================
import time
import numpy as np
import streamlit as st
import scipy.stats as stats
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import re
import io
import os
from PIL import Image
import openai
import streamlit.components.v1 as components 
import hashlib
import jsonschema
import json 
import math
import logging
from itertools import cycle



# =============================================================================
# 2. ê¸°ë³¸ ì„¤ì • ë° ì´ˆê¸°í™”
# =============================================================================

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# OpenAI API ì„¤ì •
try:
    openai.api_key = st.secrets["openai"]["api_key"]
    client = openai.OpenAI(api_key=openai.api_key)
    OPENAI_AVAILABLE = True
except KeyError:
    st.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    OPENAI_AVAILABLE = False
    client = None

# ê¸°ë³¸ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
DEFAULT_PALETTE = px.colors.qualitative.Plotly
COLOR_CYCLER = cycle(DEFAULT_PALETTE)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def safe_chat_completion(*, model="gpt-4.1-nano", messages, temperature=0.2, max_tokens=300, retries=3, backoff_base=1.0):
    last_exc = None
    for attempt in range(1, retries + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return resp
        except Exception as e:
            logging.warning(f"OpenAI call failed (attempt {attempt}): {e}")
            last_exc = e
            time.sleep(backoff_base * (2 ** (attempt - 1)))
    logging.error("OpenAI call failed after retries")
    raise last_exc

def interpret_midcategory_scores(df):
    scores = compute_midcategory_scores(df)
    if scores.empty:
        return "ì¤‘ë¶„ë¥˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    overall = scores.mean()
    high = scores[scores >= overall + 5].index.tolist()
    low = scores[scores <= overall - 5].index.tolist()

    parts = []
    parts.append(f"ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê· ì€ {overall:.1f}ì ì…ë‹ˆë‹¤.")
    if high:
        parts.append(f"í‰ê· ë³´ë‹¤ ë†’ì€ ì¤‘ë¶„ë¥˜: {', '.join(high)}.")
    if low:
        parts.append(f"í‰ê· ë³´ë‹¤ ë‚®ì€ ì¤‘ë¶„ë¥˜: {', '.join(low)}.")
    if not high and not low:
        parts.append("ëª¨ë“  ì¤‘ë¶„ë¥˜ê°€ ì „ì²´ í‰ê·  ìˆ˜ì¤€ê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.")
    return " ".join(parts)

import re

def extract_question_code(col_name: str) -> str:
    """
    ì»¬ëŸ¼ëª…ì—ì„œ ë¬¸í•­ ì½”ë“œ/ë²ˆí˜¸ë§Œ ë½‘ì•„ëƒ„.
    ì˜ˆ: 'Q1-1. ê³µê°„ ë§Œì¡±ë„' -> 'Q1-1', 'SQ2 GROUP' -> 'SQ2', 'DQ4 1ìˆœìœ„' -> 'DQ4'
    """
    # ìš°ì„  ëŒ€ë¬¸ì+ìˆ«ì+ì–¸ë”/í•˜ì´í”ˆ ì¡°í•©ì„ ì¡ì•„ë³´ì (ì˜ˆ: Q1-1, DQ4, SQ2_GROUP)
    m = re.match(r'^([A-Za-z0-9]+(?:[-_][A-Za-z0-9]+)*)', col_name.strip())
    if m:
        return m.group(1)
    # fallback: ë§ˆì¹¨í‘œ ì „
    if '.' in col_name:
        return col_name.split('.', 1)[0].strip()
    if ' ' in col_name:
        return col_name.split(' ', 1)[0].strip()
    return col_name.strip()

def expand_midcategory_to_columns(midcategory: str, df: pd.DataFrame):
    """
    ì¤‘ë¶„ë¥˜ ì´ë¦„(ì˜ˆ: 'ê³µê°„ ë° ì´ìš©í¸ì˜ì„±')ì„ ë°›ì•„ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œ ì»¬ëŸ¼ëª… ëª©ë¡ìœ¼ë¡œ í™•ì¥.
    """
    for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
        if midcategory.strip().lower() == mid.strip().lower():
            return [c for c in df.columns if predicate(c)]
    return []

def get_questions_used(spec: dict, df_full: pd.DataFrame, df_subset: pd.DataFrame):
    """
    ìì—°ì–´ ì§ˆì˜ ìŠ¤í™ì„ ë³´ê³  ì‹¤ì œ ì°¸ê³ ëœ ë¬¸í•­ ì „ì²´ ì´ë¦„ë“¤ê³¼ ë¬¸í•­ ì½”ë“œ(ë²ˆí˜¸)ë§Œ ì¶”ì¶œí•œë‹¤.
    ë°˜í™˜: (used_full_list, used_codes_list) â€” ìˆœì„œ ë³´ì¡´, ì¤‘ë³µ ì œê±°
    """
    used_full = []

    # x, y, groupby
    for key in ("x", "y", "groupby"):
        val = spec.get(key)
        if not val:
            continue
        if isinstance(val, list):
            for v in val:
                if v in df_subset.columns:
                    used_full.append(v)
                else:
                    expanded = expand_midcategory_to_columns(v, df_subset)
                    if expanded:
                        used_full.extend(expanded)
                    else:
                        used_full.append(v)
        else:
            if val in df_subset.columns:
                used_full.append(val)
            else:
                expanded = expand_midcategory_to_columns(val, df_subset)
                if expanded:
                    used_full.extend(expanded)
                else:
                    used_full.append(val)

    # filters: ì»¬ëŸ¼ëª… í¬í•¨
    for f in spec.get("filters", []):
        col = f.get("col")
        if col:
            used_full.append(col)

    # focusì— 'ì¤‘ë¶„ë¥˜', 'ê°•ì ', 'ì•½ì ', 'ì „ì²´ í‰ê· ' ë“±ì´ ë“¤ì–´ê°€ë©´ ì¤‘ë¶„ë¥˜ ê´€ë ¨ ëª¨ë“  ë¬¸í•­ í¬í•¨
    focus = spec.get("focus", "").lower()
    if any(k in focus for k in ["ì¤‘ë¶„ë¥˜", "ê°•ì ", "ì•½ì ", "ì „ì²´ í‰ê· "]):
        for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
            cols = [c for c in df_subset.columns if predicate(c)]
            used_full.extend(cols)

    # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
    seen = set()
    used_full_unique = []
    for c in used_full:
        if c and c not in seen:
            seen.add(c)
            used_full_unique.append(c)

    # ì½”ë“œë§Œ ë½‘ê³  ì¤‘ë³µ ì œê±°
    used_codes = []
    seen_codes = set()
    for col in used_full_unique:
        code = extract_question_code(col)
        if code not in seen_codes:
            seen_codes.add(code)
            used_codes.append(code)

    return used_full_unique, used_codes


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì „ì²˜ë¦¬/ë§¤í•‘ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def remove_parentheses(text):
    return re.sub(r'\(.*?\)', '', text).strip()

def wrap_label(label, width=10):
    return '<br>'.join([label[i:i+width] for i in range(0, len(label), width)])

def get_qualitative_colors(n):
    palette = DEFAULT_PALETTE
    return [c for _, c in zip(range(n), cycle(palette))]

def wrap_label_fixed(label: str, width: int = 35) -> str:
    # í•œ ì¤„ì— ê³µë°± í¬í•¨ ì •í™•íˆ width ê¸€ìì”© ìë¥´ê³  <br>ë¡œ ì—°ê²°
    parts = [label[i:i+width] for i in range(0, len(label), width)]
    return "<br>".join(parts)

# ğŸ”§ KDC ë§¤í•‘ ë° ë¶„ì„ ìœ í‹¸
KDC_KEYWORD_MAP = {
    '000 ì´ë¥˜': ["ë°±ê³¼ì‚¬ì „", "ë„ì„œê´€", "ë…ì„œ", "ë¬¸í—Œì •ë³´", "ê¸°ë¡", "ì¶œíŒ", "ì„œì§€"],
    '100 ì² í•™': ["ì² í•™", "ëª…ìƒ", "ìœ¤ë¦¬", "ë…¼ë¦¬í•™", "ì‹¬ë¦¬í•™"],
    '200 ì¢…êµ': ["ì¢…êµ", "ê¸°ë…êµ", "ë¶ˆêµ", "ì²œì£¼êµ", "ì‹ í™”", "ì‹ ì•™", "ì¢…êµí•™"],
    '300 ì‚¬íšŒê³¼í•™': ["ì‚¬íšŒ", "ì •ì¹˜", "ê²½ì œ", "ë²•ë¥ ", "í–‰ì •", "êµìœ¡", "ë³µì§€", "ì—¬ì„±", "ë…¸ì¸", "ìœ¡ì•„", "ì•„ë™ë³µì§€", "ì‚¬íšŒë¬¸ì œ", "ë…¸ë™", "í™˜ê²½ë¬¸ì œ", "ì¸ê¶Œ"],
    '400 ìì—°ê³¼í•™': ["ìˆ˜í•™", "ë¬¼ë¦¬", "í™”í•™", "ìƒë¬¼", "ì§€êµ¬ê³¼í•™", "ê³¼í•™", "ì²œë¬¸", "ê¸°í›„", "ì˜í•™", "ìƒëª…ê³¼í•™"],
    '500 ê¸°ìˆ ê³¼í•™': ["ê±´ê°•", "ì˜ë£Œ", "ìš”ë¦¬", "ê°„í˜¸", "ê³µí•™", "ì»´í“¨í„°", "AI", "IT", "ë†ì—…", "ì¶•ì‚°", "ì‚°ì—…", "ê¸°ìˆ ", "ë¯¸ìš©"],
    '600 ì˜ˆìˆ ': ["ë¯¸ìˆ ", "ìŒì•…", "ë¬´ìš©", "ì‚¬ì§„", "ì˜í™”", "ì—°ê·¹", "ë””ìì¸", "ê³µì˜ˆ", "ì˜ˆìˆ ", "ë¬¸í™”ì˜ˆìˆ "],
    '700 ì–¸ì–´': ["ì–¸ì–´", "êµ­ì–´", "ì˜ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´", "ì™¸êµ­ì–´", "í•œì", "ë¬¸ë²•"],
    '800 ë¬¸í•™': ["ì†Œì„¤", "ì‹œ", "ìˆ˜í•„", "ì—ì„¸ì´", "í¬ê³¡", "ë¬¸í•™", "ë™í™”", "ì›¹íˆ°", "íŒíƒ€ì§€", "ë¬¸ì˜ˆ"],
    '900 ì—­ì‚¬Â·ì§€ë¦¬': ["ì—­ì‚¬", "ì§€ë¦¬", "í•œêµ­ì‚¬", "ì„¸ê³„ì‚¬", "ì—¬í–‰", "ë¬¸í™”ìœ ì‚°", "ê´€ê´‘"],
    'ì›ì„œ(ì˜ì–´)': ["ì›ì„œ", "ì˜ë¬¸ë„ì„œ", "ì˜ë¬¸íŒ", "ì˜ì–´ì›ì„œ"],
    'ì—°ì†ê°„í–‰ë¬¼': ["ì¡ì§€", "ê°„í–‰ë¬¼", "ì—°ì†ê°„í–‰ë¬¼"],
    'í•´ë‹¹ì—†ìŒ': []
}

def is_trivial(text):
    text = str(text).strip()
    return text in ["", "X", "x", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬", "ì—†ìŒ"]

def split_keywords_simple(text):
    parts = re.split(r"[.,/\s]+", text)
    return [p.strip() for p in parts if len(p.strip()) > 1]

def map_keyword_to_category(keyword):
    for cat, kws in KDC_KEYWORD_MAP.items():
        if any(k in keyword for k in kws):
            return cat
    return "í•´ë‹¹ì—†ìŒ"

def escape_tildes(text: str, mode: str = "html") -> str:
    """
    mode="html": ì¹´ë“œì²˜ëŸ¼ HTMLë¡œ ë Œë”ë§í•  ë•Œ ë¬¼ê²°í‘œ ì²˜ë¦¬.
    mode="markdown": st.markdown ê°™ì€ ë§ˆí¬ë‹¤ìš´ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì·¨ì†Œì„  ë°©ì§€.
    """
    if mode == "html":
        # '~~' ë¨¼ì € ë°”ê¾¸ê³  ë‹¨ì¼ ~ë„ ì—”í‹°í‹°ë¡œ
        text = text.replace("~~", "&#126;&#126;")
        return text.replace("~", "&#126;")
    else:  # markdown
        text = text.replace("~~", r"\~\~")
        return text.replace("~", r"\~")

def safe_markdown(text, **kwargs):
    # ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë°©ì§€ë¥¼ ìœ„í•´ ~ë¥¼ ì´ìŠ¤ì¼€ì´í”„
    safe = escape_tildes(text, mode="markdown")
    st.markdown(safe, **kwargs)

def build_common_overall_insight_prompt(midcat_scores: dict, within_deviations: dict, abc_df: pd.DataFrame) -> str:
    midcat_str = ", ".join(f"{k} {v:.1f}" for k, v in midcat_scores.items())
    strengths = []
    weaknesses = []
    for mid, series in within_deviations.items():
        if series is None:
            continue
        # ë§Œì•½ ì‹œë¦¬ì¦ˆë©´, ì •ë ¬í•´ì„œ ìƒ/í•˜ìœ„ í•­ëª© ë½‘ê¸°
        try:
            dev = series.dropna()
        except Exception:
            continue
        if dev.empty:
            continue
        sorted_series = dev.sort_values(ascending=False)
        top_pos_label = sorted_series.index[0]
        top_pos_val = sorted_series.iloc[0]
        top_neg_label = sorted_series.index[-1]
        top_neg_val = sorted_series.iloc[-1]
        strengths.append(f"{mid}ì˜ '{top_pos_label}' +{top_pos_val:.1f}")
        weaknesses.append(f"{mid}ì˜ '{top_neg_label}' {top_neg_val:.1f}")
    # ABC ë¹„êµ ìš”ì•½ ë¬¸ìì—´
    abc_lines = []
    for _, row in abc_df.iterrows():
        mid = row.get("ì¤‘ë¶„ë¥˜", "")
        eval_val = row.get("í‰ê· ê°’") if row.get("ë¬¸í•­ìœ í˜•") == "ì„œë¹„ìŠ¤ í‰ê°€" else None
        # ì‚¬ì‹¤ pivotí•œ ê°’ì´ ë” ì •í™•í•˜ë‹ˆ ì•ˆì „í•˜ê²Œ pivot ë°©ì‹ìœ¼ë¡œë„ ë½‘ì„ ìˆ˜ ìˆê²Œ
    # ë³´ë‹¤ ëª…í™•í•˜ê²Œ A/B/Cë¥¼ ì •ë¦¬
    try:
        abc_pivot = abc_df.pivot(index="ì¤‘ë¶„ë¥˜", columns="ë¬¸í•­ìœ í˜•", values="í‰ê· ê°’")
    except Exception:
        abc_pivot = pd.DataFrame()
    abc_lines = []
    if not abc_pivot.empty:
        for mid in abc_pivot.index:
            eval_val = abc_pivot.loc[mid].get("ì„œë¹„ìŠ¤ í‰ê°€", None)
            effect_val = abc_pivot.loc[mid].get("ì„œë¹„ìŠ¤ íš¨ê³¼", None)
            sat_val = abc_pivot.loc[mid].get("ì „ë°˜ì  ë§Œì¡±ë„", None)
            eval_str = f"{eval_val:.1f}" if pd.notna(eval_val) else "N/A"
            effect_str = f"{effect_val:.1f}" if pd.notna(effect_val) else "N/A"
            sat_str = f"{sat_val:.1f}" if pd.notna(sat_val) else "N/A"
            abc_lines.append(f"{mid}: í‰ê°€ {eval_str}, íš¨ê³¼ {effect_str}, ë§Œì¡±ë„ {sat_str}")
    abc_str = "\n".join(abc_lines)

    prompt = f"""
ì„¤ë¬¸ ë°ì´í„° ê³µí†µ ì‹¬í™” ë¶„ì„ ìš”ì•½ì„ ë§Œë“¤ì–´ì¤˜.

ì…ë ¥ ìš”ì•½:
- ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê·  ë§Œì¡±ë„: {midcat_str}
- ì¤‘ë¶„ë¥˜ë³„ ê°•ì  ì˜ˆì‹œ: {', '.join(strengths[:3])}
- ì¤‘ë¶„ë¥˜ë³„ ì•½ì  ì˜ˆì‹œ: {', '.join(weaknesses[:3])}
- ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) ë¹„êµ:
{abc_str}

ìš”ì²­:
1. ì£¼ìš” ê´€ì°° íŒ¨í„´ 2~3ê°œë¥¼ ê°„ê²°í•˜ê²Œ ê¸°ìˆ í•´ì¤˜.
2. ì–´ë–¤ ì¤‘ë¶„ë¥˜ê°€ ìƒëŒ€ì  ê°•ì /ì•½ì ì¸ì§€ ìˆ«ìì™€ í•¨ê»˜ ì„¤ëª…í•´ì¤˜.
3. A/B/C ë¹„êµì—ì„œ ë“œëŸ¬ë‚˜ëŠ” íŠ¹ì§•ì„ ì§šì–´ì¤˜.
4. ì „ëµì  ì œì•ˆ 3ê°œ: (1) ìš°ì„  ê°œì…í•  ëŒ€ìƒ, (2) í™•ì¥í•  ê°•ì , (3) ë³´ì™„í•  ì•½ì  ê°ê° êµ¬ì²´ì ìœ¼ë¡œ ì¨ì¤˜.

ì œí•œ: ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, ë¹„ì¦ˆë‹ˆìŠ¤ í†¤, ì†Œì œëª© í¬í•¨, ì „ì²´ 700~1100ì. ì¶œë ¥ì€ í…ìŠ¤íŠ¸ë§Œ.
"""
    return prompt.strip()


def build_area_insight_prompt(midcat_scores: dict, abc_df: pd.DataFrame) -> str:
    midcat_str = ", ".join(f"{k} {v:.1f}" for k, v in midcat_scores.items())
    abc_markdown = abc_df.to_markdown(index=False)
    prompt = f"""
ì„¤ë¬¸ ë°ì´í„° ì˜ì—­ë³„ A/B/C ë¹„êµ ìš”ì•½ì„ ë§Œë“¤ì–´ì¤˜.

ì…ë ¥:
- ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê· : {midcat_str}
- A/B/C ìœ í˜•ë³„ ì¤‘ë¶„ë¥˜ë³„ í‰ê· ê°’ (ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„):
{abc_markdown}

ìš”ì²­:
1. A/B/C íë¦„ì—ì„œ ëˆˆì— ë„ëŠ” ì°¨ì´ 2ê°œ(ì˜ˆ: í‰ê°€ ëŒ€ë¹„ ë§Œì¡±ë„ê°€ ë‚®ì€ ì¤‘ë¶„ë¥˜ ë“±).
2. ê° ìœ í˜•ë³„ ìƒëŒ€ì  ê°•ì /ì•½ì  í•œë‘ ë¬¸ì¥ì”© ì •ë¦¬.
3. ìš°ì„ ìˆœìœ„ ì œì•ˆ: ìœ ì§€Â·í™•ì¥í•  ê²ƒ 1ê°œ, ë³´ì™„í•  ê²ƒ 1ê°œì”©.

ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, ì†Œì œëª© í¬í•¨, ë¹„ì¦ˆë‹ˆìŠ¤ í†¤, 600~1000ì ë¶„ëŸ‰, ì¶œë ¥ì€ í…ìŠ¤íŠ¸ë§Œ.
"""
    return prompt.strip()



# ------------------ ì¥ë¬¸ ì‘ë‹µìš© ì •ì œ ------------------
def is_meaningful_long(text: str) -> bool:
    exclude = ['ì—†ìŒ', 'ëª¨ë¦„', 'ì—†ì–´ìš”', 'x', 'í•´ë‹¹ì—†ìŒ', 'ì—†ë‹¤', 'ì—†ìŠµë‹ˆë‹¤', 'ì—†ìŠµë‹ˆë‹¤.']
    t = str(text).strip()
    if len(t) < 4:
        return False
    for e in exclude:
        if e in t:
            return False
    return True

@st.cache_data(show_spinner=False)
def get_clean_long_responses(raw: list[str]) -> list[str]:
    return [r for r in raw if is_meaningful_long(r)]

# ------------------ GPT í”„ë¡¬í”„íŠ¸ ìƒì„±/íŒŒì‹± ------------------
def make_theme_messages(batch: list[str]) -> list[dict]:
    system_content = (
        "ë‹¹ì‹ ì€ ë„ì„œê´€ ì´ìš©ì ì„¤ë¬¸ ììœ ì„œìˆ  ì‘ë‹µì„ ì•„ë˜ 6ê°œ ì£¼ì œ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ê³ , "
        "ê° ì£¼ì œë³„ ëŒ€í‘œ í‚¤ì›Œë“œ(ì‰¼í‘œë¡œ), ìš”ì•½(í•œ ë¬¸ì¥ ë‚´ì™¸)ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
        "ë°˜ë“œì‹œ ì£¼ì œëª…ì€ ë‹¤ìŒë§Œ ì‚¬ìš©í•˜ì„¸ìš”: ê³µê°„ ë° ì‹œì„¤, ìë£Œ í™•ì¶©, í”„ë¡œê·¸ë¨ ë‹¤ì–‘í™”, ìš´ì˜ ë° ì‹œìŠ¤í…œ, ì§ì› ë° ì‘ëŒ€, ê¸°íƒ€.\n"
        "ì¶œë ¥ì€ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•íƒœë¡œ ì•„ë˜ì²˜ëŸ¼:\n"
        "| ì£¼ì œëª… | ëŒ€í‘œ í‚¤ì›Œë“œ | ìš”ì•½ |"
    )
    user_block = "[ì‹¤ì œ ì…ë ¥ ì‘ë‹µ]\n" + "\n".join(batch)
    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_block + "\n\n[ê²°ê³¼ í‘œ]\n| ì£¼ì œëª… | ëŒ€í‘œ í‚¤ì›Œë“œ | ìš”ì•½ |"}
    ]

def df_to_markdown_manual(df: pd.DataFrame) -> str:
    cols = list(df.columns)
    # í—¤ë”
    header = "| " + " | ".join(cols) + " |"
    separator = "| " + " | ".join(["---"] * len(cols)) + " |"
    # í–‰ë“¤
    rows = []
    for _, r in df.iterrows():
        # ì¤„ë°”ê¿ˆ ì œê±°, ë„ˆë¬´ ê¸¸ë©´ ì¤„ì„ ì²˜ë¦¬ ê°€ëŠ¥
        cells = [str(r[c]).replace("\n", " ").strip() for c in cols]
        row = "| " + " | ".join(cells) + " |"
        rows.append(row)
    return "\n".join([header, separator] + rows)


def make_sentiment_messages(batch: list[str], theme_df: pd.DataFrame) -> list[dict]:
    system_content = (
        "ë‹¹ì‹ ì€ ë„ì„œê´€ ììœ ì„œìˆ  ì‘ë‹µì„ ì£¼ì œë³„ë¡œ ê°ì„±(ê¸ì •/ë¶€ì •/ì¤‘ë¦½) ë¶„ë¥˜í•˜ê³ , "
        "ê° ì£¼ì œ+ê°ì„± ì¡°í•©ì— ëŒ€í•´ íŠ¹ì§•ì ì¸ í‘œí˜„ ì–‘ìƒì„ 200ì ë‚´ì™¸ë¡œ ìš”ì•½í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
        "ê°ì„±ì€ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½'ë§Œ ì‚¬ìš©í•˜ë©° ì¶œë ¥ì€ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì•„ë˜ì²˜ëŸ¼:\n"
        "| ì£¼ì œëª… | ê°ì„± | í‘œí˜„ ì–‘ìƒ ìš”ì•½ |"
    )
    user_block = "[ì‹¤ì œ ì…ë ¥ ì‘ë‹µ]\n" + "\n".join(batch)
    theme_table_md = df_to_markdown_manual(theme_df)
    combined = user_block + "\n\n[ì£¼ì œ í…Œì´ë¸”]\n" + theme_table_md
    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": combined + "\n\n[ê²°ê³¼ í‘œ]\n| ì£¼ì œëª… | ê°ì„± | í‘œí˜„ ì–‘ìƒ ìš”ì•½ |"}
    ]


def parse_markdown_table(table_text: str, cols: list[str]) -> pd.DataFrame:
    # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì²˜ìŒ ë‚˜ì˜¤ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ ë¸”ë¡(í—¤ë” + êµ¬ë¶„ì„  + í–‰ë“¤) ì¶”ì¶œ
    table_match = re.search(
        r"(\|[^\n]*\|\s*\n\|\s*[-:]+\s*\|\s*[-:]+\s*\|\s*[-:]+\s*\n(?:\|[^\n]*\|\s*\n?)*)",
        table_text
    )
    if table_match:
        table_block = table_match.group(1)
    else:
        # fallback: ì „ì²´ì—ì„œ íŒŒì´í”„ í¬í•¨ ë¼ì¸ë§Œ
        table_block = "\n".join([l for l in table_text.splitlines() if "|" in l])

    lines = []
    for line in table_block.splitlines():
        # separator ë¼ì¸ì€ ì œì™¸
        if re.match(r'^\|\s*[-:]+\s*\|\s*[-:]+\s*\|\s*[-:]+\s*\|', line):
            continue
        if "|" in line:
            lines.append(line)

    records = []
    for line in lines:
        parts = [p.strip() for p in line.strip().split("|")[1:-1]]
        if len(parts) == len(cols):
            records.append(parts)
    df = pd.DataFrame(records, columns=cols)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DataFrame & visualization helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _sanitize_dataframe_for_streamlit(df: pd.DataFrame) -> pd.DataFrame:
    df2 = df.copy()

    for col in df2.select_dtypes(include=["object"]).columns:
        df2[col] = df2[col].apply(lambda x: str(x) if not pd.isna(x) else x)

    if isinstance(df2.index, pd.MultiIndex):
        df2.index = df2.index.map(lambda tup: " | ".join(map(str, tup)))
    else:
        df2.index = df2.index.map(lambda x: str(x))

    df2.columns = [str(c) for c in df2.columns]
    return df2

def render_chart_and_table(bar, table, title, key_prefix=""):
    if bar is not None:
        st.plotly_chart(bar, use_container_width=True, key=f"{key_prefix}-bar-{title}")
    if isinstance(table, go.Figure):
        st.plotly_chart(table, use_container_width=True, key=f"{key_prefix}-tbl-fig-{title}")
    elif isinstance(table, pd.DataFrame):
        try:
            safe_tbl = _sanitize_dataframe_for_streamlit(table)
            st.dataframe(safe_tbl, key=f"{key_prefix}-tbl-df-{title}")
        except Exception as e:
            logging.warning(f"DataFrame rendering failed, showing head only: {e}")
            try:
                safe_head = _sanitize_dataframe_for_streamlit(table.head(200))
                st.dataframe(safe_head, key=f"{key_prefix}-tbl-df-{title}-sample")
                st.warning(f"ì „ì²´ í…Œì´ë¸” ë Œë”ë§ì— ì‹¤íŒ¨í•˜ì—¬ ìƒìœ„ 200ê°œë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤: {e}")
            except Exception as e2:
                st.error(f"í…Œì´ë¸” ë Œë”ë§ ë¶ˆê°€: {e2}")
    elif table is not None:
        st.write(table, key=f"{key_prefix}-tbl-raw-{title}")

def show_table(df, caption):
    st.dataframe(df)


def render_insight_card(title: str, content: str, key: str = None):
    if content is None:
        content = "(ë‚´ìš© ì—†ìŒ)"
    content = str(content)
    content_html = escape_tildes(content, mode="html").replace("\n", "<br>")
    line_count = content.count("\n") + 3
    height = min(800, 70 + 20 * line_count)
    html = f"""
    <div style="
        border:1px solid #e2e8f0;
        border-radius:12px;
        padding:16px;
        margin-bottom:16px;
        background: #ffffff;
        box-shadow: 0 8px 24px rgba(0,0,0,0.04);
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', Arial;
    ">
        <h4 style="margin:0 0 8px 0; font-size:1.1rem;">{title}</h4>
        <div style="font-size:0.95em; line-height:1.4em;">{content_html}</div>
    </div>
    """
    try:
        components.html(html, height=height, key=key)
    except Exception as e:
        logging.warning(f"components.html failed for key={key}: {e}")
        # Fallback so the user still sees something
        st.markdown(f"**{title}**\n\n{content}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Likert / ì¤‘ë¶„ë¥˜ ì ìˆ˜ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def scale_likert(series):
    return 100 * (pd.to_numeric(series, errors='coerce') - 1) / 6

MIDDLE_CATEGORY_MAPPING = {
    "ê³µê°„ ë° ì´ìš©í¸ì˜ì„±":       lambda col: str(col).startswith("Q1-"),
    "ì •ë³´ íšë“ ë° í™œìš©":       lambda col: str(col).startswith("Q2-"),
    "ì†Œí†µ ë° ì •ì±… í™œìš©":       lambda col: str(col).startswith("Q3-"),
    "ë¬¸í™”Â·êµìœ¡ í–¥ìœ ":         lambda col: str(col).startswith("Q4-"),
    "ì‚¬íšŒì  ê´€ê³„ í˜•ì„±":       lambda col: str(col).startswith("Q5-"),
    "ê°œì¸ì˜ ì‚¶ê³¼ ì—­ëŸ‰":       lambda col: str(col).startswith("Q6-"),
    "ë„ì„œê´€ì˜ ê³µìµì„± ë° ê¸°ì—¬ë„": lambda col: (str(col).startswith("Q7-") or str(col).startswith("Q8")),
}

@st.cache_data(show_spinner=False)
def compute_midcategory_scores(df):
    results = {}
    for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
        cols = [c for c in df.columns if predicate(c)]
        if not cols:
            continue
        scaled = df[cols].apply(scale_likert)
        mid_mean = scaled.mean(axis=0, skipna=True).mean()
        results[mid] = mid_mean
    return pd.Series(results).dropna()

@st.cache_data(show_spinner=False)
def compute_within_category_item_scores(df):
    item_scores = {}
    for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
        cols = [c for c in df.columns if predicate(c)]
        if not cols:
            continue
        scaled = df[cols].apply(scale_likert)
        item_means = scaled.mean(axis=0, skipna=True)
        item_scores[mid] = item_means
    return item_scores

def midcategory_avg_table(df):
    s = compute_midcategory_scores(df)
    if s.empty:
        return pd.DataFrame()
    tbl = s.rename("í‰ê·  ì ìˆ˜(0~100)").to_frame().reset_index().rename(columns={"index": "ì¤‘ë¶„ë¥˜"})
    tbl["í‰ê·  ì ìˆ˜(0~100)"] = tbl["í‰ê·  ì ìˆ˜(0~100)"].round(2)
    tbl = tbl.sort_values(by="í‰ê·  ì ìˆ˜(0~100)", ascending=False).reset_index(drop=True)
    return tbl

def plot_midcategory_radar(df):
    mid_scores = compute_midcategory_scores(df)
    if mid_scores.empty:
        return None
    categories = list(mid_scores.index)
    values = mid_scores.values.tolist()
    categories_closed = categories + categories[:1]
    values_closed = values + values[:1]
    overall_mean = mid_scores.mean()
    avg_values_closed = [overall_mean] * len(categories_closed)

    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=values_closed,
        theta=categories_closed,
        fill='toself',
        name='ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„',
        hovertemplate="%{theta}: %{r:.1f}<extra></extra>"
    ))
    fig.add_trace(go.Scatterpolar(
        r=avg_values_closed,
        theta=categories_closed,
        fill=None,
        name=f"ì „ì²´ í‰ê·  ({overall_mean:.1f})",
        line=dict(color='red', dash='solid'),
        hovertemplate=f"ì „ì²´ í‰ê· : {overall_mean:.1f}<extra></extra>"
    ))
    fig.update_layout(
        polar=dict(radialaxis=dict(range=[0,100], tickformat=".0f")),
        title="ì¤‘ë¶„ë¥˜ë³„ ë§Œì¡±ë„ ìˆ˜ì¤€ (0~100 í™˜ì‚°, ë ˆì´ë” ì°¨íŠ¸)",
        showlegend=True,
        height=450,
        margin=dict(t=40, b=20)
    )
    return fig

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPT ê´€ë ¨ í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_used_questions_from_spec(spec: dict, df_full: pd.DataFrame, df_filtered: pd.DataFrame):
    """
    spec ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì‚¬ìš©ëœ ë¬¸í•­ ì „ì²´ ì´ë¦„ê³¼ ë¬¸í•­ ì½”ë“œ(ë²ˆí˜¸)ë¥¼ ì¶”ì¶œ.
    ë°˜í™˜: (used_full_list, used_codes_list) â€” ìˆœì„œ ë³´ì¡´, ì¤‘ë³µ ì œê±°
    """
    used_full = []

    # 1. ëª…ì‹œëœ x, y, groupby (groupbyëŠ” str ë˜ëŠ” list í—ˆìš©)
    for key in ("x", "y"):
        val = spec.get(key)
        if val and val in df_full.columns:
            used_full.append(val)

    gb = spec.get("groupby")
    if gb:
        if isinstance(gb, list):
            for g in gb:
                if g in df_full.columns:
                    used_full.append(g)
        else:
            if gb in df_full.columns:
                used_full.append(gb)

    # 2. filtersì— ì“°ì¸ ì»¬ëŸ¼
    for f in spec.get("filters", []):
        col = f.get("col")
        if col and col in df_full.columns:
            used_full.append(col)

    # 3. focus/ì°¨íŠ¸ ìœ í˜• ë“±ì— ë”°ë¼ ì¤‘ë¶„ë¥˜ ê¸°ë°˜ ë¬¸í•­ í¬í•¨
    focus = (spec.get("focus") or "").lower()
    chart_type = (spec.get("chart") or "").lower() if spec.get("chart") else ""
    need_midcat = any(k in focus for k in ["ì¤‘ë¶„ë¥˜", "ê°•ì ", "ì•½ì ", "ì „ì²´ í‰ê· ", "í”„ë¡œíŒŒì¼", "ë¹„êµ"]) or chart_type in {"radar", "delta_bar", "heatmap", "grouped_bar"}
    if need_midcat:
        # ì‹¤ì œ í•„í„°ëœ ë°ì´í„°ì— í•´ë‹¹ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ë©° í¬í•¨
        for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
            cols = [c for c in df_filtered.columns if predicate(c)]
            if cols:
                used_full.extend(cols)

    # 4. groupbyê°€ ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©ì¼ ê²½ìš° (ì˜ˆ: ì—¬ëŸ¬ íŒŒìƒ ì»¬ëŸ¼ì´ ê²°í•©ëœ ê²½ìš°), df_filteredì— ìˆëŠ” ê·¸ ìì²´ë„ í¬í•¨
    if gb:
        if isinstance(gb, list):
            for g in gb:
                if g in df_filtered.columns:
                    used_full.append(g)
        else:
            if gb in df_filtered.columns:
                used_full.append(gb)

    # ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
    seen = set()
    used_full_unique = []
    for c in used_full:
        if c and c not in seen:
            seen.add(c)
            used_full_unique.append(c)

    # ì½”ë“œë§Œ ë½‘ê³  ì¤‘ë³µ ì œê±°
    used_codes = []
    seen_codes = set()
    for col in used_full_unique:
        code = extract_question_code(col)
        if code not in seen_codes:
            seen_codes.add(code)
            used_codes.append(code)

    return used_full_unique, used_codes

# ---- ëª¨ë¸ ì„ íƒ í—¬í¼ ----
_COMPLEXITY_KEYWORDS = [
    "ë¹„êµ", "ê°•ì ", "ì•½ì ", "ì „ì²´ í‰ê· ", "í”„ë¡œíŒŒì¼", "ì°¨ì´", "ìš°ì„ ìˆœìœ„", "í¸ì°¨", "êµ°ì§‘", "ì„¸ê·¸ë¨¼íŠ¸"
]

def select_model_for_explanation(spec: dict, computed_metrics: dict) -> str:
    """
    ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë³µì¡ë„ í‰ê°€ í›„ ëª¨ë¸ ì„ íƒ.
    ì ìˆ˜ ê¸°ì¤€:
      - groupby ìˆìœ¼ë©´ +1
      - ì°¨íŠ¸ê°€ ë³µì¡í•œ ìœ í˜•ì´ë©´ +1 (radar, delta_bar, heatmap, grouped_bar)
      - filters ê°œìˆ˜ë§Œí¼ +1ì”©
      - top_segmentsê°€ 2ê°œ ì´ìƒì´ë©´ +1
      - focusì— ë³µì¡ë„ í‚¤ì›Œë“œ í¬í•¨ë˜ë©´ ê°ê° +1 (ì¤‘ë³µ ì—†ì´)
    threshold >=3 ì´ë©´ gpt-4.1, ì•„ë‹ˆë©´ gpt-3.5-turbo
    """
    score = 0
    if spec.get("groupby"):
        score += 1
    chart = (spec.get("chart") or "").lower() if spec.get("chart") else ""
    if chart in {"radar", "delta_bar", "heatmap", "grouped_bar"}:
        score += 1
    filters = spec.get("filters") or []
    score += len(filters)
    top_segments = computed_metrics.get("top_segments", [])
    if len(top_segments) >= 2:
        score += 1
    focus = (spec.get("focus") or "").lower()
    for kw in _COMPLEXITY_KEYWORDS:
        if kw.lower() in focus:
            score += 1
    # ì„ íƒ
    if score >= 3:
        return "gpt-4.1"
    else:
        return "gpt-3.5-turbo"




def cohen_d(x, y):
    x = np.array(x.dropna(), dtype=float)
    y = np.array(y.dropna(), dtype=float)
    nx, ny = len(x), len(y)
    if nx < 2 or ny < 2:
        return None
    # pooled standard deviation
    pooled = np.sqrt(((nx - 1) * x.var(ddof=1) + (ny - 1) * y.var(ddof=1)) / (nx + ny - 2)) if (nx + ny - 2) > 0 else 0
    if pooled == 0:
        return 0.0
    return (x.mean() - y.mean()) / pooled

def compare_midcategory_by_group(df, group_col):
    """
    group_col ê¸°ì¤€ìœ¼ë¡œ ê° ê·¸ë£¹ì˜ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ë¥¼ ì „ì²´ ë‚˜ë¨¸ì§€ì™€ ë¹„êµí•˜ì—¬
    mean, delta, Welch t-test p-value, Cohen's d, sample sizeë¥¼ ê³„ì‚°.
    ë°˜í™˜ í˜•íƒœ: {group_label: {midcategory: {mean, delta_vs_overall, p_value_vs_rest, cohen_d_vs_rest, n}}}
    """
    results = {}
    global_mid = compute_midcategory_scores(df)
    
    # per-row midcategory scores (ê° ì‘ë‹µìë³„ë¡œ ì¤‘ë¶„ë¥˜ ì ìˆ˜ ê³„ì‚°)
    def per_row_mid_scores(subdf):
        per_mid = {}
        for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
            cols = [c for c in subdf.columns if predicate(c)]
            if not cols:
                continue
            scaled = subdf[cols].apply(scale_likert)
            per_mid[mid] = scaled.mean(axis=1, skipna=True)
        return per_mid  # dict of Series

    # ì „ì²´ ë°ì´í„° ê¸°ì¤€ per-row
    overall_per_row = per_row_mid_scores(df)

    for group_value, sub in df.groupby(df[group_col].astype(str)):
        group_per_row = per_row_mid_scores(sub)
        group_summary = {}
        for mid in global_mid.index:
            if mid not in group_per_row or mid not in overall_per_row:
                continue
            grp_scores = group_per_row[mid].dropna()
            rest_mask = df[group_col].astype(str) != str(group_value)
            rest = df[rest_mask]
            rest_per_row = per_row_mid_scores(rest).get(mid, pd.Series(dtype=float)).dropna()
            if grp_scores.empty or rest_per_row.empty:
                continue
            # Welch's t-test
            try:
                stat, p = stats.ttest_ind(grp_scores, rest_per_row, equal_var=False)
            except Exception:
                p = None
            d = cohen_d(grp_scores, rest_per_row)
            mean_group = compute_midcategory_scores(sub).get(mid, None)
            delta = None
            if mean_group is not None and mid in global_mid:
                delta = mean_group - global_mid.get(mid)
            group_summary[mid] = {
                "mean": round(float(mean_group), 1) if mean_group is not None else None,
                "delta_vs_overall": round(float(delta), 1) if delta is not None else None,
                "p_value_vs_rest": round(p, 4) if p is not None else None,
                "cohen_d_vs_rest": round(d, 2) if d is not None else None,
                "n": int(len(grp_scores))
            }
        results[str(group_value)] = group_summary
    return results


@st.cache_data(show_spinner=False)
def process_answers(responses):
    expanded = []
    for ans in responses:
        if is_trivial(ans):
            continue
        parts = [p.strip() for p in ans.split(',') if p.strip()]
        if len(parts) > 1:
            expanded.extend(parts)
        else:
            expanded.append(ans)

    processed = []
    batches = extract_keyword_and_audience(expanded, batch_size=8)
    for resp, kws, aud in batches:
        if is_trivial(resp):
            continue
        if not kws:
            kws = split_keywords_simple(resp)
        for kw in kws:
            cat = map_keyword_to_category(kw)
            if cat == 'í•´ë‹¹ì—†ìŒ' and aud == 'ì¼ë°˜':
                continue
            processed.append({
                'ì‘ë‹µ': resp,
                'í‚¤ì›Œë“œ': kw,
                'ì£¼ì œë²”ì£¼': cat,
                'ëŒ€ìƒë²”ì£¼': aud
            })
    return pd.DataFrame(processed)

@st.cache_data(show_spinner=False)
def extract_keyword_and_audience(responses, batch_size=20):
    results = []
    for i in range(0, len(responses), batch_size):
        batch = responses[i:i+batch_size]
        prompt = f"""
ë‹¹ì‹ ì€ ë„ì„œê´€ ììœ ì‘ë‹µì—ì„œ ì•„ë˜ í˜•ì‹ì˜ JSON ë°°ì—´ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
ê° ê°ì²´ëŠ” ì‘ë‹µ, í‚¤ì›Œë“œ ëª©ë¡(1~3ê°œ), ëŒ€ìƒì¸µ(ìœ ì•„/ì•„ë™/ì²­ì†Œë…„/ì¼ë°˜)ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì˜ˆì‹œOutput:
[
  {{"response": "ì‘ë‹µ1", "keywords": ["í‚¤ì›Œë“œ1","í‚¤ì›Œë“œ2"], "audience": "ì²­ì†Œë…„"}},
  ...
]

ì‘ë‹µ ëª©ë¡:
{chr(10).join(f"{j+1}. {txt}" for j, txt in enumerate(batch))}
"""
        try:
            resp = safe_chat_completion(
                model="gpt-4.1",
                messages=[{"role": "system", "content": prompt}],
                temperature=0.2,
                max_tokens=300
            )
            content = resp.choices[0].message.content.strip()
            try:
                data = pd.read_json(content)
            except Exception:
                raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨, fallbackìœ¼ë¡œ ì „í™˜")
        except Exception:
            # fallback
            data = []
            for txt in batch:
                kws = split_keywords_simple(txt)
                audience = 'ì¼ë°˜'
                for w in ['ì–´ë¦°ì´', 'ì´ˆë“±']:
                    if w in txt:
                        audience = 'ì•„ë™'
                for w in ['ìœ ì•„', 'ë¯¸ì·¨í•™', 'ê·¸ë¦¼ì±…']:
                    if w in txt:
                        audience = 'ìœ ì•„'
                for w in ['ì²­ì†Œë…„', 'ì§„ë¡œ', 'ìê¸°ê³„ë°œ']:
                    if w in txt:
                        audience = 'ì²­ì†Œë…„'
                data.append({
                    'response': txt,
                    'keywords': kws,
                    'audience': audience
                })
            data = pd.DataFrame(data)
        for _, row in data.iterrows():
            results.append((row['response'], row['keywords'], row['audience']))
    return results

def call_gpt_for_insight(prompt, model="gpt-4.1-mini", temperature=0.2, max_tokens=1500):
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì „ëµ ë¦¬í¬íŠ¸ ì‘ì„±ìì´ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•œë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )
        content = resp.choices[0].message.content.strip()
        return content
    except Exception as e:
        logging.warning(f"GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return f"GPT í•´ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"

def build_radar_prompt(overall_profile: dict, combos: list):
    overall_str = ", ".join(f"{k}: {v:.1f}" for k, v in overall_profile.items())
    # ìƒìœ„ 10ê°œ ì¡°í•©ë§Œ ê³ ë ¤í•˜ë„ë¡ ëª…ì‹œ
    combo_lines = []
    for c in combos[:10]:
        prof = ", ".join(f"{k}: {v:.1f}" for k, v in c["profile"].items())
        combo_lines.append(f"{c['label']} (ì‘ë‹µììˆ˜={c['n']}): {prof}")
    combo_str = "\n".join(combo_lines)
    prompt = f"""
ì „ëµ ë³´ê³ ì„œìš© - ë ˆì´ë” ì°¨íŠ¸ í•´ì„

ì…ë ¥:
- ì „ì²´ í‰ê·  ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„: {overall_str}
- ìƒìœ„ 10ê°œ ì¡°í•©ë³„ í”„ë¡œíŒŒì¼:
{combo_str}

ìš”ì²­:
1. [ì‹œê°í™” í•´ì„]ì—ì„œ ë ˆì´ë” ì°¨íŠ¸ê°€ ë³´ì—¬ì£¼ëŠ” ì „ì²´ ë¶„í¬, êµì°¨Â·ì¤‘ì²©Â·ê·¹ë‹¨ ì˜ì—­ì„ 200ì ì´ë‚´ë¡œ ì •ë¦¬.
2. ê° ìƒìœ„ 10ê°œ ì¡°í•©ë³„ë¡œ ì „ì²´ í‰ê·  ëŒ€ë¹„ ì–´ë–¤ ì¤‘ë¶„ë¥˜ê°€ ê°•ì ì¸ì§€ ì•½ì ì¸ì§€ ëª…í™•í•˜ê²Œ 1ì¤„ì”© ì •ë¦¬.
3. ì¡°í•© ê°„ ì°¨ì´ë¥¼ ë¹„êµí•˜ë©´ì„œ, ì´ ì¤‘ ì–´ë–¤ ì¡°í•©ì´ ê°€ì¥ ë§Œì¡±ë„ê°€ ë†’ì€ì§€, ì–´ë–¤ ì¡°í•©ì´ ê°€ì¥ ë¶ˆë§Œì¡±ì´ ëšœë ·í•œì§€ ì„¤ëª….
4. ê´€ì°°ëœ íŒ¨í„´ 3ê°œ ë„ì¶œ(ì˜ˆ: íŠ¹ì • ì„±ë³„/ì—°ë ¹ëŒ€ ê·¸ë£¹ì˜ ë°˜ë³µì  íŠ¹ì§•).
5. ì „ëµì  ì‹œì‚¬ì : ê° ì£¼ìš” ì¡°í•©ëª…ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì „ëµì´ í•„ìš”í•œì§€ 3ê°œ ì´ìƒ ì œì•ˆ.
-ì€ ëª¨ë‘ -ë¡œ, ìˆ«ì í•œ ìë¦¬ ì†Œìˆ˜, ì¡°í•©ëª… ë°˜ë³µ, ì†Œì œëª© í¬í•¨, ì „ì²´ ê¸¸ì´ 1200ì ë‚´ì™¸.

[ì¶œë ¥ ì˜ˆì‹œ]
---
[ì‹œê°í™” í•´ì„]
ì •ë³´ íšë“ê³¼ ë¬¸í™”-êµìœ¡ í–¥ìœ ê°€ ì—¬ëŸ¬ ì¡°í•©ì—ì„œ ë†’ê²Œ ê²¹ì¹˜ì§€ë§Œ ì†Œí†µ ë° ì •ì±… í™œìš©ì€ ê±°ì˜ ëª¨ë“  ì¡°í•©ì—ì„œ ë‚®ì•„ êµì°¨ê°€ ëšœë ·í•˜ë‹¤. ì¼ë¶€ ì¡°í•©ì€ ë§Œì¡±-ë¶ˆë§Œì¡±ì˜ ëŒ€ë¹„ê°€ ê°•í•˜ê²Œ ë¶„ë¦¬ëœë‹¤.

### 1. ì¡°í•©ë³„ ê°•ì Â·ì•½ì  (ìƒìœ„ 10ê°œ ì¤‘ ëŒ€í‘œ 3ê°œ ì˜ˆì‹œ)
* ì—¬ì„± | 30-34ì„¸ëŠ” ì •ë³´ íšë“ì´ +10.2ë¡œ ê°•ì ì´ê³ , ì†Œí†µ ë° ì •ì±… í™œìš©ì€ -7.3ë¡œ ì•½ì ì´ë‹¤.
* ë‚¨ì„± | 40-44ì„¸ëŠ” ë¬¸í™”-êµìœ¡ í–¥ìœ ê°€ +8.5ë¡œ ë†’ê³ , ê³µê°„-ì´ìš©í¸ì˜ì„±ì€ -5.1ë¡œ ì•½ì ì´ë‹¤.
* ì—¬ì„± | 50-54ì„¸ëŠ” ê°œì¸ì˜ ì‚¶ê³¼ ì—­ëŸ‰ì´ í‰ê· ë³´ë‹¤ ë†’ê³ , ì •ë³´ íšë“ì´ ì•½ê°„ ë‚®ë‹¤.

### 2. ì¡°í•© ê°„ ì°¨ì´ì™€ ë§Œì¡±/ë¶ˆë§Œì¡±
ì—¬ì„± | 30-34ì„¸ëŠ” ì •ë³´ íšë“ì—ì„œ ê°€ì¥ ë§Œì¡±ë„ê°€ ë†’ê³ , ì†Œí†µ ì €í•˜ ì¡°í•©(ì˜ˆ: ë‚¨ì„± | 55-59ì„¸)ì€ ì •ì±… í™œìš©ì—ì„œ ê°€ì¥ ë¶ˆë§Œì¡±ì´ ëšœë ·í•˜ë‹¤. ëŒ€ë¹„ë˜ëŠ” ì¡°í•© ê°„ ì°¨ì´ë¡œ íƒ€ê²Ÿ ë§ì¶¤ ì½˜í…ì¸  í•„ìš”ì„±ì´ ë“œëŸ¬ë‚œë‹¤.

### 3. íŒ¨í„´
* 30ëŒ€ ì—¬ì„± ì¡°í•©ì€ ì •ë³´ íšë“ ê°•ì„¸ê°€ ë°˜ë³µëœë‹¤.
* 40ëŒ€ ë‚¨ì„±ì€ ë¬¸í™”-êµìœ¡ í–¥ìœ ëŠ” ë†’ê³  ì†Œí†µì´ ë‚®ë‹¤.
* ê³ ì—°ë ¹ëŒ€ ì¡°í•©ì€ ì „ì²´ì ìœ¼ë¡œ ë§Œì¡±ë„ ë¶„ì‚°ì´ í¬ë‹¤.

### 4. ì „ëµì  ì‹œì‚¬ì 
* ì—¬ì„± | 30-34ì„¸ì—ëŠ” ì •ë³´ í™œìš© ê³ ë„í™” í”„ë¡œê·¸ë¨ì„ í™•ëŒ€í•˜ì—¬ ê°•ì ì„ í™œìš©.
* ë‚¨ì„± | 40-44ì„¸ì—ëŠ” ë¬¸í™”-êµìœ¡ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ìœ ì§€í•˜ë©´ì„œ ê³µê°„ í¸ì˜ì„± ê°œì„  ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê°•í™”.
* ì†Œí†µ ì•½ì ì´ ë°˜ë³µë˜ëŠ” ì¡°í•©(ì˜ˆ: ë‚¨ì„± | 55-59ì„¸)ì—ëŠ” ì •ì±… ì•ˆë‚´ ì±„ë„ ì¬ì„¤ê³„ ë° ì°¸ì—¬ ìœ ë„ ì „ëµ ì ìš©.
---
"""
    return prompt.strip()

def build_heatmap_prompt(table_df, midcats, label_col="ì¡°í•©"):
    # ì…ë ¥ ê²€ì¦
    if table_df.empty:
        raise ValueError("table_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    missing_midcats = [mc for mc in midcats if mc not in table_df.columns]
    if missing_midcats:
        raise KeyError(f"ë‹¤ìŒ ì¤‘ë¶„ë¥˜ ì»¬ëŸ¼ì´ table_dfì— ì—†ìŠµë‹ˆë‹¤: {missing_midcats}")
    if label_col not in table_df.columns:
        raise KeyError(f"ë¼ë²¨ìš© ì»¬ëŸ¼ '{label_col}'ì´ table_dfì— ì—†ìŠµë‹ˆë‹¤.")

    # ì „ì²´ í‰ê·  ê³„ì‚°
    overall_avg = {mc: table_df[mc].mean() for mc in midcats}

    # ì¤‘ë¶„ë¥˜ë³„ ìµœê³ /ìµœì € ì¡°í•© ì°¾ê¸°
    extremes = {}
    for mc in midcats:
        if table_df[mc].dropna().empty:
            continue
        highest_idx = table_df[mc].idxmax()
        lowest_idx = table_df[mc].idxmin()
        highest_row = table_df.loc[highest_idx]
        lowest_row = table_df.loc[lowest_idx]
        hi_label = highest_row[label_col]
        lo_label = lowest_row[label_col]
        hi_score = highest_row[mc]
        lo_score = lowest_row[mc]
        extremes[mc] = {
            "highest": (hi_label, hi_score),
            "lowest": (lo_label, lo_score)
        }

    # ì¡°í•©ë³„ ì ìˆ˜ ë¬¸ìì—´
    combo_lines = []
    for _, r in table_df.iterrows():
        label = r[label_col]
        scores = ", ".join(f"{mc}: {r[mc]:.1f}" for mc in midcats)
        combo_lines.append(f"{label}: {scores}")
    combos_str = "\n".join(combo_lines)

    # ì „ì²´ í‰ê·  ì„œìˆ 
    avg_str = ", ".join(f"{mc}({overall_avg[mc]:.1f}ì )" for mc in midcats)

    # ì˜ˆì‹œ ë¸”ë¡ ìƒì„±
    extremes_example_lines = []
    for mc, vals in extremes.items():
        hi_name, hi_score = vals["highest"]
        lo_name, lo_score = vals["lowest"]
        extremes_example_lines.append(f"* [{mc}]  ìµœê³ : {hi_name}({hi_score:.1f})  ìµœì €: {lo_name}({lo_score:.1f})")
    extremes_example_block = "\n".join(extremes_example_lines)

    prompt = f"""
ì „ëµ ë³´ê³ ì„œìš© - íˆíŠ¸ë§µ ì¸ì‚¬ì´íŠ¸

ì…ë ¥:
- ì „ì²´ í‰ê· : {avg_str}
- ì¡°í•©ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ (ì¡°í•©ëª…ê³¼ ì ìˆ˜ë§Œ, ì‘ë‹µì ìˆ˜ ê¸°ë°˜ í‘œí˜„ ê¸ˆì§€):
{combos_str}

ìš”ì²­:
1. [ì‹œê°í™” í•´ì„]ì—ì„œ íˆíŠ¸ë§µì˜ ì£¼ìš” êµ°ì§‘, ê³ ì /ì €ì , ì¡°í•© ê°„ ìœ ì‚¬ì„±Â·ì°¨ì´ì ì„ 200ì ë‚´ì™¸ë¡œ ìš”ì•½.
2. ê° ì¤‘ë¶„ë¥˜ë³„ë¡œ ì „ì²´ í‰ê·  íë¦„ì„ ì§šì€ ë’¤, ì‹¤ì œ ì¡°í•©ëª…ì„ ì¨ì„œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬:
{extremes_example_block}
   - ì ˆëŒ€ 'ì‘ë‹µì Xëª…'ì²˜ëŸ¼ í‘œí˜„í•˜ì§€ ë§ê³ , ì…ë ¥ëœ ì •í™•í•œ ì¡°í•©ëª…ë§Œ ì‚¬ìš©í•  ê²ƒ.
3. ì „ì²´ ê²½í–¥ ë° ì˜ˆì™¸ëŠ” ì¡°í•©ëª… ëª…ì‹œ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ìœ ì§€.
4. ë§ˆì§€ë§‰ì— ìš”ì•½ í•œ ë¬¸ë‹¨ê³¼ ì¡°í•©ëª…ì„ í¬í•¨í•œ êµ¬ì²´ì  í–‰ë™ ê¶Œì¥ì  3ê°œ ì œì‹œ.
-ì€ ëª¨ë‘ -ë¡œ, ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, ì†Œì œëª© í¬í•¨, ì¡°í•©ëª… ë°˜ë³µ, ì „ì²´ ê¸¸ì´ 1200ì ë‚´ì™¸.

[ì¶œë ¥ ì˜ˆì‹œ]
---
[ì‹œê°í™” í•´ì„]
ì •ë³´ íšë“ê³¼ ë¬¸í™”-êµìœ¡ í–¥ìœ ê°€ ìƒìœ„ì— ëª°ë ¤ ìˆê³ , ì†Œí†µ ë° ì •ì±… í™œìš©ì€ ì—¬ëŸ¬ ì¡°í•©ì—ì„œ ë‚®ì•„ ëŒ€ë¹„ê°€ ëšœë ·í•˜ë‹¤. ì—¬ì„± | 30-34ì„¸ì™€ ì—¬ì„± | 35-39ì„¸ëŠ” ìœ ì‚¬í•œ í”„ë¡œíŒŒì¼ë¡œ êµ°ì§‘ì„ ì´ë£¬ë‹¤.

### 1. ì¤‘ë¶„ë¥˜ë³„ ìµœê³ Â·ìµœì €
* [ê³µê°„ ë° ì´ìš©í¸ì˜ì„±]  ìµœê³ : ì—¬ì„± | 35-39ì„¸(90.8)  ìµœì €: ë‚¨ì„± | 20-25ì„¸(51.2)
* [ì •ë³´ íšë“ ë° í™œìš©]  ìµœê³ : ì—¬ì„± | 30-34ì„¸(88.1)  ìµœì €: ë‚¨ì„± | 55-59ì„¸(60.4)
* [ì†Œí†µ ë° ì •ì±… í™œìš©]  ìµœê³ : ì—¬ì„± | 40-44ì„¸(83.0)  ìµœì €: ë‚¨ì„± | 50-54ì„¸(58.7)

### 2. ì „ì²´ ê²½í–¥ ë° ì˜ˆì™¸
* ì „ë°˜ì ìœ¼ë¡œ ì •ë³´ íšë“ì´ ë†’ê³  ì†Œí†µì´ ë‚®ì€ íë¦„ì´ë©°, ì—¬ì„± | 40-44ì„¸ë§Œ ì†Œí†µì—ì„œ ì˜ˆì™¸ì ìœ¼ë¡œ í‰ê· ì„ ìƒíšŒí•œë‹¤.

### 3. ìš”ì•½ ë° í–‰ë™ ê¶Œì¥ì 
* ì—¬ì„± 30-40ëŒ€ì—ê²Œ ì •ë³´ ì ‘ê·¼ì„± ê°•í™”.
* ì†Œí†µ ì•½ì  ë°˜ë³µ ì¡°í•©(ì˜ˆ: ë‚¨ì„± | 50-54ì„¸)ì—ëŠ” ë§ì¶¤ ì•ˆë‚´ ì¬ì„¤ê³„.
* ë¬¸í™”-êµìœ¡ í–¥ìœ ê°€ ë†’ì€ ì¡°í•©ì€ í”„ë¡œê·¸ë¨ ì—°ê³„ ê°•í™”.
---
"""
    return prompt.strip()

def build_delta_prompt(delta_df, midcats):
    rows = []
    for _, r in delta_df.iterrows():
        combo = r.name
        diffs = ", ".join(f"{mc}: {r.get(f'{mc}_delta', 0):+.1f}" for mc in midcats)
        rows.append(f"{combo}: {diffs}")
    table_str = "\n".join(rows)
    prompt = f"""
ì „ëµ ë³´ê³ ì„œìš© - Delta í•´ì„

ì…ë ¥:
- ì „ì²´ í‰ê·  ëŒ€ë¹„ ì¡°í•©ë³„ ì¤‘ë¶„ë¥˜ í¸ì°¨:
{table_str}

ìš”ì²­:
1. [ì‹œê°í™” í•´ì„]ì—ì„œ ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ í° í¸ì°¨ë¥¼ ë³´ì´ëŠ” ì¡°í•©ë“¤ë§Œ ê³¨ë¼ì„œ ê·¸ íŠ¹ì§•(í”ŒëŸ¬ìŠ¤/ë§ˆì´ë„ˆìŠ¤ ë°©í–¥, ë°˜ë³µ íŒ¨í„´)ì„ 200ì ë‚´ì™¸ë¡œ ì •ë¦¬.
2. ê·¸ ë‹¤ìŒì— ìœ ì‚¬í•œ í¸ì°¨ íŒ¨í„´ì„ ê°€ì§„ ì¡°í•©ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ ë¹„êµ ì„¤ëª…í•˜ê³ , ì–´ë–¤ ì¡°í•©ì„ ë¨¼ì € ê°œì…í•˜ê±°ë‚˜ í™•ì¥í• ì§€ ìš°ì„ ìˆœìœ„ í¬í•¨í•´ ì‘ì„±.
-ì€ ëª¨ë‘ -ë¡œ, ìˆ«ì í•œ ìë¦¬ ì†Œìˆ˜, ì¡°í•©ëª… ë°˜ë³µ, ì†Œì œëª© í¬í•¨, ì „ì²´ ê¸¸ì´ 1200ì ë‚´ì™¸.

[ì¶œë ¥ ì˜ˆì‹œ]
---
[ì‹œê°í™” í•´ì„]
ì—¬ì„± | 30-34ì„¸ì˜ ì •ë³´ íšë“ í¸ì°¨ê°€ +12.0ìœ¼ë¡œ ê°€ì¥ í¬ê³ , ì†Œí†µ ë° ì •ì±… í™œìš©ì€ -9.0ìœ¼ë¡œ ë°˜ëŒ€ ë°©í–¥ í¸ì°¨ê°€ ëšœë ·í•˜ë‹¤. ë‚¨ì„± | 50-54ì„¸ëŠ” ì—¬ëŸ¬ ì¤‘ë¶„ë¥˜ì—ì„œ ìŒì˜ í¸ì°¨ê°€ ë°˜ë³µëœë‹¤.

### 1. ì£¼ìš” í¸ì°¨ ê·¸ë£¹
* ì—¬ì„± | 30-34ì„¸ëŠ” ì •ë³´ íšë“ +12.0ìœ¼ë¡œ ê°•ì , ì†Œí†µ ë° ì •ì±… í™œìš© -9.0ìœ¼ë¡œ ì•½ì ì´ ëšœë ·í•˜ë‹¤.
* ë‚¨ì„± | 50-54ì„¸ëŠ” ì „ë°˜ì ìœ¼ë¡œ ë‚®ì€ í¸ì°¨ê°€ ë°˜ë³µë˜ì–´ ë¶ˆë§Œì¡±êµ°ìœ¼ë¡œ ë¬¶ì¸ë‹¤.

### 2. ê°œì„ Â·í™•ì¥ ìš°ì„ ìˆœìœ„
* ì •ë³´ íšë“ì´ ë†’ê³  ì†Œí†µì´ ë‚®ì€ ì—¬ì„± | 30-34ì„¸ëŠ” í™•ì¥ê³¼ ë³´ì™„ ë³‘í–‰.
* ì§€ì†ì  ìŒì˜ í¸ì°¨ë¥¼ ë³´ì´ëŠ” ë‚¨ì„± | 50-54ì„¸ë¶€í„° ìš°ì„  ê°œì„ .
* ë¬¸í™”-êµìœ¡ í–¥ìœ ì—ì„œ í”ŒëŸ¬ìŠ¤ì¸ ì¡°í•©ì€ ì¶”ê°€ ì—°ê³„ ê¸°íšŒë¡œ í™œìš©.
---
"""
    return prompt.strip()


def build_ci_prompt(subset_df, mc):
    rows = []
    for _, r in subset_df.iterrows():
        combo = r.get("ì¡°í•©", "")
        delta = r.get("delta", 0)
        se = r.get("se", 0)
        rows.append(f"{combo}: í¸ì°¨ {delta:.1f}, í‘œì¤€ì˜¤ì°¨ {se:.2f}")
    table_str = "\n".join(rows)
    prompt = f"""
ì „ëµ ë³´ê³ ì„œìš© - '{mc}' í¸ì°¨ ë°•ìŠ¤í”Œë¡¯/ì‹ ë¢°ë„ í•´ì„

ì…ë ¥:
{table_str}

ìš”ì²­:
1. [ì‹œê°í™” í•´ì„]ì—ì„œ ì´ ë°•ìŠ¤í”Œë¡¯(ë˜ëŠ” í¸ì°¨+ì‹ ë¢°êµ¬ê°„ ì‹œê°í™”)ì´ ë¬´ì—‡ì„ ë‚˜íƒ€ë‚´ëŠ”ì§€ ê°œë…ê³¼ êµ¬ì„± ìš”ì†Œ(ì¤‘ë¶„ë¥˜ í¸ì°¨, error bar/ì‹ ë¢°êµ¬ê°„, ê¸°ì¤€ì„  0ì˜ ì˜ë¯¸ ë“±)ë¥¼ 200ì ë‚´ì™¸ë¡œ ì„¤ëª….
2. ì´ì–´ì„œ ê° ì¡°í•©ë³„ í¸ì°¨ì™€ í‘œì¤€ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì§ˆì  ê°•ì Â·ì•½ì (í¸ì°¨ í¬ê¸° ëŒ€ë¹„ ë¶ˆí™•ì‹¤ì„±), ë¶ˆí™•ì‹¤ì„±ì´ í° ì¡°í•© vs ì•ˆì •ì  ì¡°í•© ë¹„êµ, ê°œì… ìš°ì„ ìˆœìœ„ 3ê°œë¥¼ ì†Œì œëª© êµ¬ì¡°ë¡œ 1000ì ë‚´ì™¸ë¡œ ì‘ì„±.
-ì€ ëª¨ë‘ -ë¡œ, ìˆ«ì í•œ ìë¦¬ ì†Œìˆ˜, ì¡°í•©ëª… ë°˜ë³µ, ì†Œì œëª© í¬í•¨, ì „ì²´ ê¸¸ì´ 1200ì ë‚´ì™¸.

[ì¶œë ¥ ì˜ˆì‹œ]
---
[ì‹œê°í™” í•´ì„]
í¸ì°¨ ë°•ìŠ¤í”Œë¡¯ì€ ê° ì¡°í•©ì´ ì „ì²´ í‰ê·  ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€ì™€ ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ ë³´ì—¬ì¤€ë‹¤. ì¤‘ì‹¬ì„  0ì€ í‰ê· ì´ë©° error barê°€ ì§§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ê°€ ë†’ê³  ì´ë¥¼ ë²—ì–´ë‚œ í¸ì°¨ëŠ” ì‹¤ì§ˆì  ì°¨ì´ë¡œ í•´ì„ëœë‹¤.

### 1. ì‹¤ì§ˆì  í¸ì°¨ íŒë³„
* ì—¬ì„± | 30-34ì„¸(í¸ì°¨ 8.1, í‘œì¤€ì˜¤ì°¨ 1.0)ëŠ” ê°•í•œ ê¸ì • í¸ì°¨ì´ë©° ì•ˆì •ì ì´ë‹¤.
* ë‚¨ì„± | 50-54ì„¸(í¸ì°¨ -7.0, í‘œì¤€ì˜¤ì°¨ 2.5)ëŠ” ëšœë ·í•œ ë¶€ì • í¸ì°¨ì´ë‚˜ ë¶ˆí™•ì‹¤ì„±ì´ ë‹¤ì†Œ í¬ë‹¤.

### 2. ì•ˆì • vs ë¶ˆí™•ì‹¤
* ì—¬ì„± | 40-44ì„¸(í¸ì°¨ 4.0, í‘œì¤€ì˜¤ì°¨ 3.2)ëŠ” í¸ì°¨ëŠ” ìˆìœ¼ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì£¼ì˜.
* ì—¬ì„± | 30-34ì„¸ëŠ” í¸ì°¨ í¬ê³  ì˜¤ì°¨ ì‘ì•„ ê°œì… ê°€ì¹˜ê°€ ë†’ë‹¤.

### 3. ì „ëµì  ìš°ì„ ìˆœìœ„
* ì—¬ì„± | 30-34ì„¸ëŠ” ê°•ì  í™•ì¥ ìš°ì„ .
* ë‚¨ì„± | 50-54ì„¸ëŠ” ì•½ì  ê°œì„  ìš°ì„ .
* ë¶ˆí™•ì‹¤í•œ ì—¬ì„± | 40-44ì„¸ëŠ” ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ í›„ íŒë‹¨ í•„ìš”.
---
"""
    return prompt.strip()
@st.cache_data(show_spinner=False)
def extract_theme_table_long(responses: list[str], batch_size: int = 30) -> pd.DataFrame:
    all_parts = []
    for i in range(0, len(responses), batch_size):
        batch = responses[i:i+batch_size]
        messages = make_theme_messages(batch)
        try:
            resp = safe_chat_completion(model="gpt-4.1", messages=messages, temperature=0.1, max_tokens=800)
            content = resp.choices[0].message.content.strip()
        except Exception as e:
            st.error(f"ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            continue
        df_part = parse_markdown_table(content, ['ì£¼ì œëª…', 'ëŒ€í‘œ í‚¤ì›Œë“œ', 'ìš”ì•½'])
        all_parts.append(df_part)
    if all_parts:
        result = pd.concat(all_parts, ignore_index=True)
    else:
        result = pd.DataFrame(columns=['ì£¼ì œëª…', 'ëŒ€í‘œ í‚¤ì›Œë“œ', 'ìš”ì•½'])
    # 6ê°œ ì£¼ì œ ë³´ì¥
    topics = ['ê³µê°„ ë° ì‹œì„¤','ìë£Œ í™•ì¶©','í”„ë¡œê·¸ë¨ ë‹¤ì–‘í™”','ìš´ì˜ ë° ì‹œìŠ¤í…œ','ì§ì› ë° ì‘ëŒ€','ê¸°íƒ€']
    existing = result['ì£¼ì œëª…'].tolist()
    for t in topics:
        if t not in existing:
            result = pd.concat([result, pd.DataFrame([[t, "", ""]], columns=result.columns)], ignore_index=True)
    result = result.set_index('ì£¼ì œëª…').loc[topics].reset_index()
    return result

def extract_sentiment_table(responses, theme_df, batch_size=50):
    all_tables = []
    for i in range(0, len(responses), batch_size):
        batch = [r for r in responses[i:i+batch_size] if isinstance(r, str) and r.strip()]
        if not batch:
            continue
        messages = make_sentiment_messages(batch, theme_df)
        try:
            resp = openai.ChatCompletion.create(
                model="gpt-4.1-mini",
                messages=messages,
                temperature=0.1,
                max_tokens=900
            )
            content = resp.choices[0].message.content.strip()
        except Exception as e:
            logging.warning(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            continue

        print("LLM ê°ì„± ì¶œë ¥ (ë””ë²„ê·¸):\n", content)
        all_tables.append(content)

    def table_to_df(table_text):
        lines = [l for l in table_text.split('\n') if "|" in l and "---" not in l]
        records = []
        for line in lines:
            parts = [p.strip() for p in line.strip().split("|")[1:-1]]
            if len(parts) == 3:
                records.append(parts)
        if not records:
            return pd.DataFrame(columns=['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])
        return pd.DataFrame(records, columns=['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])

    if all_tables:
        dfs = [table_to_df(tbl) for tbl in all_tables]
        result_df = pd.concat(dfs, ignore_index=True)
        result_df = result_df.drop_duplicates(subset=["ì£¼ì œëª…", "ê°ì„±"]).reset_index(drop=True)
    else:
        result_df = pd.DataFrame(columns=['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])
    return result_df

@st.cache_data(show_spinner=False)
def extract_sentiment_table_long(responses: list[str], theme_df: pd.DataFrame, batch_size: int = 50) -> pd.DataFrame:
    all_parts = []
    if not responses:
        return pd.DataFrame(columns=['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])
    for i in range(0, len(responses), batch_size):
        batch = [r for r in responses[i:i+batch_size] if isinstance(r, str) and r.strip()]
        if not batch:
            continue
        messages = make_sentiment_messages(batch, theme_df)
        try:
            resp = safe_chat_completion(model="gpt-4.1-mini", messages=messages, temperature=0.1, max_tokens=900)
            content = resp.choices[0].message.content.strip()
        except Exception as e:
            st.error(f"ê°ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            logging.exception("LLM í˜¸ì¶œ ì˜¤ë¥˜")
            continue

        logging.info(f"[ê°ì„± LLM ì‘ë‹µ]\n{content}")

        df_part = parse_markdown_table(content, ['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])
        if not df_part.empty:
            all_parts.append(df_part)
    if all_parts:
        result = pd.concat(all_parts, ignore_index=True).drop_duplicates(subset=['ì£¼ì œëª…','ê°ì„±'])
    else:
        result = pd.DataFrame(columns=['ì£¼ì œëª…', 'ê°ì„±', 'í‘œí˜„ ì–‘ìƒ ìš”ì•½'])
    return result.reset_index(drop=True)

# ---------- ìì—°ì–´ ì§ˆì˜  ì¸ì‚¬ì´íŠ¸ íŒŒì´í”„ë¼ì¸ ----------


# 1. ìŠ¤í™ ìŠ¤í‚¤ë§ˆ ì •ì˜ (ê²€ì¦+ì •ê·œí™”ìš©)
SPEC_SCHEMA = {
    "type": "object",
    "properties": {
        "chart": {"type": ["string", "null"], "enum": ["bar", "line", "heatmap", "radar", "delta_bar", "grouped_bar", None]},
        "x": {"type": ["string", "null"]},
        "y": {"type": ["string", "null"]},
        "groupby": {"type": ["string", "array", "null"]},
        "filters": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "col": {"type": "string"},
                    "op": {"type": "string", "enum": ["contains", "==", "=", "in"]},
                    "value": {}
                },
                "required": ["col", "op", "value"]
            }
        },
        "focus": {"type": "string"},
    },
    "additionalProperties": False
}

DEFAULT_SPEC = {
    "chart": None,
    "x": None,
    "y": None,
    "groupby": None,
    "filters": [],
    "focus": ""
}

def normalize_and_validate_spec(raw_spec: dict, question: str) -> dict:
    # start from default then overlay
    spec = {**DEFAULT_SPEC, **{k: raw_spec.get(k) for k in DEFAULT_SPEC.keys() if k in raw_spec}}
    if not spec.get("focus"):
        spec["focus"] = question
    # coerce groupby list to single if necessary
    gb = spec.get("groupby")
    if isinstance(gb, list) and len(gb) == 1:
        spec["groupby"] = gb[0]
    try:
        jsonschema.validate(instance=spec, schema=SPEC_SCHEMA)
    except Exception as e:
        # sloppy fixups: drop invalid filters, coerce bad chart to None
        if spec.get("chart") not in {"bar", "line", "heatmap", "radar", "delta_bar", "grouped_bar", None}:
            spec["chart"] = None
        valid_filters = []
        for f in spec.get("filters", []):
            if isinstance(f, dict) and "col" in f and "op" in f and "value" in f:
                valid_filters.append(f)
        spec["filters"] = valid_filters
        # fallback for other violations: ensure required keys present
    return spec

# 2. LLM + rule hybrid parser
def parse_nl_query_to_spec_v2(question: str) -> dict:
    system_prompt = """
ë„ˆëŠ” ì„¤ë¬¸ ë°ì´í„° ìì—°ì–´ ì§ˆì˜ë¥¼ êµ¬ì¡°í™”ëœ JSON ìŠ¤í™ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” íŒŒì„œì•¼.
ë°˜í™˜ì€ ì˜¤ì§ JSON í•˜ë‚˜. ê°€ëŠ¥í•œ ê²½ìš° ì•„ë˜ í•„ë“œë¥¼ ì±„ì›Œë¼.
í•„ë“œ ì„¤ëª…:
- x: ì£¼ ì¶• (ì»¬ëŸ¼ëª… ë˜ëŠ” ì¤‘ë¶„ë¥˜)
- y: (ê°€ëŠ¥í•˜ë©´) ë¹„êµ ì¶•
- groupby: ë¹„êµ ê¸°ì¤€ (ë‹¨ì¼ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
- focus: ì‚¬ìš©ìì˜ ì˜ë„ ìš”ì•½

ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì´ë©´ nullì„ ë„£ê³ , focusëŠ” í•­ìƒ ì§ˆë¬¸ì„ ì••ì¶•í•´ì„œ ì§§ê²Œ ì‘ì„±í•´.
ì¶œë ¥ì€ ì½”ë“œë¸”ëŸ­ ì—†ì´ ìˆœìˆ˜ JSONë§Œ.
"""
    try:
        resp = safe_chat_completion(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            temperature=0.2,
            max_tokens=400
        )
        content = resp.choices[0].message.content.strip()
        content = re.sub(r"^```json\s*|\s*```$", "", content, flags=re.IGNORECASE).strip()
        raw = json.loads(content)
    except Exception:
        # fallback rule-based quick parser
        raw = {"x": None, "y": None, "groupby": None, "focus": question}
        lower = question.lower()
        if "ë¹„êµ" in lower and "ë„ì„œê´€" in lower:
            raw["groupby"] = "SQ4"
        if "ì—°ë ¹" in lower or "ë‚˜ì´" in lower:
            raw["x"] = "SQ2_GROUP"
    # enforce no chart/filters regardless of LLM output
    raw["chart"] = None
    raw["filters"] = []
    final_spec = normalize_and_validate_spec(raw, question)
    final_spec["filters"] = []  # double ensure
    return final_spec

# 3. í†µí•©ëœ ë¬¸í•­ ì¶”ì¶œ (ê¸°ì¡´ ë‘ ë²„ì „ í•©ì³ì„œ í•˜ë‚˜ë¡œ)
def extract_questions_used(spec: dict, df_full: pd.DataFrame, df_filtered: pd.DataFrame):
    used_full = []

    def add_val(val):
        if isinstance(val, list):
            for v in val:
                used_full.append(v)
        elif val:
            used_full.append(val)

    for key in ("x", "y", "groupby"):
        add_val(spec.get(key))

    for f in spec.get("filters", []):
        col = f.get("col")
        if col:
            used_full.append(col)

    focus = (spec.get("focus") or "").lower()
    if any(k in focus for k in ["ì¤‘ë¶„ë¥˜", "ê°•ì ", "ì•½ì ", "ì „ì²´ í‰ê· ", "í”„ë¡œíŒŒì¼", "ë¹„êµ"]):
        for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
            cols = [c for c in df_filtered.columns if predicate(c)]
            used_full.extend(cols)

    # expand midcategory names if provided in place of column
    expanded = []
    for c in used_full:
        if c and c not in df_full.columns:
            for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
                if c.strip().lower() == mid.strip().lower():
                    expanded += [col for col in df_full.columns if predicate(col)]
        else:
            expanded.append(c)
    # dedupe preserving order
    seen = set()
    used_full_unique = []
    for c in expanded:
        if c and c not in seen:
            seen.add(c)
            used_full_unique.append(c)

    # extract codes
    used_codes = []
    seen_codes = set()
    for col in used_full_unique:
        code = extract_question_code(col)
        if code not in seen_codes:
            seen_codes.add(code)
            used_codes.append(code)

    return used_full_unique, used_codes

def generate_explanation_from_spec(df_subset: pd.DataFrame, spec: dict, computed_metrics: dict, extra_group_stats=None):
    focus = spec.get("focus", "ê¸°ë³¸ ìš”ì•½")
    parts = []
    if "overall_mid_scores" in computed_metrics:
        mids = computed_metrics["overall_mid_scores"]
        parts.append("ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê· : " + ", ".join(f"{k} {v:.1f}" for k, v in mids.items()))
    if "deltas" in computed_metrics:
        deltas = computed_metrics["deltas"]
        delta_str = ", ".join(f"{k} {v:+.1f}" for k, v in deltas.items())
        parts.append("ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨: " + delta_str)
    if "top_segments" in computed_metrics:
        top = computed_metrics["top_segments"]
        parts.append("ì£¼ìš” ì„¸ê·¸ë¨¼íŠ¸/ì¡°í•©: " + "; ".join(f"{t['label']} (n={t['n']})" for t in top))
    if extra_group_stats:
        summary_lines = []
        for group_label, mids in extra_group_stats.items():
            for mid, stats in mids.items():
                line = f"{group_label}ì˜ '{mid}' í‰ê·  {stats.get('mean')}, ì „ì²´ ëŒ€ë¹„ {stats.get('delta_vs_overall'):+.1f}"
                if stats.get("p_value_vs_rest") is not None:
                    line += f", p={stats['p_value_vs_rest']}"
                if stats.get("cohen_d_vs_rest") is not None:
                    line += f", d={stats['cohen_d_vs_rest']}"
                summary_lines.append(line)
        parts.append("ê·¸ë£¹ ë¹„êµ: " + " / ".join(summary_lines[:3]))  # ê¸¸ì´ ì œí•œ ê°ì•ˆ

    summary_context = "\n".join(parts)

    prompt = f"""
    ë„ˆëŠ” ì „ëµ ë³´ê³ ì„œ ì‘ì„±ìë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì§ˆì˜ í¬ì»¤ìŠ¤ë¥¼ ì°¸ê³ í•´ ëª…í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤˜.

    ì‚¬ìš©ì ì§ˆì˜ í¬ì»¤ìŠ¤: {spec.get('focus', '')}
    ì°¸ê³ í•œ ë¬¸í•­ ì½”ë“œ: {', '.join(computed_metrics.get('questions_used_codes', []))}

    ë°ì´í„° ìš”ì•½:
    {summary_context}

    ìš”ì²­:
    1. ì£¼ìš” ê´€ì°° íŒ¨í„´ 2~3ê°œë¥¼ ê¸°ìˆ í•´ì¤˜.
    2. ê°•ì ê³¼ ì•½ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¡°í•©ëª…ì´ë‚˜ í•­ëª©ëª…ì„ ì“°ë©´ì„œ ìˆ«ìì™€ í•¨ê»˜ ì„¤ëª…í•´ì¤˜.
    3. ìš°ì„  ê°œì…/í™•ì¥í• ë§Œí•œ í–‰ë™ ì œì•ˆ 2ê°œë¥¼ ì œì‹œí•´ì¤˜.
    4. ì „ì²´ ê¸¸ì´ 500~1000ì, ë¹„ì¦ˆë‹ˆìŠ¤ í†¤, ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, '-' ì‚¬ìš©.

    ì¶œë ¥ë§Œ í…ìŠ¤íŠ¸ë¡œ í•´ì¤˜.
    """

    explanation = call_gpt_for_insight(prompt)
    explanation = explanation.replace("~", "-")
    render_insight_card("GPT ìƒì„±í˜• í•´ì„", explanation, key="explanation")


def apply_filters(df: pd.DataFrame, filters: list):
    dff = df.copy()
    for f in filters:
        col = f.get("col")
        op = f.get("op", "==")
        val = f.get("value")
        if col not in dff.columns or val is None:
            continue
        if op in ("==", "="):
            dff = dff[dff[col].astype(str) == str(val)]
        elif op == "in" and isinstance(val, list):
            dff = dff[dff[col].astype(str).isin([str(v) for v in val])]
        elif op == "contains":
            dff = dff[dff[col].astype(str).str.contains(str(val), na=False)]
    return dff

# ---- 1. í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜ (ëª¨ë¸ ë¶„ê¸° í¬í•¨) ----
def build_explanation_from_spec(spec: dict, computed_metrics: dict, extra_group_stats=None) -> tuple[str, str]:
    focus = spec.get("focus", "ê¸°ë³¸ ìš”ì•½")
    q_codes = computed_metrics.get("questions_used_codes", [])

    model = select_model_for_explanation(spec, computed_metrics)

    # ìš”ì•½ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    parts = []
    if "overall_mid_scores" in computed_metrics:
        mids = computed_metrics["overall_mid_scores"]
        parts.append("ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê· : " + ", ".join(f"{k} {v:.1f}" for k, v in mids.items()))
    if "deltas" in computed_metrics:
        deltas = computed_metrics["deltas"]
        delta_str = ", ".join(f"{k} {v:+.1f}" for k, v in deltas.items())
        parts.append("ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨: " + delta_str)
    if "top_segments" in computed_metrics:
        top = computed_metrics["top_segments"]
        parts.append("ì£¼ìš” ì„¸ê·¸ë¨¼íŠ¸/ì¡°í•©: " + "; ".join(f"{t['label']} (n={t['n']})" for t in top))
    if extra_group_stats:
        summary_lines = []
        for group_label, mids in extra_group_stats.items():
            for mid, stats in mids.items():
                line = f"{group_label}ì˜ '{mid}' í‰ê·  {stats.get('mean')}, ì „ì²´ ëŒ€ë¹„ {stats.get('delta_vs_overall'):+.1f}"
                if stats.get("p_value_vs_rest") is not None:
                    line += f", p={stats['p_value_vs_rest']}"
                if stats.get("cohen_d_vs_rest") is not None:
                    line += f", d={stats['cohen_d_vs_rest']}"
                summary_lines.append(line)
        parts.append("ê·¸ë£¹ ë¹„êµ: " + " / ".join(summary_lines[:3]))

    summary_context = "\n".join(parts)

    prompt = f"""
ë„ˆëŠ” ì „ëµ ë³´ê³ ì„œ ì‘ì„±ìë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì§ˆì˜ í¬ì»¤ìŠ¤ë¥¼ ì°¸ê³ í•´ ëª…í™•í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤˜.

ì‚¬ìš©ì ì§ˆì˜ í¬ì»¤ìŠ¤: {focus}
ì°¸ê³ í•œ ë¬¸í•­ ì½”ë“œ: {', '.join(q_codes)}

ë°ì´í„° ìš”ì•½:
{summary_context}

ìš”ì²­:
1. ì£¼ìš” ê´€ì°° íŒ¨í„´ 2~3ê°œë¥¼ ê¸°ìˆ í•´ì¤˜.
2. ê°•ì ê³¼ ì•½ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¡°í•©ëª…ì´ë‚˜ í•­ëª©ëª…ì„ ì“°ë©´ì„œ ìˆ«ìì™€ í•¨ê»˜ ì„¤ëª…í•´ì¤˜.
3. ìš°ì„  ê°œì…/í™•ì¥í• ë§Œí•œ í–‰ë™ ì œì•ˆ 2ê°œë¥¼ ì œì‹œí•´ì¤˜.
4. ì „ì²´ ê¸¸ì´ 500~1000ì, ë¹„ì¦ˆë‹ˆìŠ¤ í†¤, ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, '-' ì‚¬ìš©.

ì¶œë ¥ì€ ì˜¤ì§ í…ìŠ¤íŠ¸ë¡œë§Œ í•´ì¤˜.
"""
    max_tokens = 1000 if model == "gpt-4.1" else 700

    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì „ëµ ë¦¬í¬íŠ¸ ì‘ì„±ìì´ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì•¼ í•œë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=max_tokens,
        )
        explanation = resp.choices[0].message.content.strip()
    except Exception as e:
        logging.warning(f"GPT í˜¸ì¶œ ì‹¤íŒ¨ ({model}): {e}")
        explanation = f"GPT í•´ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"

    explanation = explanation.replace("~", "-")
    return explanation.strip(), model

# ---- 2. ë Œë”ë§ ë‹´ë‹¹ í•¨ìˆ˜ (ê¸°ì¡´ ì¹´ë“œ ìŠ¤íƒ€ì¼ ìœ ì§€) ----
def render_explanation_from_spec(title: str, explanation_text: str, model: str, key: str = None):
    # ëª¨ë¸ ì •ë³´ë¥¼ ë§¨ ì•ì— ë¶™ì—¬ì„œ ë³´ì—¬ì¤Œ
    decorated = f"**ì‚¬ìš© ëª¨ë¸:** {model}\n\n{explanation_text}"
    render_insight_card(title, decorated, key=key)
def normalize_chart_choice(raw_chart):
    # ì•ˆì „í•˜ê²Œ í•˜ë‚˜ì˜ scalarë¡œ ì •ë¦¬
    if isinstance(raw_chart, list):
        return raw_chart[0] if raw_chart else None
    if raw_chart in {None, "bar", "line", "heatmap", "radar", "delta_bar", "grouped_bar"}:
        return raw_chart
    return None


def handle_nl_question_v2(df: pd.DataFrame, question: str):
    st.markdown("## ìì—°ì–´ ì§ˆì˜ ê²°ê³¼")
    st.markdown(f"**ì›ë¬¸ ì§ˆì˜:** {question}")

    q_hash = hashlib.sha256(question.strip().encode("utf-8")).hexdigest()
    cache_key = f"nlq_cache_{q_hash}"
    cached = st.session_state.get(cache_key)

    # íŒŒì‹± ë° ì •ê·œí™” (ì°¨íŠ¸/í•„í„° ê´€ë ¨ ì œê±°)
    try:
        spec = parse_nl_query_to_spec_v2(question)
    except Exception as e:
        logging.error(f"ìì—°ì–´ ì§ˆì˜ íŒŒì‹± ì‹¤íŒ¨: {e}", exc_info=True)
        spec = {"x": None, "y": None, "groupby": None, "focus": question, "chart": None, "filters": []}
        spec = normalize_and_validate_spec(spec, question)
        spec["filters"] = []

    st.markdown("### íŒŒì‹±ëœ ìŠ¤í™ (ìˆ˜ë™ ìˆ˜ì • ê°€ëŠ¥)")
    with st.expander("ìŠ¤í™ ìƒì„¸ ë° ìˆ˜ë™ ìˆ˜ì •", expanded=True):
        # focusë§Œ ë…¸ì¶œ
        spec["focus"] = st.text_input("ì§ˆì˜ ìš”ì•½ (focus)", value=spec.get("focus") or question, key=f"nlq_focus_{q_hash}")

        if spec.get("groupby"):
            st.markdown(f"- ë¹„êµ ê¸°ì¤€ (groupby): {spec['groupby']}")
        if spec.get("x"):
            st.markdown(f"- x ì¶•: {spec['x']}")
        if spec.get("y"):
            st.markdown(f"- y ì¶•: {spec['y']}")

        # ê°•ì œ: í•„í„°ëŠ” í•­ìƒ ì—†ìŒ
        spec["filters"] = []

        # ì¬ê²€ì¦
        spec = normalize_and_validate_spec(spec, question)
        spec["filters"] = []  # ë‹¤ì‹œ ë³´ì¥

        # ìš”ì•½ í‘œì‹œ
        readable = []
        if spec.get("x"):
            readable.append(f"x: {spec['x']}")
        if spec.get("y"):
            readable.append(f"y: {spec['y']}")
        if spec.get("groupby"):
            readable.append(f"groupby: {spec['groupby']}")
        readable.append(f"focus: {spec.get('focus')}")
        st.markdown("**ìš”ì•½ëœ ìŠ¤í™ í•´ì„:** " + " | ".join(readable))

    use_cache = cached and cached.get("spec") == spec
    if use_cache:
        logging.info("ìºì‹œëœ ê²°ê³¼ ì‚¬ìš©")
        st.info("ì´ì „ ë™ì¼ ì§ˆì˜/ìŠ¤í™ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        computed_metrics = cached["computed_metrics"]
        extra_group_stats = cached.get("extra_group_stats")
        explanation_text = cached["explanation_text"]
        used_model = cached.get("used_model", "unknown")
        render_explanation_from_spec(f"ì¬ì‚¬ìš©ëœ GPT ìƒì„±í˜• í•´ì„ ({used_model})", explanation_text, model=used_model, key=f"nlq-insight-cached-{q_hash}")
        df_filtered = df  # í•„í„° ì—†ì´ ì „ì²´
    else:
        df_filtered = df  # í•„í„° ì—†ì´ ì „ì²´

        if df_filtered.empty:
            st.warning("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            questions_used_full, questions_used_codes = extract_questions_used(spec, df, df_filtered)
        except Exception:
            questions_used_full, questions_used_codes = [], []

        if questions_used_codes:
            unique_codes = []
            seen = set()
            for c in questions_used_codes:
                if c not in seen:
                    seen.add(c)
                    unique_codes.append(c)
            st.markdown("**ì°¸ê³ í•œ ë¬¸í•­ (ë¬¸í•­ë²ˆí˜¸):** " + ", ".join(unique_codes))

        overall_mid_scores = compute_midcategory_scores(df_filtered)
        overall_mid_dict = {k: float(v) for k, v in overall_mid_scores.items()} if not overall_mid_scores.empty else {}
        global_mid_scores = compute_midcategory_scores(df)
        deltas = {
            k: overall_mid_dict.get(k, 0) - float(global_mid_scores.get(k, overall_mid_dict.get(k, 0)))
            for k in overall_mid_dict
        }

        top_segments = []
        gb = spec.get("groupby")
        # groupby ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        if isinstance(gb, list):
            gb_list = gb
        elif gb:
            gb_list = [gb]
        else:
            gb_list = []

        valid_gb = [g for g in gb_list if g in df_filtered.columns]

        if valid_gb:
            def make_combo_label(row):
                return " | ".join(str(row[g]) for g in valid_gb)
            df_filtered["_gb_combined"] = df_filtered.apply(make_combo_label, axis=1)
            counts = df_filtered["_gb_combined"].value_counts().nlargest(3)
            for label, n in counts.items():
                subset = df_filtered[df_filtered["_gb_combined"] == label]
                profile = compute_midcategory_scores(subset)
                top_segments.append({
                    "label": label,
                    "n": int(n),
                    "profile": {k: float(v) for k, v in profile.items()}
                })
            df_filtered.drop(columns=["_gb_combined"], inplace=True)
        else:
            top_segments.append({
                "label": "ì „ì²´ ë°ì´í„°",
                "n": len(df_filtered),
                "profile": overall_mid_dict
            })

        computed_metrics = {
            "overall_mid_scores": overall_mid_dict,
            "deltas": deltas,
            "top_segments": top_segments,
            "questions_used_full": questions_used_full,
            "questions_used_codes": questions_used_codes
        }

        extra_group_stats = None
        if isinstance(gb, str) and gb in df_filtered.columns:
            extra_group_stats = compare_midcategory_by_group(df_filtered, gb)

        # explanation ìƒì„±/ë Œë”ë§ (í•­ìƒ ìˆ˜í–‰)
        try:
            explanation_text, used_model = build_explanation_from_spec(spec, computed_metrics, extra_group_stats=extra_group_stats)
        except Exception as e:
            logging.error("build_explanation_from_spec ì‹¤íŒ¨", exc_info=True)
            explanation_text = f"GPT í•´ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
            used_model = "error"

        if not explanation_text:
            explanation_text = "(í•´ì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.)"

        render_explanation_from_spec(f"GPT ìƒì„±í˜• í•´ì„ ({used_model})", explanation_text, model=used_model, key=f"nlq-insight-{q_hash}")

        st.session_state[cache_key] = {
            "spec": spec,
            "computed_metrics": computed_metrics,
            "extra_group_stats": extra_group_stats,
            "explanation_text": explanation_text,
            "used_model": used_model
        }



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¸ê·¸ë¨¼íŠ¸ íŒŒìƒ/ë§¤í•‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_derived_columns(df):
    df = df.copy()
    if "DQ1_FREQ" not in df.columns:
        dq1_cols = [c for c in df.columns if "DQ1" in c]
        if dq1_cols:
            dq1_col = dq1_cols[0]
            monthly = pd.to_numeric(df[dq1_col].astype(str).str.extract(r"(\d+\.?\d*)")[0], errors="coerce")
            yearly = monthly * 12
            bins = [0,12,24,48,72,144,1e10]
            labels = ["0~11íšŒ: ì—° 1íšŒ ë¯¸ë§Œ", "12~23íšŒ: ì›” 1íšŒ", "24~47íšŒ: ì›” 2~4íšŒ", "48~71íšŒ: ì£¼ 1íšŒ", "72~143íšŒ: ì£¼ 2~3íšŒ", "144íšŒ ì´ìƒ: ê±°ì˜ ë§¤ì¼"]
            df["DQ1_FREQ"] = pd.cut(yearly, bins=bins, labels=labels, right=False)
    if "DQ2_YEARS" not in df.columns or "DQ2_YEARS_GROUP" not in df.columns:
        dq2_cols = [c for c in df.columns if "DQ2" in c]
        if dq2_cols:
            dq2_col = dq2_cols[0]
            def parse_years(s):
                s = str(s)
                m = re.match(r'^(\d+)\s*ë…„\s*(\d+)\s*ê°œì›”$', s)
                if m: return int(m.group(1)) + (1 if int(m.group(2)) > 0 else 0)
                m = re.match(r'^(\d+)\s*ë…„$', s)
                if m: return int(m.group(1))
                m = re.match(r'^(\d+)\s*ê°œì›”$', s)
                if m: return 1
                return None
            years = df[dq2_col].dropna().apply(parse_years)
            df["DQ2_YEARS"] = years

        def year_group(y):
            if pd.isna(y):
                return None
            y = int(y)
            if y < 5:
                return "1~4ë…„"
            elif y < 10:
                return "5~9ë…„"
            elif y < 15:
                return "10~14ë…„"
            elif y < 20:
                return "15~19ë…„"
            else:
                return "20ë…„ ì´ìƒ"
        df["DQ2_YEARS_GROUP"] = df["DQ2_YEARS"].apply(year_group)

    if "DQ4_1ST" not in df.columns:
        dq4_cols = [c for c in df.columns if ("DQ4" in c) and ("1ìˆœìœ„" in c)]
        if dq4_cols:
            df["DQ4_1ST"] = df[dq4_cols[0]]

    if "SQ2_GROUP" not in df.columns:
        sq2_cols = [c for c in df.columns if "SQ2" in c]
        if sq2_cols:
            sq2_col = sq2_cols[0]
            data = df[sq2_col].dropna().astype(str).str.extract(r'(\d+)')
            data.columns = ['age']
            data['age'] = pd.to_numeric(data['age'], errors='coerce')
            def age_group(age):
                if pd.isna(age):
                    return None
                age = int(age)
                if age < 15:
                    return '14ì„¸ ì´í•˜'
                elif age >= 80:
                    return '80ì„¸ ì´ìƒ'
                else:
                    base = (age // 5) * 5
                    return f"{base}~{base+4}ì„¸"
            df["SQ2_GROUP"] = data['age'].apply(age_group)
    return df

def get_segment_columns(df, key):
    if key == "SQ2":
        # SQ2 ì„ íƒ ì‹œ ë¬´ì¡°ê±´ 5ë…„ ë‹¨ìœ„ ê·¸ë£¹ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        if "SQ2_GROUP" in df.columns:
            return ["SQ2_GROUP"]
        else:
            # ì˜ˆì™¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ raw ì»¬ëŸ¼ì´ë¼ë„ ìµœì†Œ í•˜ë‚˜ëŠ” ë°˜í™˜
            return [c for c in df.columns if c.startswith("SQ2")][:1]
    elif key == "DQ2":
        if "DQ2_YEARS_GROUP" in df.columns:
            return ["DQ2_YEARS_GROUP"]
        elif "DQ2_YEARS" in df.columns:
            return ["DQ2_YEARS"]
        return [col for col in df.columns if "DQ2" in col]
    elif key == "DQ4":
        return [col for col in df.columns if ("DQ4" in col) and ("1ìˆœìœ„" in col)]
    elif key == "DQ1":
        if "DQ1_FREQ" in df.columns:
            return ["DQ1_FREQ"]
        return [col for col in df.columns if "DQ1" in col]
    else:
        return [col for col in df.columns if key in col]

SEGMENT_OPTIONS = [
    {"label": "SQ1. ì„±ë³„",        "key": "SQ1"},
    {"label": "SQ2. ì—°ë ¹",        "key": "SQ2"},
    {"label": "SQ3. ê±°ì£¼ì§€",      "key": "SQ3"},
    {"label": "SQ4. ì£¼ ì´ìš© ë„ì„œê´€", "key": "SQ4"},
    {"label": "SQ5. ì£¼ë¡œ ì´ìš© ì„œë¹„ìŠ¤", "key": "SQ5"},
    {"label": "DQ1. ì›”í‰ê·  ì´ìš© ë¹ˆë„", "key": "DQ1"},
    {"label": "DQ2. ì´ìš©ê¸°ê°„", "key": "DQ2"},
    {"label": "DQ4. (1ìˆœìœ„)ì´ìš©ëª©ì ", "key": "DQ4"},
]
MIDCAT_MAP = {
    "ê³µê°„ ë° ì´ìš©í¸ì˜ì„±": "Q1-",
    "ì •ë³´ íšë“ ë° í™œìš©": "Q2-",
    "ì†Œí†µ ë° ì •ì±… í™œìš©": "Q3-",
    "ë¬¸í™”Â·êµìœ¡ í–¥ìœ ": "Q4-",
    "ì‚¬íšŒì  ê´€ê³„ í˜•ì„±": "Q5-",
    "ê°œì¸ì˜ ì‚¶ê³¼ ì—­ëŸ‰": "Q6-",
    "ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­": "Q9-D-3",
    "ê³µìµì„± ë° ê¸°ì—¬ë„": ["Q7-", "Q8-"],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹œê°í™” í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def plot_age_histogram_with_labels(df, question):
    data = df[question].dropna().astype(str).str.extract(r'(\d+)')
    data.columns = ['age']
    data['age'] = pd.to_numeric(data['age'], errors='coerce').dropna()

    def age_group(age):
        if age < 15:
            return '14ì„¸ ì´í•˜'
        elif age >= 80:
            return '80ì„¸ ì´ìƒ'
        else:
            return f"{(age//5)*5}~{(age//5)*5+4}ì„¸"

    data['group'] = data['age'].apply(age_group)
    grouped = data['group'].value_counts().sort_index()
    percent = (grouped / grouped.sum() * 100).round(1)

    fig = go.Figure(go.Bar(
        x=grouped.index, y=grouped.values,
        text=grouped.values, textposition='outside',
        marker_color=get_qualitative_colors(1)[0]
    ))
    fig.update_layout(
        title=question, yaxis_title="ì‘ë‹µ ìˆ˜",
        bargap=0.1, height=450, margin=dict(t=40, b=10)
    )

    table_df = pd.DataFrame({'ì‘ë‹µ ìˆ˜': grouped, 'ë¹„ìœ¨ (%)': percent}).T
    return fig, table_df

def plot_bq2_bar(df, question):
    data = df[question].dropna().astype(str)
    counts_raw = data.value_counts()
    percent_raw = (counts_raw / counts_raw.sum() * 100).round(1)

    categories_raw = counts_raw.index.tolist()
    categories = [label.split('. ', 1)[-1] for label in categories_raw]
    counts = counts_raw.values
    percent = percent_raw.values

    wrapped_labels = [wrap_label(remove_parentheses(label), width=10) for label in categories]

    colors = get_qualitative_colors(len(categories))
    fig = go.Figure(go.Bar(
        x=categories,
        y=counts,
        text=counts,
        textposition='outside',
        marker_color=colors
    ))
    y_max = counts.max() + 20
    fig.update_layout(
        title=dict(text=question, font=dict(size=16)),
        yaxis=dict(title="ì‘ë‹µ ìˆ˜", range=[0, y_max]),
        height=450,
        margin=dict(t=50, b=100),
        xaxis_tickangle=-30
    )

    table_df = pd.DataFrame(
        [counts, percent],
        index=["ì‘ë‹µ ìˆ˜", "ë¹„ìœ¨ (%)"],
        columns=wrapped_labels
    )
    return fig, table_df

def plot_sq4_custom_bar(df, question):
    data = df[question].dropna().astype(str)
    cats = sorted(data.unique())
    counts = data.value_counts().reindex(cats).fillna(0).astype(int)
    percent = (counts / counts.sum() * 100).round(1)
    display_labels = [wrap_label(remove_parentheses(x), 10) for x in cats]

    fig = go.Figure()
    colors = get_qualitative_colors(len(cats))
    for i, cat in enumerate(cats):
        fig.add_trace(go.Bar(
            x=[percent[cat]], y=[question],
            orientation='h', name=remove_parentheses(cat),
            marker_color=colors[i],
            text=f"{percent[cat]}%", textposition='inside'
        ))
    fig.update_layout(
        barmode='stack', showlegend=True,
        legend=dict(orientation='h', y=-0.5, x=0.5, xanchor='center', traceorder='reversed'),
        title=dict(text=question, font=dict(size=16)),
        yaxis=dict(showticklabels=False),
        height=250, margin=dict(t=40, b=100)
    )

    table_df = pd.DataFrame(
        [counts.values, percent.values],
        index=["ì‘ë‹µ ìˆ˜", "ë¹„ìœ¨ (%)"],
        columns=display_labels
    )
    return fig, table_df

def plot_categorical_stacked_bar(df, question):
    data = df[question].dropna().astype(str)
    categories_raw = sorted(data.unique())
    display_labels = [label.split('. ', 1)[-1] for label in categories_raw]

    counts = data.value_counts().reindex(categories_raw).fillna(0).astype(int)
    percent = (counts / counts.sum() * 100).round(1)

    fig = go.Figure()
    colors = get_qualitative_colors(len(display_labels))
    for i, (raw_cat, label) in enumerate(zip(categories_raw, display_labels)):
        fig.add_trace(go.Bar(
            x=[percent[raw_cat]],
            y=[question],
            orientation='h',
            name=label,
            marker=dict(color=colors[i]),
            text=f"{percent[raw_cat]}%",
            textposition='inside',
            insidetextanchor='middle',
            hoverinfo='x+name'
        ))


    fig.update_layout(
        barmode='stack',
        showlegend=True,
        legend=dict(
            orientation='h',
            yanchor='bottom', y=-1,
            xanchor='center', x=0.5,
            traceorder='reversed'
        ),
        title=dict(text=question, font=dict(size=16)),
        yaxis=dict(showticklabels=False),
        height=250, margin=dict(t=40, b=100)
    )

    table_df = pd.DataFrame(
        [counts.values, percent.values],
        index=["ì‘ë‹µ ìˆ˜", "ë¹„ìœ¨ (%)"],
        columns=display_labels
    )
    return fig, table_df

def plot_stacked_bar_with_table(df, question):
    data = pd.to_numeric(df[question].dropna(), errors='coerce').dropna().astype(int)
    order = [1,2,3,4,5,6,7]
    counts = data.value_counts().reindex(order, fill_value=0)
    percent = (counts / counts.sum() * 100).round(1)

    colors = {
        1: "#d73027", 2: "#fc8d59", 3: "#fee090",
        4: "#dddddd", 5: "#91bfdb", 6: "#4575b4", 7: "#313695"
    }
    fig = go.Figure()
    for v in order:
        fig.add_trace(go.Bar(
            x=[percent[v]], y=[question], orientation='h', name=f"{v}ì ",
            marker_color=colors[v], text=f"{percent[v]}%", textposition='inside'
        ))
    fig.update_layout(
        barmode='stack', showlegend=False,
        title=question, xaxis_title="ë§¤ìš° ë¶ˆë§Œì¡± â†’ ë§¤ìš° ë§Œì¡±",
        yaxis=dict(showticklabels=False), height=180, margin=dict(t=40,b=2)
    )

    table_df = pd.DataFrame(
        [counts.values, percent.values],
        index=["ì‘ë‹µ ìˆ˜", "ë¹„ìœ¨ (%)"],
        columns=[f"{v}ì " for v in order]
    )
    return fig, table_df

def show_short_answer_keyword_analysis(df_result):
    st.subheader("ğŸ“˜ Q9-DS-4 ë‹¨ë¬¸ ì‘ë‹µ í‚¤ì›Œë“œ ë¶„ì„")
    order = list(KDC_KEYWORD_MAP.keys())
    df_cat = df_result.groupby("ì£¼ì œë²”ì£¼")["í‚¤ì›Œë“œ"].count().reindex(order, fill_value=0).reset_index(name="ë¹ˆë„ìˆ˜")
    fig = px.bar(df_cat, x="ì£¼ì œë²”ì£¼", y="ë¹ˆë„ìˆ˜", title="ì£¼ì œë²”ì£¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„", text="ë¹ˆë„ìˆ˜")
    fig.update_traces(textposition="outside")
    st.plotly_chart(fig, use_container_width=True)
    df_aud = df_result.groupby("ëŒ€ìƒë²”ì£¼")["í‚¤ì›Œë“œ"].count().reset_index(name="ë¹ˆë„ìˆ˜")
    fig2 = px.bar(df_aud, x="ëŒ€ìƒë²”ì£¼", y="ë¹ˆë„ìˆ˜", title="ëŒ€ìƒë²”ì£¼ë³„ í‚¤ì›Œë“œ ë¹ˆë„", text="ë¹ˆë„ìˆ˜", color="ëŒ€ìƒë²”ì£¼")
    fig2.update_traces(textposition="outside")
    st.plotly_chart(fig2, use_container_width=True)
    st.markdown("#### ğŸ” ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”")
    st.dataframe(df_result[["ì‘ë‹µ", "í‚¤ì›Œë“œ", "ì£¼ì œë²”ì£¼", "ëŒ€ìƒë²”ì£¼"]])

def plot_within_category_bar(df, midcategory):
    item_scores = compute_within_category_item_scores(df)
    if midcategory not in item_scores:
        return None, None
    predicate = MIDDLE_CATEGORY_MAPPING[midcategory]
    orig_cols = [c for c in df.columns if predicate(c)]
    if not orig_cols:
        return None, None

    series_plot = item_scores[midcategory].reindex(orig_cols[::-1])
    series_table = item_scores[midcategory].reindex(orig_cols)
    mid_scores = compute_midcategory_scores(df)
    mid_mean = mid_scores.get(midcategory, None)

    wrapped_labels = [wrap_label_fixed(label, width=35) for label in series_plot.index]

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=series_plot.values,
        y=wrapped_labels,
        orientation='h',
        text=series_plot.round(1),
        textposition='outside',
        marker_color='steelblue',
        hovertemplate="<b>%{customdata}</b><br>í‰ê·  ì ìˆ˜: %{x:.1f}<extra></extra>",
        customdata=series_plot.index
    ))
    if mid_mean is not None:
        fig.add_vline(x=mid_mean, line_color="red")

    per_item_height = 50
    total_height = max(300, per_item_height * len(wrapped_labels))

    fig.update_layout(
        title=f"{midcategory} ë‚´ ë¬¸í•­ë³„ í‰ê·  ì ìˆ˜ ë¹„êµ (0~100 í™˜ì‚°)",
        xaxis_title=f"{midcategory} í‰ê·  {mid_mean:.2f}" if mid_mean is not None else "í‰ê·  ì ìˆ˜",
        margin=dict(t=40, b=60),
        height=total_height
    )

    if mid_mean is not None:
        diff = series_table - mid_mean
        table_df = pd.DataFrame({
            'í‰ê·  ì ìˆ˜': series_table.round(2),
            'ì¤‘ë¶„ë¥˜ í‰ê· ': [round(mid_mean,2)] * len(series_table),
            'í¸ì°¨ (ë¬¸í•­ - ì¤‘ë¶„ë¥˜ í‰ê· )': diff.round(2)
        }, index=series_table.index)
    else:
        table_df = pd.DataFrame({
            'í‰ê·  ì ìˆ˜': series_table.round(2)
        }, index=series_table.index)
    return fig, table_df

def plot_dq1(df):
    cols = [c for c in df.columns if c.startswith("DQ1")]
    if not cols:
        return None, None, ""
    question = cols[0]
    data = df[question].dropna().astype(str).str.extract(r"(\d+\.?\d*)")[0]
    monthly = pd.to_numeric(data, errors='coerce')
    yearly = monthly * 12

    def categorize(f):
        try:
            f = float(f)
        except:
            return None
        if f < 12:
            return "0~11íšŒ: ì—° 1íšŒ ë¯¸ë§Œ"
        elif f < 24:
            return "12~23íšŒ: ì›” 1íšŒ ì •ë„"
        elif f < 48:
            return "24~47íšŒ: ì›” 2~4íšŒ ì •ë„"
        elif f < 72:
            return "48~71íšŒ: ì£¼ 1íšŒ ì •ë„"
        elif f < 144:
            return "72~143íšŒ: ì£¼ 2~3íšŒ"
        else:
            return "144íšŒ ì´ìƒ: ê±°ì˜ ë§¤ì¼"

    cat = yearly.apply(categorize)
    order = ["0~11íšŒ: ì—° 1íšŒ ë¯¸ë§Œ","12~23íšŒ: ì›” 1íšŒ ì •ë„","24~47íšŒ: ì›” 2~4íšŒ ì •ë„",
             "48~71íšŒ: ì£¼ 1íšŒ ì •ë„","72~143íšŒ: ì£¼ 2~3íšŒ","144íšŒ ì´ìƒ: ê±°ì˜ ë§¤ì¼"]
    grp = cat.value_counts().reindex(order, fill_value=0)
    pct = (grp/grp.sum()*100).round(1)

    fig = go.Figure(go.Bar(x=grp.index, y=grp.values, text=grp.values,
                            textposition='outside', marker_color=get_qualitative_colors(1)[0]))
    fig.update_layout(title=question, xaxis_title="ì´ìš© ë¹ˆë„ êµ¬ê°„", yaxis_title="ì‘ë‹µ ìˆ˜",
                      bargap=0.2, height=450, margin=dict(t=30,b=50), xaxis_tickangle=-15)

    tbl_df = pd.DataFrame({"ì‘ë‹µ ìˆ˜":grp, "ë¹„ìœ¨ (%)":pct}).T
    return fig, tbl_df, question

def plot_dq2(df):
    cols = [c for c in df.columns if c.startswith("DQ2")]
    if not cols:
        return None, None, ""
    question = cols[0]

    def parse(s):
        s = str(s).strip()
        m = re.match(r'^(\d+)\s*ë…„\s*(\d+)\s*ê°œì›”$', s)
        if m:
            return int(m.group(1)) + (1 if int(m.group(2))>0 else 0)
        m = re.match(r'^(\d+)\s*ë…„$', s)
        if m:
            return int(m.group(1))
        m = re.match(r'^(\d+)\s*ê°œì›”$', s)
        if m:
            return 1
        return None

    yrs = df[question].dropna().apply(parse)
    grp = yrs.value_counts().sort_index()
    pct = (grp/grp.sum()*100).round(1)
    labels = [f"{y}ë…„" for y in grp.index]
    fig = go.Figure(go.Bar(x=labels, y=grp.values, text=grp.values,
                            textposition='outside', marker_color=get_qualitative_colors(1)[0]))
    fig.update_layout(title=question, xaxis_title="ì´ìš© ê¸°ê°„ (ë…„)", yaxis_title="ì‘ë‹µ ìˆ˜",
                      bargap=0.2, height=450, margin=dict(t=30,b=50), xaxis_tickangle=-15)
    tbl_df = pd.DataFrame({"ì‘ë‹µ ìˆ˜":grp, "ë¹„ìœ¨ (%)":pct}).T
    return fig, tbl_df, question

def plot_dq3(df):
    cols = [c for c in df.columns if c.startswith("DQ3")]
    if not cols:
        return None, None, ""
    question = cols[0]
    bar, table_df = plot_categorical_stacked_bar(df[[question]].dropna().astype(str), question)
    return bar, table_df, question

def plot_dq4_bar(df):
    cols = [c for c in df.columns if c.startswith("DQ4")]
    if len(cols) < 2:
        return None, None, ""
    col1, col2 = cols[0], cols[1]
    question = f"{col1} vs {col2}"

    s1 = df[col1].dropna().astype(str)
    s2 = df[col2].dropna().astype(str)
    cats = sorted(set(s1.unique()).union(s2.unique()))
    labels = [c.split('. ',1)[-1] if '. ' in c else c for c in cats]

    counts1 = s1.value_counts().reindex(cats, fill_value=0)
    counts2 = s2.value_counts().reindex(cats, fill_value=0)
    pct1 = (counts1 / counts1.sum() * 100).round(1)
    pct2 = (counts2 / counts2.sum() * 100).round(1)

    order_idx = counts1.sort_values(ascending=False).index.tolist()
    sorted_labels = [lbl.split('. ',1)[-1] if '. ' in lbl else lbl for lbl in order_idx]
    sorted_counts1 = counts1.reindex(order_idx)
    sorted_counts2 = counts2.reindex(order_idx)
    sorted_pct1 = pct1.reindex(order_idx)
    sorted_pct2 = pct2.reindex(order_idx)

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=sorted_labels, y=sorted_counts1.values,
        name='1ìˆœìœ„', marker_color="#1f77b4", text=sorted_counts1.values, textposition='outside'
    ))
    fig.add_trace(go.Bar(
        x=sorted_labels, y=sorted_counts2.values,
        name='2ìˆœìœ„', marker_color="#2ca02c", text=sorted_counts2.values, textposition='outside'
    ))
    fig.update_layout(
        barmode='stack',
        title="DQ4. ë„ì„œê´€ ì´ìš© ì£¼ìš” ëª©ì  1ìˆœìœ„ vs 2ìˆœìœ„",
        xaxis_title="ì´ìš© ëª©ì ",
        yaxis_title="ì‘ë‹µì ìˆ˜",
        height=550,
        margin=dict(t=40, b=10),
        xaxis_tickangle=-23
    )

    table_df = pd.DataFrame({
        '1ìˆœìœ„ ì‘ë‹µ ìˆ˜': sorted_counts1.values,
        '1ìˆœìœ„ ë¹„ìœ¨(%)': sorted_pct1.values,
        '2ìˆœìœ„ ì‘ë‹µ ìˆ˜': sorted_counts2.values,
        '2ìˆœìœ„ ë¹„ìœ¨(%)': sorted_pct2.values
    }, index=sorted_labels).T
    return fig, table_df, question

def plot_dq5(df):
    cols = [c for c in df.columns if c.startswith("DQ5")]
    if not cols:
        return None, None, ""
    question = cols[0]
    temp_df = df[[question]].dropna().astype(str)
    fig, table_df = plot_categorical_stacked_bar(temp_df, question)
    return fig, table_df, question

def plot_likert_diverging(df, prefix="DQ7-E"):
    cols = [c for c in df.columns if c.startswith(prefix)]
    if not cols:
        return None, None
    dist = {}
    for col in cols:
        counts = df[col].dropna().astype(int).value_counts().reindex(range(1,8), fill_value=0)
        pct = (counts / counts.sum() * 100).round(1)
        dist[col] = pct
    likert_df = pd.DataFrame(dist).T
    likert_df = likert_df.reindex(columns=range(1,8))
    table_df = likert_df.copy()
    likert_df = likert_df.reindex(index=likert_df.index[::-1])

    fig = go.Figure()
    neg_scores = [4,3,2,1]
    neg_colors = ["#dddddd","#91bfdb","#4575b4","#313695"]
    for score, color in zip(neg_scores, neg_colors):
        fig.add_trace(go.Bar(
            y=likert_df.index,
            x=-likert_df[score],
            name=f"{score}ì ",
            orientation='h',
            marker_color=color
        ))
    fig.add_trace(go.Bar(
        y=likert_df.index,
        x=likert_df[5],
        name="5ì ",
        orientation='h',
        marker_color="#fee090"
    ))
    for score, color in zip([6,7],["#fc8d59","#d73027"]):
        fig.add_trace(go.Bar(
            y=likert_df.index,
            x=likert_df[score],
            name=f"{score}ì ",
            orientation='h',
            marker_color=color
        ))

    fig.add_vline(
        x=0,
        line_color="black",
        line_width=2,
        line_dash="solid"
    )

    fig.update_layout(
        barmode='relative',
        title="DQ7-E ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„í¬ (ë‹¤ì´ë²„ì§• ë°”)",
        xaxis=dict(visible=False),
        legend=dict(traceorder='normal'),
        height=250,
        margin=dict(t=30, b=5),
    )

    return fig, table_df

def plot_pair_bar(df, prefix):
    cols = [c for c in df.columns if c.startswith(prefix)]
    if len(cols) < 2:
        return None, None, ""
    col1, col2 = cols[0], cols[1]
    question = f"{col1} vs (2ìˆœìœ„)"
    s1 = df[col1].dropna().astype(str)
    s2 = df[col2].dropna().astype(str)
    cats = sorted(set(s1.unique()).union(s2.unique()))
    labels = [c.split('. ',1)[-1] if '. ' in c else c for c in cats]

    counts1 = s1.value_counts().reindex(cats, fill_value=0)
    counts2 = s2.value_counts().reindex(cats, fill_value=0)
    pct1 = (counts1 / counts1.sum() * 100).round(1)
    pct2 = (counts2 / counts2.sum() * 100).round(1)

    fig = go.Figure()
    fig.add_trace(go.Bar(x=labels, y=counts1, name='1ìˆœìœ„', marker_color="#1f77b4", text=counts1, textposition='outside'))
    fig.add_trace(go.Bar(x=labels, y=counts2, name='2ìˆœìœ„', marker_color="#2ca02c", text=counts2, textposition='outside'))
    fig.update_layout(
        barmode='stack',
        title=f"{question}",
        yaxis_title="ì‘ë‹µì ìˆ˜",
        height=550,
        margin=dict(t=50, b=70),
        xaxis_tickangle=-23
    )

    table_df = pd.DataFrame({
        '1ìˆœìœ„ ì‘ë‹µ ìˆ˜': counts1.values,
        '1ìˆœìœ„ ë¹„ìœ¨(%)': pct1.values,
        '2ìˆœìœ„ ì‘ë‹µ ìˆ˜': counts2.values,
        '2ìˆœìœ„ ë¹„ìœ¨(%)': pct2.values
    }, index=labels).T
    return fig, table_df, question

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í˜ì´ì§€ êµ¬ì¡°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def page_home(df):
    st.subheader("ğŸ‘¤ ì¸êµ¬í†µê³„ ë¬¸í•­ (SQ1 ~ 5 / BQ1 ~ 2)")
    soc_qs = [c for c in df.columns if c.startswith("SQ") or c.startswith("BQ")]
    for q in soc_qs:
        try:
            if q.startswith("SQ2"):
                bar, table_df = plot_age_histogram_with_labels(df, q)
            elif q.startswith("BQ2"):
                bar, table_df = plot_bq2_bar(df, q)
            elif q.startswith("SQ4"):
                bar, table_df = plot_sq4_custom_bar(df, q)
            else:
                bar, table_df = plot_categorical_stacked_bar(df, q)
            render_chart_and_table(bar, table_df, q, key_prefix="home")
            st.divider()
        except Exception as e:
            st.error(f"{q} ì—ëŸ¬: {e}")

def page_basic_vis(df):
    st.subheader("ğŸ“ˆ 7ì  ì²™ë„ ë§Œì¡±ë„ ë¬¸í•­ (Q1 ~ Q8)")
    likert_qs = [
        col for col in df.columns
        if re.match(r"Q[1-9][\.-]", str(col))
    ]
    section_mapping = {
        "ê³µê°„ ë° ì´ìš©í¸ì˜ì„±":       [q for q in likert_qs if q.startswith("Q1-")],
        "ì •ë³´ íšë“ ë° í™œìš©":       [q for q in likert_qs if q.startswith("Q2-")],
        "ì†Œí†µ ë° ì •ì±… í™œìš©":       [q for q in likert_qs if q.startswith("Q3-")],
        "ë¬¸í™”Â·êµìœ¡ í–¥ìœ ":         [q for q in likert_qs if q.startswith("Q4-")],
        "ì‚¬íšŒì  ê´€ê³„ í˜•ì„±":       [q for q in likert_qs if q.startswith("Q5-")],
        "ê°œì¸ì˜ ì‚¶ê³¼ ì—­ëŸ‰":       [q for q in likert_qs if q.startswith("Q6-")],
        "ë„ì„œê´€ì˜ ê³µìµì„± ë° ê¸°ì—¬ë„": [
            q for q in likert_qs 
            if q.startswith("Q7-") or q.startswith("Q8")
        ]
    }
    tabs2 = st.tabs(list(section_mapping.keys()))
    for tab, section_name in zip(tabs2, section_mapping.keys()):
        with tab:
           # st.markdown(f"### {section_name}")
            for q in section_mapping[section_name]:
                try:
                    bar, table_df = plot_stacked_bar_with_table(df, q)
                    render_chart_and_table(bar, table_df, q, key_prefix="basic")
                except Exception as e:
                    st.error(f"{q} ì—ëŸ¬: {e}")
            st.divider()

def page_short_keyword(df):
    with st.spinner("ğŸ” GPT ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘..."):
        target_cols = [col for col in df.columns if "Q9-DS-4" in col]
        if not target_cols:
            st.warning("Q9-DS-4 ê´€ë ¨ ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        answers = df[target_cols[0]].dropna().astype(str).tolist()
        df_result = process_answers(answers)
        show_short_answer_keyword_analysis(df_result)

CATEGORY_MAP = {
    "ê³µê°„ ë° ì´ìš©í¸ì˜ì„±": "Q1",
    "ì •ë³´ íšë“ ë° í™œìš©": "Q2",
    "ì†Œí†µ ë° ì •ì±… í™œìš©": "Q3",
    "ë¬¸í™”Â·êµìœ¡ í–¥ìœ ": "Q4",
    "ì‚¬íšŒì  ê´€ê³„ í˜•ì„±": "Q5",
    "ê°œì¸ì˜ ì‚¶ê³¼ ì—­ëŸ‰": "Q6",
}
TYPE_MAP = {
    "A": "ì„œë¹„ìŠ¤ í‰ê°€",
    "B": "ì„œë¹„ìŠ¤ íš¨ê³¼",
    "C": "ì „ë°˜ì  ë§Œì¡±ë„",
}

def get_abc_category_means(df):
    result = []
    for cat, prefix in CATEGORY_MAP.items():
        for t in ["A", "B", "C"]:
            if t == "C":
                cols = [c for c in df.columns if c.startswith(f"{prefix}-C")]
            else:
                cols = [c for c in df.columns if c.startswith(f"{prefix}-{t}-")]
            if not cols:
                mean_val = None
            else:
                vals = df[cols].apply(pd.to_numeric, errors='coerce')
                mean_val = 100 * (vals.mean(axis=1, skipna=True) - 1) / 6
                mean_val = mean_val.mean()
            result.append({
                "ì¤‘ë¶„ë¥˜": cat,
                "ë¬¸í•­ìœ í˜•": TYPE_MAP[t],
                "í‰ê· ê°’": round(mean_val, 2) if mean_val is not None else None
            })
    return pd.DataFrame(result)

def plot_abc_radar(df_mean):
    categories = df_mean['ì¤‘ë¶„ë¥˜'].unique().tolist()
    fig = go.Figure()
    color_map = {
        "ì„œë¹„ìŠ¤ í‰ê°€": "#2ca02c",
        "ì„œë¹„ìŠ¤ íš¨ê³¼": "#1f77b4",
        "ì „ë°˜ì  ë§Œì¡±ë„": "#d62728"
    }
    for t in TYPE_MAP.values():
        vals = df_mean[df_mean['ë¬¸í•­ìœ í˜•'] == t].set_index('ì¤‘ë¶„ë¥˜').reindex(categories)['í‰ê· ê°’'].tolist()
        fig.add_trace(go.Scatterpolar(
            r=vals + [vals[0]],
            theta=categories + [categories[0]],
            fill='none',
            name=t,
            line=dict(color=color_map.get(t, None)),
        ))
    fig.update_layout(
        polar=dict(radialaxis=dict(range=[50, 100])),
        title="ì¤‘ë¶„ë¥˜ë³„ ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) ë ˆì´ë”ì°¨íŠ¸",
        showlegend=True,
        height=450
    )
    return fig

def plot_abc_grouped_bar(df_mean):
    fig = px.bar(
        df_mean,
        x='ì¤‘ë¶„ë¥˜',
        y='í‰ê· ê°’',
        color='ë¬¸í•­ìœ í˜•',
        barmode='group',
        text='í‰ê· ê°’',
        height=450,
        title="ì¤‘ë¶„ë¥˜ë³„ ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) í‰ê· ê°’"
    )
    fig.update_traces(texttemplate='%{text:.1f}', textposition='outside')
    fig.update_yaxes(range=[0,100])
    return fig

def safe_markdown(text, **kwargs):
    # ë§ˆí¬ë‹¤ìš´ í•´ì„ì— ë”°ë¥¸ ì·¨ì†Œì„  ë°©ì§€ (~ â†’ \~ ë˜ëŠ” ëŒ€ì²´)
    escaped = escape_tildes(text, mode="markdown").replace("~~", r"\~\~")
    st.markdown(escaped, **kwargs)

def page_segment_analysis(df):
    st.header("ğŸ§© ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„")
    st.markdown("""
    - SQ1~5, DQ1, DQ2, DQ4(1ìˆœìœ„) ì¤‘ **ìµœëŒ€ 3ê°œ** ë¬¸í•­ ì„ íƒ  
    - ì„ íƒí•œ ë³´ê¸° ì¡°í•©ë³„(ì‘ë‹µì 5ëª… ì´ìƒ)ë¡œ Q1~Q6, Q9-D-3, ê³µìµì„±/ê¸°ì—¬ë„(Q7,Q8) ì¤‘ë¶„ë¥˜ë³„ ë§Œì¡±ë„ í‰ê· ì„ **íˆíŠ¸ë§µ**ìœ¼ë¡œ ë¹„êµ
    """)

    seg_labels = [o["label"] for o in SEGMENT_OPTIONS]
    sel = st.multiselect("ì„¸ê·¸ë¨¼íŠ¸ ì¡°ê±´ (ìµœëŒ€ 3ê°œ)", seg_labels, default=seg_labels[:2], max_selections=3)
    if not sel:
        st.info("ìµœì†Œ 1ê°œ ì´ìƒì„ ì„ íƒí•˜ì„¸ìš”.")
        return
    selected_keys = [o["key"] for o in SEGMENT_OPTIONS if o["label"] in sel]

    df2 = add_derived_columns(df)

    segment_cols = []
    for key in selected_keys:
        segment_cols.extend(get_segment_columns(df2, key))
    segment_cols = list(dict.fromkeys(segment_cols))

    if not segment_cols:
        st.warning("ì„ íƒí•œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    midcat_prefixes = list(MIDCAT_MAP.values())
    analysis_cols = []
    for p in midcat_prefixes:
        if isinstance(p, list):
            for sub_p in p:
                analysis_cols.extend([c for c in df2.columns if c.startswith(sub_p)])
        else:
            analysis_cols.extend([c for c in df2.columns if c.startswith(p)])
    seg_df = df2[segment_cols + analysis_cols].copy()
    seg_df = seg_df.dropna(subset=segment_cols, how='any')
    for c in segment_cols:
        seg_df[c] = seg_df[c].astype(str)

    group = seg_df.groupby(segment_cols, dropna=False)
    counts = group.size().reset_index(name="ì‘ë‹µììˆ˜")
    counts = counts[counts["ì‘ë‹µììˆ˜"] >= 5]
    if counts.empty:
        st.warning("ì‘ë‹µì 5ëª… ì´ìƒì¸ ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    midcats = list(MIDCAT_MAP.keys())
    group_means = []

    for idx, row in counts.iterrows():
        key = tuple(row[c] for c in segment_cols)
        gdf = group.get_group(key)
        means = {}
        for cat, prefix in MIDCAT_MAP.items():
            if isinstance(prefix, list):
                cols = []
                for p in prefix:
                    cols += [c for c in gdf.columns if c.startswith(p)]
            else:
                cols = [c for c in gdf.columns if c.startswith(prefix)]
            if not cols:
                means[cat] = None
                continue
            vals = gdf[cols].apply(pd.to_numeric, errors="coerce")
            mean_val = 100 * (vals.mean(axis=1, skipna=True) - 1) / 6
            means[cat] = round(mean_val.mean(), 2)
        seg_info = {col: row[col] for col in segment_cols}
        seg_info.update(means)
        group_means.append(seg_info)

    group_means = pd.DataFrame(group_means)

    segment_cols_filtered = [
        c for c in segment_cols
        if not (c.startswith("SQ2") and "GROUP" not in c) and c != "DQ2_YEARS"
    ]

    merge_keys = segment_cols_filtered
    counts_merge = counts[merge_keys + ["ì‘ë‹µììˆ˜"]]
    group_means = pd.merge(group_means, counts_merge, how='left', on=merge_keys)

    # ì¤‘ë¶„ë¥˜ í‰ê·  ë° ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨
    group_means["ì¤‘ë¶„ë¥˜í‰ê· "] = group_means[midcats].mean(axis=1).round(2)
    overall_means = group_means[midcats].mean(axis=0)
    overall_mean_of_means = overall_means.mean()
    group_means["ì „ì²´í‰ê· ëŒ€ë¹„í¸ì°¨"] = (group_means["ì¤‘ë¶„ë¥˜í‰ê· "] - overall_mean_of_means).round(2)

    st.markdown("### ì‘ë‹µì ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©ì˜ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ í”„ë¡œíŒŒì¼ ë¹„êµ")
    top_n = 10
    top_df = group_means.nlargest(top_n, "ì‘ë‹µììˆ˜").copy()

    overall_profile = group_means[midcats].mean(axis=0)
    overall_vals = [overall_profile.get(mc, overall_profile.mean()) for mc in midcats]
    overall_closed = overall_vals + [overall_vals[0]]
    cats_closed = midcats + [midcats[0]]

    fig_radar = go.Figure()
    fig_radar.add_trace(go.Scatterpolar(
        r=overall_closed,
        theta=cats_closed,
        fill=None,
        name="ì „ì²´ í‰ê· ",
        line=dict(dash="dash", width=5, color="black"),
        opacity=0.5
    ))

    colors = DEFAULT_PALETTE
    for i, (_, row) in enumerate(top_df.iterrows()):
        combo_label = " | ".join([str(row[c]) for c in segment_cols_filtered])
        vals = [row[mc] if not pd.isna(row[mc]) else overall_profile.get(mc, overall_profile.mean()) for mc in midcats]
        vals_closed = vals + [vals[0]]
        fig_radar.add_trace(go.Scatterpolar(
            r=vals_closed,
            theta=cats_closed,
            fill=None,
            name=f"{combo_label} (n={int(row['ì‘ë‹µììˆ˜'])})",
            hovertemplate="%{theta}: %{r:.1f}<extra></extra>",
            marker=dict(color=colors[i % len(colors)]),
            opacity=0.9
        ))

    fig_radar.update_layout(
        polar=dict(radialaxis=dict(range=[50, 100])),
        title=f"ìƒìœ„ {min(top_n, len(top_df))}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ í”„ë¡œíŒŒì¼ vs ì „ì²´ í‰ê· ",
        height=500,
        showlegend=True,
        legend=dict(orientation="v", y=0.85, x=1.02)
    )

    st.plotly_chart(fig_radar, use_container_width=True)

    # ì¶”ê°€: ë ˆì´ë”ì— ë“¤ì–´ê°„ ì „ì²´ í‰ê·  + ìƒìœ„ ì¡°í•© í”„ë¡œíŒŒì¼ í…Œì´ë¸”
    radar_table_rows = []

    # ì „ì²´ í‰ê·  í–‰
    overall_row = {"ì¡°í•©": "ì „ì²´ í‰ê· "}
    overall_row.update({mc: round(overall_profile.get(mc, 0), 1) for mc in midcats})
    radar_table_rows.append(overall_row)

    # ìƒìœ„ Nê°œ ì¡°í•© í–‰
    for _, row in top_df.iterrows():
        combo_label = " | ".join([str(row[c]) for c in segment_cols_filtered])
        entry = {"ì¡°í•©": combo_label}
        for mc in midcats:
            val = row.get(mc)
            entry[mc] = round(val, 1) if not pd.isna(val) else None
        radar_table_rows.append(entry)

    radar_table_df = pd.DataFrame(radar_table_rows)
    st.markdown("#### ë ˆì´ë” ì°¨íŠ¸ì— ì‚¬ìš©ëœ í”„ë¡œíŒŒì¼ ë°ì´í„°") 
    st.dataframe(_sanitize_dataframe_for_streamlit(radar_table_df))


    # ë£° ê¸°ë°˜ ìš”ì•½
    overall_profile_dict = {mc: overall_profile.get(mc, 0) for mc in midcats}
    high_low_summary = interpret_midcategory_scores(df) if 'interpret_midcategory_scores' in globals() else ""

    # GPT ê¸°ë°˜ í•´ì„ (ë ˆì´ë”)
    combos = []
    for _, row in top_df.iterrows():
        combo_label = " | ".join([str(row[c]) for c in segment_cols_filtered])
        profile = {mc: row.get(mc, overall_profile.get(mc, overall_profile.mean())) for mc in midcats}
        combos.append({"label": combo_label, "n": int(row["ì‘ë‹µììˆ˜"]), "profile": profile})
    prompt = build_radar_prompt(overall_profile_dict, combos)
    insight_text = call_gpt_for_insight(prompt)
    insight_text = insight_text.replace("~", "-")
    render_insight_card("GPT ìƒì„±í˜• í•´ì„", insight_text, key="segment-radar")

    # ì¡°í•©ëª… ìƒì„± ë° delta ê³„ì‚° (ì „ì²´ í‰ê·  ê¸°ì¤€)
    group_means["ì¡°í•©"] = group_means.apply(lambda r: " | ".join([str(r[c]) for c in segment_cols_filtered]), axis=1)
    for mc in midcats:
        group_means[f"{mc}_delta"] = group_means[mc] - overall_means[mc]

    # íˆíŠ¸ë§µ: ì¤‘ë¶„ë¥˜ í‰ê· 
    st.markdown("### íˆíŠ¸ë§µ + ì „ì²´ í‰ê·  ëŒ€ë¹„ ì¤‘ë¶„ë¥˜ë³„ í¸ì°¨ íˆíŠ¸ë§µ")
    heatmap_plot = group_means.set_index("ì¡°í•©")[midcats]
    fig_abs = px.imshow(
        heatmap_plot,
        text_auto=True,
        aspect="auto",
        color_continuous_scale="Blues",
        title="ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©ë³„ ì¤‘ë¶„ë¥˜ í‰ê· ",
        labels=dict(x="ì¤‘ë¶„ë¥˜", y="ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©", color="í‰ê· ì ìˆ˜")
    )
    st.plotly_chart(fig_abs, use_container_width=True)

    st.markdown("#### ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©ë³„ ì¤‘ë¶„ë¥˜ í‰ê·  (íˆíŠ¸ë§µ ê¸°ë°˜ ë°ì´í„°)")
    heatmap_table_display = heatmap_plot.reset_index().rename(columns={"index": "ì¡°í•©"})
    st.dataframe(_sanitize_dataframe_for_streamlit(heatmap_table_display))


    # í”„ë¡¬í”„íŠ¸ìš© í…Œì´ë¸”: ì¡°í•©ëª…ê³¼ ì¤‘ë¶„ë¥˜ ì ìˆ˜ë§Œ ì „ë‹¬
    heatmap_table = group_means[["ì¡°í•©", *midcats]]
    prompt_heat = build_heatmap_prompt(heatmap_table, midcats, label_col="ì¡°í•©")
    heat_insight = call_gpt_for_insight(prompt_heat)
    heat_insight = heat_insight.replace("~", "-")
    render_insight_card("GPT ìƒì„±í˜• í•´ì„ (íˆíŠ¸ë§µ)", heat_insight, key="heatmap-insight")

    # ë¸íƒ€ íˆíŠ¸ë§µ
    delta_plot = group_means.set_index("ì¡°í•©")[[f"{mc}_delta" for mc in midcats]]
    delta_plot.columns = midcats
    fig_delta = px.imshow(
        delta_plot,
        text_auto=True,
        aspect="auto",
        color_continuous_scale="RdBu_r",
        title="ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨ (Delta)",
        labels=dict(x="ì¤‘ë¶„ë¥˜", y="ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•©", color="í¸ì°¨")
    )
    st.plotly_chart(fig_delta, use_container_width=True)

    st.markdown("#### ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨ (Delta) ë°ì´í„°")
    delta_table_display = delta_plot.reset_index().rename(columns={"index": "ì¡°í•©"})
    st.dataframe(_sanitize_dataframe_for_streamlit(delta_table_display))


    delta_summary_parts = []
    for mc in midcats:
        col_delta = f"{mc}_delta"
        if col_delta in group_means:
            top_pos = group_means.nlargest(1, col_delta)
            top_neg = group_means.nsmallest(1, col_delta)
            if not top_pos.empty:
                delta_summary_parts.append(f"{mc}ì—ì„œ ê°€ì¥ ë†’ì€ í¸ì°¨: {top_pos.iloc[0]['ì¡°í•©']} (+{top_pos.iloc[0][col_delta]:.1f})")
            if not top_neg.empty:
                delta_summary_parts.append(f"{mc}ì—ì„œ ê°€ì¥ ë‚®ì€ í¸ì°¨: {top_neg.iloc[0]['ì¡°í•©']} ({top_neg.iloc[0][col_delta]:.1f})")

    delta_df_for_prompt = group_means.set_index("ì¡°í•©")
    prompt_delta = build_delta_prompt(delta_df_for_prompt, midcats)
    delta_insight = call_gpt_for_insight(prompt_delta)
    delta_insight = delta_insight.replace("~", "-")
    render_insight_card("GPT ìƒì„±í˜• í•´ì„ (ë¸íƒ€ íˆíŠ¸ë§µ)", delta_insight, key="delta-heatmap-insight")

#ì‹ ë¢°êµ¬ê°„ í¬í•¨ í¸ì°¨ ë°” ì°¨íŠ¸ í•´ì„

    for mc in midcats[:2]:
        subset = group_means.nlargest(10, "ì‘ë‹µììˆ˜").copy()
        subset["delta"] = subset[mc] - overall_means[mc]
        subset["se"] = np.sqrt((subset[mc] * (100 - subset[mc]) / subset["ì‘ë‹µììˆ˜"]).clip(lower=0))
        fig_ci = go.Figure()
        fig_ci.add_trace(go.Bar(
            x=subset["ì¡°í•©"],
            y=subset["delta"],
            error_y=dict(type="data", array=subset["se"]),
            name=f"{mc} í¸ì°¨"
        ))
        fig_ci.add_hline(y=0, line_dash="dash", line_color="black")
        fig_ci.update_layout(
            title=f"{mc} ì „ì²´ í‰ê·  ëŒ€ë¹„ í¸ì°¨ (ì‹ ë¢°êµ¬ê°„, ìƒìœ„ 10ê°œ ì¡°í•©)",
            yaxis_title="í¸ì°¨",
            height=350,
            margin=dict(t=40, b=60)
        )
        st.plotly_chart(fig_ci, use_container_width=True)
        # ë£° ê¸°ë°˜: 0ì„ ë²—ì–´ë‚˜ëŠ”ì§€ ì²´í¬
        ci_summary = []
        subset_local = subset  # ê¸°ì¡´ ë³€ìˆ˜
        for _, r in subset_local.iterrows():
            combo = r["ì¡°í•©"]
            delta = r["delta"]
            se = r["se"]
            ci_lower = delta - se
            ci_upper = delta + se
            signif = "ìœ ì˜ë¯¸" if not (ci_lower <= 0 <= ci_upper) else "ë¶ˆí™•ì‹¤"
            ci_summary.append(f"{combo}: í¸ì°¨ {delta:.1f}, SE {se:.2f} ({signif})")

# --- ì¶”ê°€: í¸ì°¨ + ì‹ ë¢°êµ¬ê°„ ìƒì„¸ í…Œì´ë¸” ---
        ci_display = subset_local[["ì¡°í•©", mc, "delta", "se"]].copy()
        ci_display["ì‹ ë¢°êµ¬ê°„ í•˜í•œ"] = (ci_display["delta"] - ci_display["se"]).round(2)
        ci_display["ì‹ ë¢°êµ¬ê°„ ìƒí•œ"] = (ci_display["delta"] + ci_display["se"]).round(2)
        ci_display["ìœ ì˜ë¯¸ ì—¬ë¶€"] = ci_display.apply(
            lambda r: "ìœ ì˜ë¯¸" if not (r["ì‹ ë¢°êµ¬ê°„ í•˜í•œ"] <= 0 <= r["ì‹ ë¢°êµ¬ê°„ ìƒí•œ"]) else "ë¶ˆí™•ì‹¤", axis=1
        )
        st.markdown(f"#### '{mc}' í¸ì°¨ + ì‹ ë¢°êµ¬ê°„ ìƒì„¸")
        st.dataframe(_sanitize_dataframe_for_streamlit(ci_display))


        prompt_ci = build_ci_prompt(subset_local, mc)
        ci_insight = call_gpt_for_insight(prompt_ci)
        render_insight_card("GPT ìƒì„±í˜• í•´ì„ (ì‹ ë¢°êµ¬ê°„)", ci_insight, key=f"ci-insight-{mc}")




def show_basic_strategy_insights(df):
    st.subheader("1. ì´ìš© ëª©ì  (DQ4 ê³„ì—´) Ã— ì „ë°˜ ë§Œì¡±ë„ (ì¤‘ë¶„ë¥˜ ê¸°ì¤€ ë ˆì´ë”)")
    purpose_col = None
    for c in df.columns:
        if "DQ4" in c and "1ìˆœìœ„" in c:
            purpose_col = c
            break
    if purpose_col is None:
        for c in df.columns:
            if "DQ4" in c:
                purpose_col = c
                break

    if purpose_col is None:
        st.warning("ì´ìš© ëª©ì  ê´€ë ¨ ì»¬ëŸ¼(DQ4 ê³„ì—´)ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ë°˜ ë§Œì¡±ë„ ëŒ€ë¹„ ë ˆì´ë”ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        overall_mid_scores = compute_midcategory_scores(df)
        if overall_mid_scores.empty:
            st.warning("ì¤‘ë¶„ë¥˜ ì ìˆ˜ ê³„ì‚°ì— í•„ìš”í•œ ë¬¸í•­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        else:
            midcats = list(overall_mid_scores.index)
            purpose_counts = df[purpose_col].dropna().astype(str).value_counts()

            default_n = min(5, len(purpose_counts))
            top_n = st.session_state.get("strategy_radar_top_n_main", default_n)
            top_purposes = purpose_counts.nlargest(top_n).index.tolist()

            fig = go.Figure()
            overall_vals = [overall_mid_scores.get(m, 0) for m in midcats]
            fig.add_trace(go.Scatterpolar(
                r=overall_vals + [overall_vals[0]],
                theta=midcats + [midcats[0]],
                fill=None,
                name="ì „ì²´ í‰ê· ",
                line=dict(dash='dash', width=2),
                opacity=1
            ))

            colors = DEFAULT_PALETTE
            for i, purpose in enumerate(top_purposes):
                subset = df[df[purpose_col].astype(str) == purpose]
                if len(subset) < 5:
                    continue
                purpose_scores = compute_midcategory_scores(subset)
                vals = [purpose_scores.get(m, overall_mid_scores.get(m, 0)) for m in midcats]
                fig.add_trace(go.Scatterpolar(
                    r=vals + [vals[0]],
                    theta=midcats + [midcats[0]],
                    fill=None,
                    name=f"{purpose} (n={int(purpose_counts[purpose])})",
                    hovertemplate="%{theta}: %{r:.1f}<extra></extra>",
                    marker=dict(color=colors[i % len(colors)]),
                    opacity=0.6
                ))

            fig.update_layout(
                polar=dict(radialaxis=dict(range=[50, 100])),
                title=f"ìƒìœ„ {len(top_purposes)}ê°œ ì´ìš© ëª©ì ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ vs ì „ì²´ í‰ê· ",
                height=450,
                legend=dict(orientation="v", x=1.02, y=0.9)
            )
            st.plotly_chart(fig, use_container_width=True)

            top_n = st.number_input(
                "ë ˆì´ë”ì— í‘œì‹œí•  ìƒìœ„ ì´ìš© ëª©ì  ê°œìˆ˜",
                min_value=1,
                max_value=max(1, len(purpose_counts)),
                value=default_n,
                step=1,
                key="strategy_radar_top_n_main"
            )

            top_purposes = purpose_counts.nlargest(top_n).index.tolist()
            summary_rows = []
            for purpose in top_purposes:
                subset = df[df[purpose_col].astype(str) == purpose]
                if len(subset) < 5:
                    continue
                purpose_scores = compute_midcategory_scores(subset)
                row = {"ì´ìš©ëª©ì ": purpose, "ì‘ë‹µììˆ˜": int(purpose_counts[purpose])}
                for m in midcats:
                    row[f"{m} (ëª©ì )"] = round(purpose_scores.get(m, overall_mid_scores.get(m, 0)), 1)
                    row[f"{m} (ì „ì²´ í‰ê· )"] = round(overall_mid_scores.get(m, 0), 1)
                summary_rows.append(row)
            if summary_rows:
                summary_df = pd.DataFrame(summary_rows)
                st.markdown("#### ìƒìœ„ ì´ìš© ëª©ì ë³„ ì¤‘ë¶„ë¥˜ í”„ë¡œíŒŒì¼ ìš”ì•½")
                st.dataframe(summary_df)

    st.subheader("2. ì´ìš© ëª©ì  (DQ4 ê³„ì—´) Ã— ì„¸ë¶€ í•­ëª© íš¨ê³¼ (Q6-B ê³„ì—´)")
    q6b_cols = [c for c in df.columns if c.startswith("Q6-B")]
    if purpose_col is None:
        st.warning("ì´ìš© ëª©ì  ì»¬ëŸ¼ì´ ì—†ì–´ Q6-B ê³„ì—´ íš¨ê³¼ë¥¼ ì´ìš© ëª©ì ë³„ë¡œ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif not q6b_cols:
        st.warning("Q6-B ê³„ì—´ ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        purpose_counts = df[purpose_col].dropna().astype(str).value_counts()
        effect_rows = []
        for purpose in purpose_counts.index:
            subset = df[df[purpose_col].astype(str) == purpose]
            if len(subset) < 5:
                continue
            vals = subset[q6b_cols].apply(pd.to_numeric, errors='coerce')
            scaled = 100 * (vals.mean(axis=1, skipna=True) - 1) / 6
            mean_effect = scaled.mean()
            effect_rows.append({
                "ì´ìš©ëª©ì ": purpose,
                "Q6-B ê³„ì—´ íš¨ê³¼ í‰ê· (0~100)": round(mean_effect, 2),
                "ì‘ë‹µììˆ˜": len(scaled.dropna())
            })
        if effect_rows:
            effect_df = pd.DataFrame(effect_rows).sort_values("Q6-B ê³„ì—´ íš¨ê³¼ í‰ê· (0~100)", ascending=False)
            fig = px.bar(
                effect_df,
                x="ì´ìš©ëª©ì ",
                y="Q6-B ê³„ì—´ íš¨ê³¼ í‰ê· (0~100)",
                text="Q6-B ê³„ì—´ íš¨ê³¼ í‰ê· (0~100)",
                title="ì´ìš© ëª©ì ë³„ Q6-B ê³„ì—´ ì„¸ë¶€ íš¨ê³¼ í‰ê·  ë¹„êµ",
                hover_data=["ì‘ë‹µììˆ˜"]
            )
            fig.update_traces(texttemplate="%{text:.1f}", textposition="outside")
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(effect_df)
        else:
            st.info("ì´ìš© ëª©ì ë³„ë¡œ ì¶©ë¶„í•œ ì‘ë‹µì´ ì—†ì–´ Q6-B íš¨ê³¼ ë¹„êµë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.subheader("3. ì£¼ì´ìš©ì„œë¹„ìŠ¤ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ (SQ5 ê¸°ì¤€)")
    service_col = None
    for candidate in df.columns:
        if "SQ5" in candidate or "ì£¼ë¡œ ì´ìš© ì„œë¹„ìŠ¤" in candidate:
            service_col = candidate
            break
    if service_col is None:
        st.warning("ì£¼ì´ìš©ì„œë¹„ìŠ¤ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ ë¹„êµë¥¼ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        services = df[service_col].dropna().astype(str).unique()
        overall_mid_scores = compute_midcategory_scores(df)
        midcats = list(overall_mid_scores.index)
        plot_data = []
        for service in services:
            subset = df[df[service_col].astype(str) == service]
            if len(subset) < 5:
                continue
            service_scores = compute_midcategory_scores(subset)
            for m in midcats:
                plot_data.append({
                    "ì£¼ì´ìš©ì„œë¹„ìŠ¤": service,
                    "ì¤‘ë¶„ë¥˜": m,
                    "ì„œë¹„ìŠ¤ë³„ ë§Œì¡±ë„": service_scores.get(m, None),
                    "ì „ì²´ í‰ê· ": overall_mid_scores.get(m, None)
                })
        if plot_data:
            plot_df = pd.DataFrame(plot_data)
            fig = px.bar(
                plot_df,
                x="ì¤‘ë¶„ë¥˜",
                y="ì„œë¹„ìŠ¤ë³„ ë§Œì¡±ë„",
                color="ì£¼ì´ìš©ì„œë¹„ìŠ¤",
                barmode="group",
                title="ì£¼ì´ìš©ì„œë¹„ìŠ¤ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ ë¹„êµ",
                text="ì„œë¹„ìŠ¤ë³„ ë§Œì¡±ë„"
            )
            avg_df = plot_df.drop_duplicates(subset=["ì¤‘ë¶„ë¥˜"])[["ì¤‘ë¶„ë¥˜", "ì „ì²´ í‰ê· "]]
            fig.add_trace(go.Scatter(
                x=avg_df["ì¤‘ë¶„ë¥˜"],
                y=avg_df["ì „ì²´ í‰ê· "],
                mode="lines+markers",
                name="ì „ì²´ í‰ê· ",
                line=dict(dash="dash"),
                hovertemplate="%{x}: %{y:.1f}<extra></extra>"
            ))
            for trace in fig.data:
                if getattr(trace, "type", None) == "bar":
                    trace.texttemplate = '%{text:.1f}'
                    trace.textposition = 'outside'
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ì£¼ì´ìš©ì„œë¹„ìŠ¤ë³„ë¡œ ë¹„êµí•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.subheader("4. ë¶ˆì´ìš© ì‚¬ìœ  Ã— ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„")
    reason_col = None
    for candidate in df.columns:
        low = candidate.lower()
        if "ë¶ˆì´ìš©" in candidate or "ì´ìš© ì•ˆí•¨" in low or "ì´ìš©í•˜ì§€" in low or "ì‚¬ìš© ì•ˆí•¨" in low:
            reason_col = candidate
            break
    if reason_col is None:
        st.warning("ë¶ˆì´ìš© ì‚¬ìœ  ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        reasons = df[reason_col].dropna().astype(str).unique()
        rows_exist = False
        for reason in reasons:
            subset = df[df[reason_col].astype(str) == reason]
            if len(subset) < 5:
                continue
            reason_scores = compute_midcategory_scores(subset)
            if reason_scores.empty:
                continue
            rows_exist = True
            plot_df = pd.DataFrame({
                "ì¤‘ë¶„ë¥˜": list(reason_scores.index),
                "ë§Œì¡±ë„": [reason_scores.get(m, None) for m in reason_scores.index]
            })
            fig = px.bar(
                plot_df,
                x="ì¤‘ë¶„ë¥˜",
                y="ë§Œì¡±ë„",
                text="ë§Œì¡±ë„",
                title=f"ë¶ˆì´ìš© ì‚¬ìœ  '{reason}' ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„",
            )
            fig.update_traces(texttemplate="%{text:.1f}", textposition="outside")
            st.plotly_chart(fig, use_container_width=True)
        if not rows_exist:
            st.info("ë¶ˆì´ìš© ì‚¬ìœ ë³„ë¡œ ë¹„êµí•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.subheader("5. ì´ìš© ëª©ì  (DQ4 ê³„ì—´) Ã— ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ (Q7-D-7)")
    time_sat_col = None
    for c in df.columns:
        if c.upper().startswith("Q7-D-7"):
            time_sat_col = c
            break
        if "ìš´ì˜ì‹œê°„" in c or "ì‹œê°„ ë§Œì¡±ë„" in c:
            time_sat_col = c
            break

    if purpose_col is None or time_sat_col is None:
        st.warning("ì´ìš© ëª©ì  ë˜ëŠ” ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        purpose_counts = df[purpose_col].dropna().astype(str).value_counts()
        rows = []
        for purpose in purpose_counts.index:
            subset = df[df[purpose_col].astype(str) == purpose]
            if subset.empty:
                continue
            vals = pd.to_numeric(subset[time_sat_col], errors='coerce').dropna().astype(float)
            if vals.empty:
                continue
            mean_score = vals.mean()
            rows.append({
                "ì´ìš©ëª©ì ": purpose,
                "ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ í‰ê· ": round(mean_score, 2),
                "ì‘ë‹µììˆ˜": len(vals)
            })
        if rows:
            time_df = pd.DataFrame(rows).sort_values("ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ í‰ê· ", ascending=False)
            fig = px.bar(
                time_df,
                x="ì´ìš©ëª©ì ",
                y="ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ í‰ê· ",
                text="ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ í‰ê· ",
                title="ì´ìš© ëª©ì ë³„ ìš´ì˜ì‹œê°„(ê¸°ëŒ€ ëŒ€ë¹„) ë§Œì¡±ë„",
                hover_data=["ì‘ë‹µììˆ˜"]
            )
            fig.update_traces(texttemplate="%{text:.1f}", textposition="outside")
            st.plotly_chart(fig, use_container_width=True)
            st.dataframe(time_df)
        else:
            st.info("ë¹„êµ ê°€ëŠ¥í•œ ì´ìš©ëª©ì ë³„ ìš´ì˜ì‹œê°„ ë§Œì¡±ë„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ì—”íŠ¸ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì—¬ê¸°ë¶€í„° ì¶”ê°€ ----------------------
# í˜ì´ì§€ ìµœìƒë‹¨ì— ê³ ì • í—¤ë” ì‚½ì…
st.markdown(
    """
    <div style="
        position: sticky;
        top: 0;
        background-color: white;
        z-index: 1000;
        padding: 8px 0;
        text-align: center;
        border-bottom: 2px solid #eee;
    ">
        <h1 style="margin: 0; font-size: 3rem; font-weight: bold;">LIBanalysiscusor</h1>
    </div>
    """,
    unsafe_allow_html=True
)
# í˜ì´ì§€ ë‚´ìš©ì´ í—¤ë”ì— ê°€ë ¤ì§€ì§€ ì•Šë„ë¡ ì•½ê°„ì˜ ì—¬ë°± ì‚½ì…
st.markdown("<div style='height: 80px;'></div>", unsafe_allow_html=True)


mode = st.sidebar.radio("LIBanalysiscusor", ["ê¸°ë³¸ ë¶„ì„", "ì‹¬í™” ë¶„ì„", "ì „ëµ ì¸ì‚¬ì´íŠ¸(ê¸°ë³¸)"])

uploaded = st.file_uploader("ğŸ“‚ ì—‘ì…€(.xlsx) íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"])
if not uploaded:
    st.info("ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

try:
    df = pd.read_excel(uploaded)
    st.success("âœ… ì—…ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    st.stop()

if mode == "ê¸°ë³¸ ë¶„ì„":
    tabs = st.tabs([
        "ğŸ‘¤ ì‘ë‹µì ì •ë³´",
        "ğŸ“ˆ ë§Œì¡±ë„ ê¸°ë³¸ ì‹œê°í™”",
        "ğŸ—ºï¸ ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­",
        "ğŸ“Š ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„",
        "ğŸ–¼ï¸ ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„",
        "ğŸ‹ï¸ ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„",
    ])

    with tabs[0]:
        page_home(df)

    with tabs[1]:
        page_basic_vis(df)

    with tabs[2]:
        st.header("ğŸ—ºï¸ ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ ë¶„ì„")
        sub_tabs = st.tabs([
            "7ì  ì²™ë„ ì‹œê°í™”",
            "ë‹¨ë¬¸ ì‘ë‹µ ë¶„ì„",
            "ì¥ë¬¸ ì„œìˆ í˜• ë¶„ì„"
        ])
        with sub_tabs[0]:
            st.subheader("ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ (7ì  ì²™ë„)")
            subregion_cols = [c for c in df.columns if "Q9-D-" in c]
            if not subregion_cols:
                st.error("Q9-D- ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for idx, col in enumerate(subregion_cols):
                    bar, tbl = plot_stacked_bar_with_table(df, col)
                    #st.markdown(f"##### {col}")
                    render_chart_and_table(bar, tbl, col, key_prefix=f"subregion-{idx}")
        with sub_tabs[1]:
            page_short_keyword(df)
        with sub_tabs[2]:
            st.subheader("ì¥ë¬¸ ì„œìˆ í˜• ë¶„ì„ (Q9-DS-5) â€” ì£¼ì œ/ê°ì„± ê¸°ë°˜ ì‹¬ì¸µ")
            long_cols = [c for c in df.columns if "Q9-DS-5" in c]
            if not long_cols:
                st.warning("Q9-DS-5 ê´€ë ¨ ë¬¸í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                raw_answers = df[long_cols[0]].dropna().astype(str).tolist()
                clean_answers = get_clean_long_responses(raw_answers)
                st.markdown(f"ì›ë³¸ ì‘ë‹µ: {len(raw_answers)}ê°œ â†’ ì˜ë¯¸ ìˆëŠ” ì‘ë‹µ: {len(clean_answers)}ê°œ")
                if not clean_answers:
                    st.info("ë¶„ì„í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    if st.button("1. ì£¼ì œ/í‚¤ì›Œë“œ/ìš”ì•½ ì¶”ì¶œ"):
                        with st.spinner("ì£¼ì œ ì¶”ì¶œ ì¤‘..."):
                            theme_df = extract_theme_table_long(clean_answers)
                            st.success("ì£¼ì œ ì¶”ì¶œ ì™„ë£Œ")
                            st.dataframe(theme_df, use_container_width=True)
                            buf = io.BytesIO()
                            theme_df.to_excel(buf, index=False)
                            buf.seek(0)
                            st.download_button(
                                "í‘œ1_ì£¼ì œ_í‚¤ì›Œë“œ_ìš”ì•½.xlsx ë‹¤ìš´ë¡œë“œ",
                                data=buf.getvalue(),
                                file_name="í‘œ1_ì£¼ì œ_í‚¤ì›Œë“œ_ìš”ì•½.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )

                        # ê°ì„± í…Œì´ë¸” ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ìˆ˜ì •ëœ ë¶€ë¶„)
                        with st.spinner("ê°ì„± ë¶„ì„ ì¤‘..."):
                            sentiment_df = extract_sentiment_table_long(clean_answers, theme_df)
                            st.success("ê°ì„± ë¶„ì„ ì™„ë£Œ")
                            st.dataframe(sentiment_df, use_container_width=True)
                            buf2 = io.BytesIO()
                            sentiment_df.to_excel(buf2, index=False)
                            buf2.seek(0)
                            st.download_button(
                                "í‘œ2_ì£¼ì œë³„_ê°ì„±_ìš”ì•½.xlsx ë‹¤ìš´ë¡œë“œ",
                                data=buf2.getvalue(),
                                file_name="í‘œ2_ì£¼ì œë³„_ê°ì„±_ìš”ì•½.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )


    with tabs[3]:
        st.header("ğŸ“Š ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„")
        sub_tabs = st.tabs(["DQ1~5", "DQ6 ê³„ì—´"])
        with sub_tabs[0]:
            fig1, tbl1, q1 = plot_dq1(df)
            render_chart_and_table(fig1, tbl1, q1, key_prefix="dq1")

            fig2, tbl2, q2 = plot_dq2(df)
            render_chart_and_table(fig2, tbl2, q2, key_prefix="dq2")

            fig3, tbl3, q3 = plot_dq3(df)
            render_chart_and_table(fig3, tbl3, q3, key_prefix="dq3")

            fig4, tbl4, q4 = plot_dq4_bar(df)
            render_chart_and_table(fig4, tbl4, q4, key_prefix="dq4")

            fig5, tbl5, q5 = plot_dq5(df)
            render_chart_and_table(fig5, tbl5, q5, key_prefix="dq5")
        with sub_tabs[1]:
            st.subheader("DQ6 ê³„ì—´ ë¬¸í•­ ë¶„ì„")
            dq6_cols = [c for c in df.columns if c.startswith("DQ6")]
            if not dq6_cols:
                st.warning("DQ6 ê³„ì—´ ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for col in dq6_cols:
                    st.markdown(f"### {col}")
                    if col == dq6_cols[0]:
                        multi = df[col].dropna().astype(str).str.split(',')
                        exploded = multi.explode().str.strip()
                        counts = exploded.value_counts()
                        percent = (counts / counts.sum() * 100).round(1)

                        fig = go.Figure(go.Bar(
                            x=counts.values, y=counts.index,
                            orientation='h', text=counts.values,
                            textposition='outside', marker_color=get_qualitative_colors(len(counts))
                        ))
                        fig.update_layout(
                            title=col,
                            xaxis_title="ì‘ë‹µ ìˆ˜",
                            yaxis_title="ì„œë¹„ìŠ¤",
                            height=400,
                            margin=dict(t=50, b=100)
                        )
                        table_df = pd.DataFrame({
                            'ì‘ë‹µ ìˆ˜': counts,
                            'ë¹„ìœ¨ (%)': percent
                        }).T
                        render_chart_and_table(fig, table_df, col, key_prefix="dq6")
                    else:
                        bar, tbl = plot_categorical_stacked_bar(df, col)
                        render_chart_and_table(bar, tbl, col, key_prefix="dq6")

    with tabs[4]:
        st.header("ğŸ–¼ï¸ ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„")
        fig, tbl = plot_likert_diverging(df, prefix="DQ7-E")
        if fig is not None:
            render_chart_and_table(fig, tbl, "DQ7-E ì´ë¯¸ì§€ ë¶„í¬", key_prefix="image-diverge")
        else:
            st.warning("DQ7-E ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    with tabs[5]:
        st.header("ğŸ‹ï¸ ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„")
        fig8, tbl8, q8 = plot_pair_bar(df, "DQ8")
        if fig8 is not None:
            render_chart_and_table(fig8, tbl8, q8, key_prefix="strength")
        else:
            st.warning("DQ8 ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
        fig9, tbl9, q9 = plot_pair_bar(df, "DQ9")
        if fig9 is not None:
            render_chart_and_table(fig9, tbl9, q9, key_prefix="weakness")
        else:
            st.warning("DQ9 ë¬¸í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

elif mode == "ì‹¬í™” ë¶„ì„":
    tabs = st.tabs(["ê³µí†µ ì‹¬í™” ë¶„ì„(ì „ì²´)", "ê³µí†µ ì‹¬í™” ë¶„ì„(ì˜ì—­)", "ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„"])
    with tabs[0]:
        st.header("ğŸ” ê³µí†µ ì‹¬í™” ë¶„ì„(ì „ì²´)")
        st.subheader("ì¤‘ë¶„ë¥˜ë³„ ì „ì²´ ë§Œì¡±ë„ (ë ˆì´ë” ì°¨íŠ¸ ë° í‰ê· ê°’)")
        radar = plot_midcategory_radar(df)
        if radar is not None:
            st.plotly_chart(radar, use_container_width=True)
            tbl_avg = midcategory_avg_table(df)
            if not tbl_avg.empty:
                show_table(tbl_avg, "ì¤‘ë¶„ë¥˜ë³„ í‰ê·  ì ìˆ˜")
                st.markdown("---")
            else:
                st.warning("ì¤‘ë¶„ë¥˜ í‰ê· ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("í•„ìš”í•œ ë¬¸í•­ì´ ì—†ì–´ ì¤‘ë¶„ë¥˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.subheader("ì¤‘ë¶„ë¥˜ ë‚´ ë¬¸í•­ë³„ í¸ì°¨")
        mid_scores = compute_midcategory_scores(df)
        if mid_scores.empty:
            st.warning("ì¤‘ë¶„ë¥˜ ë¬¸í•­ì´ ì—†ì–´ í¸ì°¨ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for mid in mid_scores.index:
                fig, table_df = plot_within_category_bar(df, mid)
                if fig is None:
                    continue
                st.markdown(f"### {mid}")
                st.plotly_chart(fig, use_container_width=True)
                if table_df is not None:
                    show_table(
                        table_df.reset_index().rename(columns={"index": "ë¬¸í•­"}),
                        f"{mid} í•­ëª©ë³„ í¸ì°¨"
                    )
                    st.markdown("---")
        # --- GPT ìš”ì•½ (ì „ì²´) ì¶”ê°€ ---
        overall_mid_scores = compute_midcategory_scores(df)
        within_item_scores = compute_within_category_item_scores(df)  # midcategory -> Series
        abc_df = get_abc_category_means(df)

        prompt_overall = build_common_overall_insight_prompt(
            {k: float(v) for k, v in overall_mid_scores.items()},
            within_item_scores,
            abc_df
        )
        insight_overall = call_gpt_for_insight(prompt_overall)
        insight_overall = insight_overall.replace("~", "-")
        render_insight_card("GPT ê³µí†µ ì‹¬í™” ë¶„ì„ ìš”ì•½ (ì „ì²´)", insight_overall, key="common-overall-insight")

    with tabs[1]:
        st.header("ğŸ” ê³µí†µ ì‹¬í™” ë¶„ì„(ì˜ì—­ë³„ A/B/C ë¹„êµ)")
        df_mean = get_abc_category_means(df)
        radar_fig = plot_abc_radar(df_mean)
        bar_fig = plot_abc_grouped_bar(df_mean)

        st.subheader("ì¤‘ë¶„ë¥˜ë³„ ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) ë ˆì´ë” ì°¨íŠ¸")
        st.plotly_chart(radar_fig, use_container_width=True)

        st.subheader("ì¤‘ë¶„ë¥˜ë³„ ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) ë¬¶ìŒ(bar) ì°¨íŠ¸")
        st.plotly_chart(bar_fig, use_container_width=True)

        st.markdown("#### ìƒì„¸ ë°ì´í„°")
        st.dataframe(df_mean)
        df_mean = get_abc_category_means(df)
        # --- GPT ìš”ì•½ (ì˜ì—­) ì¶”ê°€ ---
        midcat_scores = compute_midcategory_scores(df)
        prompt_area = build_area_insight_prompt(
            {k: float(v) for k, v in midcat_scores.items()},
            df_mean
        )
        area_insight = call_gpt_for_insight(prompt_area)
        area_insight = area_insight.replace("~", "-")
        render_insight_card("GPT ê³µí†µ ì‹¬í™” ë¶„ì„ ìš”ì•½ (ì˜ì—­)", area_insight, key="common-area-insight")


    with tabs[2]:
        page_segment_analysis(df)
        

elif mode == "ì „ëµ ì¸ì‚¬ì´íŠ¸(ê¸°ë³¸)":
    st.header("ğŸ§  ì „ëµ ì¸ì‚¬ì´íŠ¸ (ê¸°ë³¸)")
    show_basic_strategy_insights(df)
#elif mode == "ìì—°ì–´ ì§ˆì˜":
#    st.header("ğŸ—£ï¸ ìì—°ì–´ ì§ˆë¬¸ ê¸°ë°˜ ìë™ ë¶„ì„")
#    st.markdown("ì˜ˆì‹œ: 'í˜¼ì ì´ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì—°ë ¹ëŒ€ ë¶„í¬ ë³´ì—¬ì£¼ê³  ì£¼ë¡œ ê°€ëŠ” ë„ì„œê´€ë³„ ì¤‘ë¶„ë¥˜ ë§Œì¡±ë„ ê°•ì /ì•½ì  ë¹„êµí•´ì¤˜.'")
#    question = st.text_input("ìì—°ì–´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: í˜¼ì ì´ìš©ìë“¤ì˜ ì£¼ ì´ìš© ë„ì„œê´€ë³„ ë§Œì¡±ë„ ë¹„êµí•˜ê³  ê°•ì  ì•½ì  ì•Œë ¤ì¤˜")
#    if question:
#        handle_nl_question_v2(df, question)
