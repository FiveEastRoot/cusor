# =============================================================================
# LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
# =============================================================================
# ì´ íŒŒì¼ì€ ê³µê³µë„ì„œê´€ ì´ìš©ì ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” 
# Streamlit ê¸°ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
# 
# ì£¼ìš” ê¸°ëŠ¥:
# - ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„
# - ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ ë¶„ì„  
# - ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„
# - ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„
# - ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„
# - ê³µí†µ ì‹¬í™” ë¶„ì„
# - AI ê¸°ë°˜ ì „ëµ ì¸ì‚¬ì´íŠ¸ ì œê³µ
# =============================================================================

# =============================================================================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì„í¬íŠ¸
# =============================================================================
import time
import numpy as np
import streamlit as st
import scipy.stats as stats
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import re
import io
import os
from PIL import Image
import openai
import streamlit.components.v1 as components 
import hashlib
import jsonschema
import json 
import math
import logging
from itertools import cycle

# =============================================================================
# 2. ê¸°ë³¸ ì„¤ì • ë° ì´ˆê¸°í™”
# =============================================================================

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# OpenAI API ì„¤ì •
openai.api_key = st.secrets["openai"]["api_key"]
client = openai.OpenAI(api_key=openai.api_key)

# ê¸°ë³¸ ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ì„¤ì •
DEFAULT_PALETTE = px.colors.qualitative.Plotly
COLOR_CYCLER = cycle(DEFAULT_PALETTE)

# =============================================================================
# 3. ìƒìˆ˜ ë° ë§¤í•‘ ì •ì˜
# =============================================================================

# ì¤‘ë¶„ë¥˜ë³„ ì»¬ëŸ¼ ë§¤í•‘ ì •ì˜
MIDDLE_CATEGORY_MAPPING = {
    'ê³µê°„ ë° ì´ìš©í¸ì˜ì„±': lambda col: any(keyword in col for keyword in ['ê³µê°„', 'ì´ìš©í¸ì˜', 'ì ‘ê·¼ì„±', 'ìœ„ì¹˜']),
    'ìë£Œ ë° ì„œë¹„ìŠ¤': lambda col: any(keyword in col for keyword in ['ìë£Œ', 'ë„ì„œ', 'ì„œë¹„ìŠ¤', 'í”„ë¡œê·¸ë¨']),
    'ì§ì› ë° ì•ˆë‚´': lambda col: any(keyword in col for keyword in ['ì§ì›', 'ì•ˆë‚´', 'ë„ì›€', 'ìƒë‹´']),
    'ì‹œì„¤ ë° í™˜ê²½': lambda col: any(keyword in col for keyword in ['ì‹œì„¤', 'í™˜ê²½', 'ì²­ê²°', 'ì¡°ìš©í•¨']),
    'ìš´ì˜ ë° ê´€ë¦¬': lambda col: any(keyword in col for keyword in ['ìš´ì˜', 'ê´€ë¦¬', 'ì‹œê°„', 'ê·œì •'])
}

# KDC ë¶„ë¥˜ í‚¤ì›Œë“œ ë§¤í•‘
KDC_KEYWORD_MAP = {
    '000 ì´ë¥˜': ["ë°±ê³¼ì‚¬ì „", "ë„ì„œê´€", "ë…ì„œ", "ë¬¸í—Œì •ë³´", "ê¸°ë¡", "ì¶œíŒ", "ì„œì§€"],
    '100 ì² í•™': ["ì² í•™", "ëª…ìƒ", "ìœ¤ë¦¬", "ë…¼ë¦¬í•™", "ì‹¬ë¦¬í•™"],
    '200 ì¢…êµ': ["ì¢…êµ", "ê¸°ë…êµ", "ë¶ˆêµ", "ì²œì£¼êµ", "ì‹ í™”", "ì‹ ì•™", "ì¢…êµí•™"],
    '300 ì‚¬íšŒê³¼í•™': ["ì‚¬íšŒ", "ì •ì¹˜", "ê²½ì œ", "ë²•ë¥ ", "í–‰ì •", "êµìœ¡", "ë³µì§€", "ì—¬ì„±", "ë…¸ì¸", "ìœ¡ì•„", "ì•„ë™ë³µì§€", "ì‚¬íšŒë¬¸ì œ", "ë…¸ë™", "í™˜ê²½ë¬¸ì œ", "ì¸ê¶Œ"],
    '400 ìì—°ê³¼í•™': ["ìˆ˜í•™", "ë¬¼ë¦¬", "í™”í•™", "ìƒë¬¼", "ì§€êµ¬ê³¼í•™", "ê³¼í•™", "ì²œë¬¸", "ê¸°í›„", "ì˜í•™", "ìƒëª…ê³¼í•™"],
    '500 ê¸°ìˆ ê³¼í•™': ["ê±´ê°•", "ì˜ë£Œ", "ìš”ë¦¬", "ê°„í˜¸", "ê³µí•™", "ì»´í“¨í„°", "AI", "IT", "ë†ì—…", "ì¶•ì‚°", "ì‚°ì—…", "ê¸°ìˆ ", "ë¯¸ìš©"],
    '600 ì˜ˆìˆ ': ["ë¯¸ìˆ ", "ìŒì•…", "ë¬´ìš©", "ì‚¬ì§„", "ì˜í™”", "ì—°ê·¹", "ë””ìì¸", "ê³µì˜ˆ", "ì˜ˆìˆ ", "ë¬¸í™”ì˜ˆìˆ "],
    '700 ì–¸ì–´': ["ì–¸ì–´", "êµ­ì–´", "ì˜ì–´", "ì¼ë³¸ì–´", "ì¤‘êµ­ì–´", "ì™¸êµ­ì–´", "í•œì", "ë¬¸ë²•"],
    '800 ë¬¸í•™': ["ì†Œì„¤", "ì‹œ", "ìˆ˜í•„", "ì—ì„¸ì´", "í¬ê³¡", "ë¬¸í•™", "ë™í™”", "ì›¹íˆ°", "íŒíƒ€ì§€", "ë¬¸ì˜ˆ"],
    '900 ì—­ì‚¬Â·ì§€ë¦¬': ["ì—­ì‚¬", "ì§€ë¦¬", "í•œêµ­ì‚¬", "ì„¸ê³„ì‚¬", "ì—¬í–‰", "ë¬¸í™”ìœ ì‚°", "ê´€ê´‘"],
    'ì›ì„œ(ì˜ì–´)': ["ì›ì„œ", "ì˜ë¬¸ë„ì„œ", "ì˜ë¬¸íŒ", "ì˜ì–´ì›ì„œ"],
    'ì—°ì†ê°„í–‰ë¬¼': ["ì¡ì§€", "ê°„í–‰ë¬¼", "ì—°ì†ê°„í–‰ë¬¼"],
    'í•´ë‹¹ì—†ìŒ': []
}

# =============================================================================
# 4. AI ë° API ê´€ë ¨ í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ OpenAI APIì™€ ê´€ë ¨ëœ ëª¨ë“  í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.
# API í˜¸ì¶œ, ì¸ì‚¬ì´íŠ¸ ìƒì„±, í…ìŠ¤íŠ¸ ë¶„ì„ ë“±ì˜ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

def safe_chat_completion(*, model="gpt-4.1-nano", messages, temperature=0.2, max_tokens=300, retries=3, backoff_base=1.0):
    """
    OpenAI API í˜¸ì¶œì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    - ì¬ì‹œë„ ë¡œì§ê³¼ ë°±ì˜¤í”„ ì „ëµì„ í¬í•¨í•˜ì—¬ API í˜¸ì¶œ ì‹¤íŒ¨ì— ëŒ€ë¹„
    - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë‚˜ API ì œí•œì— ëŒ€í•œ ë³µì›ë ¥ ì œê³µ
    """
    last_exc = None
    for attempt in range(1, retries + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return resp
        except Exception as e:
            logging.warning(f"OpenAI call failed (attempt {attempt}): {e}")
            last_exc = e
            time.sleep(backoff_base * (2 ** (attempt - 1)))
    logging.error("OpenAI call failed after retries")
    raise last_exc

def call_gpt_for_insight(prompt, model="gpt-4.1-mini", temperature=0.2, max_tokens=1500):
    """
    AI ì¸ì‚¬ì´íŠ¸ ìƒì„±ì„ ìœ„í•œ GPT í˜¸ì¶œ í•¨ìˆ˜
    - ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ëµì  ì¸ì‚¬ì´íŠ¸ ìƒì„±
    - ë„ì„œê´€ ì„œë¹„ìŠ¤ ê°œì„  ë°©ì•ˆ ì œì‹œ
    """
    try:
        response = safe_chat_completion(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"Error calling GPT: {e}")
        return "ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def build_common_overall_insight_prompt(midcat_scores: dict, within_deviations: dict, abc_df: pd.DataFrame) -> str:
    """
    ì „ì²´ ë¶„ì„ì„ ìœ„í•œ AI í”„ë¡¬í”„íŠ¸ ìƒì„±
    - ì¤‘ë¶„ë¥˜ ì ìˆ˜ì™€ í¸ì°¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ì¸ì‚¬ì´íŠ¸ ìš”ì²­
    - A/B/C ê·¸ë£¹ë³„ ë¹„êµ ë¶„ì„ ê²°ê³¼ í¬í•¨
    """
    midcat_str = ", ".join(f"{k} {v:.1f}" for k, v in midcat_scores.items())
    strengths = []
    weaknesses = []
    for mid, series in within_deviations.items():
        if series is None:
            continue
        try:
            dev = series.dropna()
        except Exception:
            continue
        if dev.empty:
            continue
        sorted_series = dev.sort_values(ascending=False)
        top_pos_label = sorted_series.index[0]
        top_pos_val = sorted_series.iloc[0]
        top_neg_label = sorted_series.index[-1]
        top_neg_val = sorted_series.iloc[-1]
        strengths.append(f"{mid}ì˜ '{top_pos_label}' +{top_pos_val:.1f}")
        weaknesses.append(f"{mid}ì˜ '{top_neg_label}' {top_neg_val:.1f}")
    
    try:
        abc_pivot = abc_df.pivot(index="ì¤‘ë¶„ë¥˜", columns="ë¬¸í•­ìœ í˜•", values="í‰ê· ê°’")
    except Exception:
        abc_pivot = pd.DataFrame()
    abc_lines = []
    if not abc_pivot.empty:
        for mid in abc_pivot.index:
            eval_val = abc_pivot.loc[mid].get("ì„œë¹„ìŠ¤ í‰ê°€", None)
            effect_val = abc_pivot.loc[mid].get("ì„œë¹„ìŠ¤ íš¨ê³¼", None)
            sat_val = abc_pivot.loc[mid].get("ì „ë°˜ì  ë§Œì¡±ë„", None)
            eval_str = f"{eval_val:.1f}" if pd.notna(eval_val) else "N/A"
            effect_str = f"{effect_val:.1f}" if pd.notna(effect_val) else "N/A"
            sat_str = f"{sat_val:.1f}" if pd.notna(sat_val) else "N/A"
            abc_lines.append(f"{mid}: í‰ê°€ {eval_str}, íš¨ê³¼ {effect_str}, ë§Œì¡±ë„ {sat_str}")
    abc_str = "\n".join(abc_lines)

    prompt = f"""
ì„¤ë¬¸ ë°ì´í„° ê³µí†µ ì‹¬í™” ë¶„ì„ ìš”ì•½ì„ ë§Œë“¤ì–´ì¤˜.

ì…ë ¥ ìš”ì•½:
- ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê·  ë§Œì¡±ë„: {midcat_str}
- ì¤‘ë¶„ë¥˜ë³„ ê°•ì  ì˜ˆì‹œ: {', '.join(strengths[:3])}
- ì¤‘ë¶„ë¥˜ë³„ ì•½ì  ì˜ˆì‹œ: {', '.join(weaknesses[:3])}
- ì„œë¹„ìŠ¤ í‰ê°€/íš¨ê³¼/ë§Œì¡±ë„ (A/B/C) ë¹„êµ:
{abc_str}

ìš”ì²­:
1. ì£¼ìš” ê´€ì°° íŒ¨í„´ 2~3ê°œë¥¼ ê°„ê²°í•˜ê²Œ ê¸°ìˆ í•´ì¤˜.
2. ì–´ë–¤ ì¤‘ë¶„ë¥˜ê°€ ìƒëŒ€ì  ê°•ì /ì•½ì ì¸ì§€ ìˆ«ìì™€ í•¨ê»˜ ì„¤ëª…í•´ì¤˜.
3. A/B/C ë¹„êµì—ì„œ ë“œëŸ¬ë‚˜ëŠ” íŠ¹ì§•ì„ ì§šì–´ì¤˜.
4. ì „ëµì  ì œì•ˆ 3ê°œ: (1) ìš°ì„  ê°œì…í•  ëŒ€ìƒ, (2) í™•ì¥í•  ê°•ì , (3) ë³´ì™„í•  ì•½ì  ê°ê° êµ¬ì²´ì ìœ¼ë¡œ ì¨ì¤˜.

ì œí•œ: ìˆ«ìëŠ” í•œ ìë¦¬ ì†Œìˆ˜, ë¹„ì¦ˆë‹ˆìŠ¤ í†¤, ì†Œì œëª© í¬í•¨, ì „ì²´ 700~1100ì. ì¶œë ¥ì€ í…ìŠ¤íŠ¸ë§Œ.
"""
    return prompt.strip()

def build_area_insight_prompt(midcat_scores: dict, abc_df: pd.DataFrame) -> str:
    """
    ì˜ì—­ë³„ ë¶„ì„ì„ ìœ„í•œ AI í”„ë¡¬í”„íŠ¸ ìƒì„±
    - íŠ¹ì • ì˜ì—­ì˜ ë§Œì¡±ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìš”ì²­
    - ê·¸ë£¹ë³„ ë¹„êµ ë¶„ì„ ê²°ê³¼ í¬í•¨
    """
    midcat_str = ", ".join(f"{k} {v:.1f}" for k, v in midcat_scores.items())
    abc_markdown = abc_df.to_markdown(index=False)
    
    prompt = f"""
íŠ¹ì • ì˜ì—­ì˜ ë„ì„œê´€ ë§Œì¡±ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ì˜ì—­ë³„ ì ìˆ˜:
{midcat_str}

ê·¸ë£¹ë³„ ë¹„êµ:
{abc_markdown}

í•´ë‹¹ ì˜ì—­ì˜ íŠ¹ì„±ê³¼ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    return prompt

# =============================================================================
# 5. í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬, ë°ì´í„° ì •ì œ, ì‹œê°í™” ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

def remove_parentheses(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê´„í˜¸ì™€ ê·¸ ë‚´ìš©ì„ ì œê±°
    - ì°¨íŠ¸ ë¼ë²¨ ì •ë¦¬ ì‹œ ì‚¬ìš©
    """
    return re.sub(r'\(.*?\)', '', text).strip()

def wrap_label(label, width=10):
    """
    ë¼ë²¨ì„ ì§€ì •ëœ ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆ
    - ì°¨íŠ¸ì˜ ê¸´ ë¼ë²¨ì„ ì—¬ëŸ¬ ì¤„ë¡œ ë¶„í• 
    """
    return '<br>'.join([label[i:i+width] for i in range(0, len(label), width)])

def get_qualitative_colors(n):
    """
    ì§ˆì  ë°ì´í„°ìš© ìƒ‰ìƒ íŒ”ë ˆíŠ¸ ìƒì„±
    - ì°¨íŠ¸ì—ì„œ ì‚¬ìš©í•  ìƒ‰ìƒ ë°°ì—´ ë°˜í™˜
    """
    palette = DEFAULT_PALETTE
    return [c for _, c in zip(range(n), cycle(palette))]

def wrap_label_fixed(label: str, width: int = 35) -> str:
    """
    ë¼ë²¨ì„ ê³ ì • ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆ (HTML íƒœê·¸ ê³ ë ¤)
    - HTML ë Œë”ë§ì„ ê³ ë ¤í•œ ë¼ë²¨ ì¤„ë°”ê¿ˆ
    """
    parts = [label[i:i+width] for i in range(0, len(label), width)]
    return "<br>".join(parts)

def is_trivial(text):
    """
    í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ íŒë‹¨
    - ë¹ˆ ì‘ë‹µì´ë‚˜ ì˜ë¯¸ì—†ëŠ” ì‘ë‹µ í•„í„°ë§
    """
    text = str(text).strip()
    return text in ["", "X", "x", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬", "ì—†ìŒ"]

def split_keywords_simple(text):
    """
    í…ìŠ¤íŠ¸ë¥¼ í‚¤ì›Œë“œë¡œ ë¶„í• 
    - ì‰¼í‘œ, ì , ê³µë°± ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    parts = re.split(r"[.,/\s]+", text)
    return [p.strip() for p in parts if len(p.strip()) > 1]

def map_keyword_to_category(keyword):
    """
    í‚¤ì›Œë“œë¥¼ KDC ì¹´í…Œê³ ë¦¬ë¡œ ë§¤í•‘
    - ë„ì„œ ë¶„ë¥˜ ì²´ê³„ì— ë”°ë¥¸ í‚¤ì›Œë“œ ë¶„ë¥˜
    """
    for cat, kws in KDC_KEYWORD_MAP.items():
        if any(k in keyword for k in kws):
            return cat
    return "í•´ë‹¹ì—†ìŒ"

def escape_tildes(text: str, mode: str = "html") -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‹¸ë“œ(~) ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
    - ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë°©ì§€ë¥¼ ìœ„í•œ ì²˜ë¦¬
    """
    if mode == "html":
        text = text.replace("~~", "&#126;&#126;")
        return text.replace("~", "&#126;")
    else:  # markdown
        text = text.replace("~~", r"\~\~")
        return text.replace("~", r"\~")

def safe_markdown(text, **kwargs):
    """
    ì•ˆì „í•œ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
    - íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ í›„ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
    """
    safe = escape_tildes(text, mode="markdown")
    st.markdown(safe, **kwargs)

def is_meaningful_long(text: str) -> bool:
    """
    ê¸´ í…ìŠ¤íŠ¸ê°€ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ íŒë‹¨
    - ì¥ë¬¸ ì‘ë‹µì˜ í’ˆì§ˆ ê²€ì¦
    """
    if not text or len(text.strip()) < 10:
        return False
    text_clean = text.strip().lower()
    trivial_phrases = ['ì—†ìŒ', 'ëª¨ë¦„', 'í•´ë‹¹ì—†ìŒ', 'íŠ¹ë³„í•œ', 'ì—†ìŠµë‹ˆë‹¤']
    return not any(phrase in text_clean for phrase in trivial_phrases)

def get_clean_long_responses(raw: list[str]) -> list[str]:
    """
    ì˜ë¯¸ìˆëŠ” ê¸´ ì‘ë‹µë§Œ í•„í„°ë§
    - ë¶„ì„ì— ì‚¬ìš©í•  ê³ í’ˆì§ˆ ì¥ë¬¸ ì‘ë‹µ ì¶”ì¶œ
    """
    return [resp for resp in raw if is_meaningful_long(resp)]

# =============================================================================
# 6. ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ ë°ì´í„° ì²˜ë¦¬, í†µê³„ ë¶„ì„, ì ìˆ˜ ê³„ì‚° ë“±ì˜ í•µì‹¬ ë¶„ì„ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

def extract_question_code(col_name: str) -> str:
    """
    ì»¬ëŸ¼ëª…ì—ì„œ ë¬¸í•­ ì½”ë“œ/ë²ˆí˜¸ë§Œ ì¶”ì¶œ
    - 'Q1-1. ê³µê°„ ë§Œì¡±ë„' -> 'Q1-1' í˜•íƒœë¡œ ë³€í™˜
    """
    m = re.match(r'^([A-Za-z0-9]+(?:[-_][A-Za-z0-9]+)*)', col_name.strip())
    if m:
        return m.group(1)
    if '.' in col_name:
        return col_name.split('.', 1)[0].strip()
    if ' ' in col_name:
        return col_name.split(' ', 1)[0].strip()
    return col_name.strip()

def expand_midcategory_to_columns(midcategory: str, df: pd.DataFrame):
    """
    ì¤‘ë¶„ë¥˜ ì´ë¦„ì„ ë°›ì•„ í•´ë‹¹í•˜ëŠ” ì‹¤ì œ ì»¬ëŸ¼ëª… ëª©ë¡ìœ¼ë¡œ í™•ì¥
    - ì¤‘ë¶„ë¥˜ë³„ë¡œ ê´€ë ¨ëœ ëª¨ë“  ë¬¸í•­ ì»¬ëŸ¼ ì¶”ì¶œ
    """
    for mid, predicate in MIDDLE_CATEGORY_MAPPING.items():
        if midcategory.strip().lower() == mid.strip().lower():
            return [c for c in df.columns if predicate(c)]
    return []

def scale_likert(series):
    """
    ë¦¬ì»¤íŠ¸ ì²™ë„ ë°ì´í„°ë¥¼ 0-100 ì ìˆ˜ë¡œ ë³€í™˜
    - í…ìŠ¤íŠ¸ ì‘ë‹µì„ ìˆ«ì ì ìˆ˜ë¡œ ë³€í™˜
    - í†µê³„ ë¶„ì„ì„ ìœ„í•œ í‘œì¤€í™”
    """
    if series.dtype in ['object', 'string']:
        mapping = {
            'ë§¤ìš° ë¶ˆë§Œì¡±': 0, 'ë¶ˆë§Œì¡±': 25, 'ë³´í†µ': 50, 'ë§Œì¡±': 75, 'ë§¤ìš° ë§Œì¡±': 100,
            'ì „í˜€ ê·¸ë ‡ì§€ ì•Šë‹¤': 0, 'ê·¸ë ‡ì§€ ì•Šë‹¤': 25, 'ë³´í†µ': 50, 'ê·¸ë ‡ë‹¤': 75, 'ë§¤ìš° ê·¸ë ‡ë‹¤': 100
        }
        return series.map(mapping).fillna(50)
    else:
        return ((series - series.min()) / (series.max() - series.min()) * 100).fillna(50)

def compute_midcategory_scores(df):
    """
    ì¤‘ë¶„ë¥˜ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
    - ê° ì¤‘ë¶„ë¥˜ì— ì†í•˜ëŠ” ë¬¸í•­ë“¤ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
    - ì „ì²´ ë§Œì¡±ë„ í‰ê°€ì˜ ê¸°ì¤€ì´ ë˜ëŠ” í•µì‹¬ í•¨ìˆ˜
    """
    scores = {}
    for midcat, predicate in MIDDLE_CATEGORY_MAPPING.items():
        cols = [c for c in df.columns if predicate(c)]
        if cols:
            scores[midcat] = df[cols].apply(scale_likert).mean().mean()
    return pd.Series(scores)

def compute_within_category_item_scores(df):
    """
    ì¹´í…Œê³ ë¦¬ ë‚´ ê°œë³„ ë¬¸í•­ ì ìˆ˜ ê³„ì‚°
    - ì¤‘ë¶„ë¥˜ ë‚´ì—ì„œ ê° ë¬¸í•­ë³„ ì„¸ë¶€ ì ìˆ˜ ê³„ì‚°
    - ì„¸ë¶€ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì œê³µ
    """
    scores = {}
    for midcat, predicate in MIDDLE_CATEGORY_MAPPING.items():
        cols = [c for c in df.columns if predicate(c)]
        if cols:
            scores[midcat] = df[cols].apply(scale_likert).mean()
    return scores

def interpret_midcategory_scores(df):
    """
    ì¤‘ë¶„ë¥˜ ì ìˆ˜ë¥¼ í•´ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì˜ë¯¸ìˆëŠ” ì •ë³´ë¥¼ ì œê³µ
    - ì „ì²´ í‰ê·  ëŒ€ë¹„ ë†’ì€/ë‚®ì€ ì¤‘ë¶„ë¥˜ë¥¼ ì‹ë³„
    - ì‚¬ìš©ì ì¹œí™”ì ì¸ í•´ì„ í…ìŠ¤íŠ¸ ìƒì„±
    """
    scores = compute_midcategory_scores(df)
    if scores.empty:
        return "ì¤‘ë¶„ë¥˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    overall = scores.mean()
    high = scores[scores >= overall + 5].index.tolist()
    low = scores[scores <= overall - 5].index.tolist()

    parts = []
    parts.append(f"ì „ì²´ ì¤‘ë¶„ë¥˜ í‰ê· ì€ {overall:.1f}ì ì…ë‹ˆë‹¤.")
    if high:
        parts.append(f"í‰ê· ë³´ë‹¤ ë†’ì€ ì¤‘ë¶„ë¥˜: {', '.join(high)}.")
    if low:
        parts.append(f"í‰ê· ë³´ë‹¤ ë‚®ì€ ì¤‘ë¶„ë¥˜: {', '.join(low)}.")
    if not high and not low:
        parts.append("ëª¨ë“  ì¤‘ë¶„ë¥˜ê°€ ì „ì²´ í‰ê·  ìˆ˜ì¤€ê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.")
    return " ".join(parts)

def cohen_d(x, y):
    """
    Cohen's d íš¨ê³¼ í¬ê¸° ê³„ì‚°
    - ë‘ ê·¸ë£¹ ê°„ì˜ í†µê³„ì  íš¨ê³¼ í¬ê¸° ì¸¡ì •
    - ìœ ì˜ë¯¸í•œ ì°¨ì´ì˜ ì •ë„ë¥¼ ì •ëŸ‰í™”
    """
    nx = len(x)
    ny = len(y)
    dof = nx + ny - 2
    pooled_std = np.sqrt(((nx - 1) * x.var() + (ny - 1) * y.var()) / dof)
    return (x.mean() - y.mean()) / pooled_std

def compare_midcategory_by_group(df, group_col):
    """
    ê·¸ë£¹ë³„ ì¤‘ë¶„ë¥˜ ì ìˆ˜ ë¹„êµ
    - íŠ¹ì • ê·¸ë£¹ ë³€ìˆ˜ì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ë³„ ì ìˆ˜ ì°¨ì´ ë¶„ì„
    - ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§Œì¡±ë„ íŒ¨í„´ íŒŒì•…
    """
    def per_row_mid_scores(subdf):
        return compute_midcategory_scores(subdf)
    
    group_scores = df.groupby(group_col).apply(per_row_mid_scores)
    return group_scores

# =============================================================================
# 7. ì‹œê°í™” ë° í‘œì‹œ í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ ì°¨íŠ¸ ìƒì„±, í…Œì´ë¸” í‘œì‹œ, UI ì»´í¬ë„ŒíŠ¸ ë“±ì˜ ì‹œê°í™” ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

def _sanitize_dataframe_for_streamlit(df: pd.DataFrame) -> pd.DataFrame:
    """
    Streamlit í‘œì‹œë¥¼ ìœ„í•œ ë°ì´í„°í”„ë ˆì„ ì •ë¦¬
    - íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ë°ì´í„° ì •ì œ
    - ì•ˆì „í•œ í‘œì‹œë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
    """
    df_clean = df.copy()
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            df_clean[col] = df_clean[col].astype(str).str.replace('\n', ' ').str.replace('\r', ' ')
    return df_clean

def render_chart_and_table(bar, table, title, key_prefix=""):
    """
    ì°¨íŠ¸ì™€ í…Œì´ë¸”ì„ í•¨ê»˜ ë Œë”ë§
    - ì‹œê°í™”ì™€ ë°ì´í„°ë¥¼ ë‚˜ë€íˆ í‘œì‹œ
    - ì‚¬ìš©ì ê²½í—˜ í–¥ìƒì„ ìœ„í•œ ë ˆì´ì•„ì›ƒ êµ¬ì„±
    """
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.plotly_chart(bar, use_container_width=True)
    
    with col2:
        st.dataframe(table, use_container_width=True)

def show_table(df, caption):
    """
    í…Œì´ë¸” í‘œì‹œ
    - ë°ì´í„°í”„ë ˆì„ì„ ê¹”ë”í•˜ê²Œ í‘œì‹œ
    - ìº¡ì…˜ê³¼ í•¨ê»˜ ì„¤ëª… ì œê³µ
    """
    st.dataframe(df, use_container_width=True)
    if caption:
        st.caption(caption)

def render_insight_card(title: str, content: str, key: str = None):
    """
    ì¸ì‚¬ì´íŠ¸ ì¹´ë“œ ë Œë”ë§
    - AI ìƒì„± ì¸ì‚¬ì´íŠ¸ë¥¼ ì¹´ë“œ í˜•íƒœë¡œ í‘œì‹œ
    - ì‹œê°ì ìœ¼ë¡œ êµ¬ë¶„ëœ ì •ë³´ ì œê³µ
    """
    with st.container():
        st.markdown(f"### {title}")
        st.markdown(content)
        st.markdown("---")

def midcategory_avg_table(df):
    """
    ì¤‘ë¶„ë¥˜ í‰ê·  ì ìˆ˜ í…Œì´ë¸” ìƒì„±
    - ì¤‘ë¶„ë¥˜ë³„ ì ìˆ˜ë¥¼ ì •ë ¬ëœ í…Œì´ë¸”ë¡œ í‘œì‹œ
    - ì„±ê³¼ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„° ì œê³µ
    """
    scores = compute_midcategory_scores(df)
    return pd.DataFrame({
        'ì¤‘ë¶„ë¥˜': scores.index,
        'í‰ê· ì ìˆ˜': scores.values
    }).sort_values('í‰ê· ì ìˆ˜', ascending=False)

def plot_midcategory_radar(df):
    """
    ì¤‘ë¶„ë¥˜ ì ìˆ˜ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±
    - ë‹¤ì°¨ì› ë§Œì¡±ë„ ì ìˆ˜ë¥¼ ë ˆì´ë” ì°¨íŠ¸ë¡œ ì‹œê°í™”
    - ì§ê´€ì ì¸ ì„±ê³¼ ë¹„êµ ê°€ëŠ¥
    """
    scores = compute_midcategory_scores(df)
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=scores.values,
        theta=scores.index,
        fill='toself',
        name='í‰ê·  ì ìˆ˜',
        line_color='blue'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )),
        showlegend=True,
        title="ì¤‘ë¶„ë¥˜ë³„ ë§Œì¡±ë„ ì ìˆ˜"
    )
    
    return fig

# =============================================================================
# 8. í…ìŠ¤íŠ¸ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ í…ìŠ¤íŠ¸ ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì • ë¶„ì„ ë“±ì˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

def process_answers(responses):
    """
    ì‘ë‹µ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    - ì›ì‹œ ì‘ë‹µ ë°ì´í„°ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì •ì œ
    - ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° ë° í‘œì¤€í™”
    """
    processed = []
    for resp in responses:
        if isinstance(resp, str) and resp.strip():
            cleaned = resp.strip().replace('\n', ' ').replace('\r', ' ')
            if len(cleaned) > 3:
                processed.append(cleaned)
    return processed

def extract_keyword_and_audience(responses, batch_size=20):
    """
    í‚¤ì›Œë“œì™€ ëŒ€ìƒ ì¶”ì¶œ
    - AIë¥¼ í™œìš©í•œ ìë™ í‚¤ì›Œë“œ ë° ì´ìš©ì ê·¸ë£¹ ì¶”ì¶œ
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì ì¸ ë¶„ì„ ìˆ˜í–‰
    """
    if not responses:
        return pd.DataFrame(), pd.DataFrame()
    
    batches = [responses[i:i+batch_size] for i in range(0, len(responses), batch_size)]
    
    all_keywords = []
    all_audiences = []
    
    for batch in batches:
        try:
            keyword_prompt = f"ë‹¤ìŒ ì‘ë‹µë“¤ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n{chr(10).join(batch)}"
            keyword_response = safe_chat_completion(
                messages=[{"role": "user", "content": keyword_prompt}],
                max_tokens=500
            )
            
            audience_prompt = f"ë‹¤ìŒ ì‘ë‹µë“¤ì—ì„œ ì–¸ê¸‰ëœ ì´ìš©ì ê·¸ë£¹ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n{chr(10).join(batch)}"
            audience_response = safe_chat_completion(
                messages=[{"role": "user", "content": audience_prompt}],
                max_tokens=500
            )
            
            all_keywords.append(keyword_response.choices[0].message.content)
            all_audiences.append(audience_response.choices[0].message.content)
            
        except Exception as e:
            logging.error(f"Error processing batch: {e}")
            continue
    
    return all_keywords, all_audiences

def make_theme_messages(batch: list[str]) -> list[dict]:
    """
    ì£¼ì œ ë¶„ì„ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„±
    - AI ëª¨ë¸ì— ì „ë‹¬í•  ì£¼ì œ ë¶„ì„ ë©”ì‹œì§€ êµ¬ì„±
    - ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í‘œì¤€í™”
    """
    return [
        {"role": "system", "content": "ë‹¤ìŒ ì‘ë‹µë“¤ì„ ì£¼ì œë³„ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”."},
        {"role": "user", "content": "\n".join(batch)}
    ]

def make_sentiment_messages(batch: list[str], theme_df: pd.DataFrame) -> list[dict]:
    """
    ê°ì • ë¶„ì„ì„ ìœ„í•œ ë©”ì‹œì§€ ìƒì„±
    - ì£¼ì œë³„ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ì • ë¶„ì„ ìˆ˜í–‰
    - ë§¥ë½ì„ ê³ ë ¤í•œ ì •í™•í•œ ê°ì • ë¶„ì„
    """
    return [
        {"role": "system", "content": "ë‹¤ìŒ ì‘ë‹µë“¤ì˜ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."},
        {"role": "user", "content": f"ì£¼ì œë³„ ë¶„ë¥˜:\n{theme_df.to_string()}\n\nì‘ë‹µë“¤:\n" + "\n".join(batch)}
    ]

# =============================================================================
# 9. í˜ì´ì§€ë³„ ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================
# ì´ ì„¹ì…˜ì€ ê° ë¶„ì„ í˜ì´ì§€ì˜ ì£¼ìš” ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.

def page_home(df):
    """
    í™ˆí˜ì´ì§€ - ì‘ë‹µì ì •ë³´ í‘œì‹œ
    - ê¸°ë³¸ í†µê³„ ì •ë³´ ë° ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    - ì „ì²´ì ì¸ ë°ì´í„° í’ˆì§ˆ í™•ì¸
    """
    st.header("ğŸ‘¤ ì‘ë‹µì ì •ë³´")
    
    # ê¸°ë³¸ í†µê³„ ì •ë³´
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ì‘ë‹µì ìˆ˜", len(df))
    
    with col2:
        st.metric("ì™„ì „í•œ ì‘ë‹µ", len(df.dropna()))
    
    with col3:
        st.metric("ì‘ë‹µë¥ ", f"{len(df.dropna()) / len(df) * 100:.1f}%")
    
    with col4:
        st.metric("ì»¬ëŸ¼ ìˆ˜", len(df.columns))
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    st.subheader("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    st.dataframe(df.head(), use_container_width=True)

def page_basic_vis(df):
    """
    ê¸°ë³¸ ì‹œê°í™” í˜ì´ì§€
    - ì¤‘ë¶„ë¥˜ë³„ ë§Œì¡±ë„ ì ìˆ˜ ì‹œê°í™”
    - ë ˆì´ë” ì°¨íŠ¸ì™€ í…Œì´ë¸”ì„ í†µí•œ ì¢…í•© ë¶„ì„
    """
    st.header("ğŸ“ˆ ë§Œì¡±ë„ ê¸°ë³¸ ì‹œê°í™”")
    
    # ì¤‘ë¶„ë¥˜ë³„ í‰ê·  ì ìˆ˜
    st.subheader("ì¤‘ë¶„ë¥˜ë³„ í‰ê·  ë§Œì¡±ë„")
    scores = compute_midcategory_scores(df)
    if not scores.empty:
        fig = plot_midcategory_radar(df)
        st.plotly_chart(fig, use_container_width=True)
        
        # í…Œì´ë¸” í‘œì‹œ
        avg_table = midcategory_avg_table(df)
        st.dataframe(avg_table, use_container_width=True)

def page_short_keyword(df):
    """
    ë‹¨ë¬¸ ì‘ë‹µ í‚¤ì›Œë“œ ë¶„ì„ í˜ì´ì§€
    - ì§§ì€ ì‘ë‹µì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶„ì„
    - ì´ìš©ì ì˜ê²¬ì˜ ì£¼ìš” í‚¤ì›Œë“œ íŒŒì•…
    """
    st.subheader("ë‹¨ë¬¸ ì‘ë‹µ í‚¤ì›Œë“œ ë¶„ì„")
    
    # í‚¤ì›Œë“œ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("í‚¤ì›Œë“œ ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_segment_analysis(df):
    """
    ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„ í˜ì´ì§€
    - ë‹¤ì–‘í•œ ì´ìš©ì ê·¸ë£¹ì˜ íŠ¹ì„± ë¶„ì„
    - ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§Œì¡±ë„ íŒ¨í„´ ë¹„êµ
    """
    st.header("ğŸ§© ì´ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ì¡°í•© ë¶„ì„")
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_district_analysis(df):
    """
    ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ ë¶„ì„ í˜ì´ì§€
    - ì§€ì—­ë³„ ë„ì„œê´€ ì´ìš© í˜„í™© ë¶„ì„
    - ìì¹˜êµ¬ë³„ ì„œë¹„ìŠ¤ í’ˆì§ˆ ë¹„êµ
    """
    st.header("ğŸ—ºï¸ ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­ ë¶„ì„")
    
    # ìì¹˜êµ¬ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ìì¹˜êµ¬ ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_usage_pattern_analysis(df):
    """
    ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„ í˜ì´ì§€
    - ì´ìš© íŒ¨í„´ ë° í–‰ë™ ë¶„ì„
    - ì´ìš© ëª©ì ë³„ ë§Œì¡±ë„ ë¶„ì„
    """
    st.header("ğŸ“Š ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„")
    
    # ì´ìš©ì–‘íƒœ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ì´ìš©ì–‘íƒœ ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_image_analysis(df):
    """
    ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„ í˜ì´ì§€
    - ì´ìš©ì ì¸ì‹ ë° ë§Œì¡±ë„ ë¶„ì„
    - ë¸Œëœë“œ ì´ë¯¸ì§€ í‰ê°€
    """
    st.header("ğŸ–¼ï¸ ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„")
    
    # ì´ë¯¸ì§€ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_strength_weakness_analysis(df):
    """
    ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„ í˜ì´ì§€
    - SWOT ë¶„ì„ ë° ê°œì„ ì  ë„ì¶œ
    - ê²½ìŸë ¥ ë¶„ì„
    """
    st.header("ğŸ‹ï¸ ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„")
    
    # ê°•ì•½ì  ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ê°•ì•½ì  ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_common_analysis(df):
    """
    ê³µí†µ ì‹¬í™” ë¶„ì„ í˜ì´ì§€
    - ì „ì²´ ë° ì˜ì—­ë³„ ìƒì„¸ ë¶„ì„
    - ì¢…í•©ì ì¸ ì„±ê³¼ í‰ê°€
    """
    st.header("ğŸ” ê³µí†µ ì‹¬í™” ë¶„ì„")
    
    # ê³µí†µ ë¶„ì„ ë¡œì§ êµ¬í˜„
    st.info("ê³µí†µ ì‹¬í™” ë¶„ì„ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

def page_strategy_insights(df):
    """
    ì „ëµ ì¸ì‚¬ì´íŠ¸ í˜ì´ì§€
    - AI ê¸°ë°˜ ì „ëµì  ì œì•ˆ
    - ê°œì„  ë°©ì•ˆ ë° ìš°ì„ ìˆœìœ„ ì œì‹œ
    """
    st.header("ğŸ§  ì „ëµ ì¸ì‚¬ì´íŠ¸")
    
    # ì „ëµ ì¸ì‚¬ì´íŠ¸ ë¡œì§ êµ¬í˜„
    st.info("ì „ëµ ì¸ì‚¬ì´íŠ¸ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.")

# =============================================================================
# 10. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ë¶€ë¶„
# =============================================================================
# ì´ ì„¹ì…˜ì€ Streamlit ì•±ì˜ ë©”ì¸ ì‹¤í–‰ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
# í˜ì´ì§€ ì„¤ì •, ë„¤ë¹„ê²Œì´ì…˜, íŒŒì¼ ì—…ë¡œë“œ, ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ë“±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ê³ ì • í—¤ë”
st.markdown(
    """
    <div style="
        position: sticky;
        top: 0;
        background-color: white;
        z-index: 1000;
        padding: 1rem 0;
        border-bottom: 2px solid #e0e0e0;
        margin-bottom: 2rem;
    ">
        <h1 style="text-align: center; color: #2c3e50; margin: 0;">
            ğŸ“š LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
        </h1>
    </div>
    """,
    unsafe_allow_html=True
)

# ì‚¬ì´ë“œë°” ë„¤ë¹„ê²Œì´ì…˜
st.sidebar.title("ğŸ“Š ë¶„ì„ ë©”ë‰´")
analysis_mode = st.sidebar.radio(
    "ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:",
    [
        "ê¸°ë³¸ ë¶„ì„",
        "ì‹¬í™” ë¶„ì„", 
        "ì „ëµ ì¸ì‚¬ì´íŠ¸(ê¸°ë³¸)"
    ]
)

# íŒŒì¼ ì—…ë¡œë“œ
uploaded = st.file_uploader("ğŸ“‚ ì—‘ì…€(.xlsx) íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"])
if not uploaded:
    st.info("ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.")
    st.stop()

try:
    df = pd.read_excel(uploaded)
    st.success("âœ… ì—…ë¡œë“œ ì™„ë£Œ")
except Exception as e:
    st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    st.stop()

# ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
if analysis_mode == "ê¸°ë³¸ ë¶„ì„":
    tabs = st.tabs([
        "ğŸ‘¤ ì‘ë‹µì ì •ë³´",
        "ğŸ“ˆ ë§Œì¡±ë„ ê¸°ë³¸ ì‹œê°í™”",
        "ğŸ—ºï¸ ìì¹˜êµ¬ êµ¬ì„± ë¬¸í•­",
        "ğŸ“Š ë„ì„œê´€ ì´ìš©ì–‘íƒœ ë¶„ì„",
        "ğŸ–¼ï¸ ë„ì„œê´€ ì´ë¯¸ì§€ ë¶„ì„",
        "ğŸ‹ï¸ ë„ì„œê´€ ê°•ì•½ì  ë¶„ì„",
    ])

    with tabs[0]:
        page_home(df)

    with tabs[1]:
        page_basic_vis(df)

    with tabs[2]:
        page_district_analysis(df)

    with tabs[3]:
        page_usage_pattern_analysis(df)

    with tabs[4]:
        page_image_analysis(df)

    with tabs[5]:
        page_strength_weakness_analysis(df)

elif analysis_mode == "ì‹¬í™” ë¶„ì„":
    page_common_analysis(df)

elif analysis_mode == "ì „ëµ ì¸ì‚¬ì´íŠ¸(ê¸°ë³¸)":
    page_strategy_insights(df)

# í‘¸í„°
st.markdown("---")
st.markdown(
    """
    <div style="text-align: center; color: #7f8c8d; padding: 2rem 0;">
        <p>ğŸ“š LIBanalysiscusor - ê³µê³µë„ì„œê´€ ì„¤ë¬¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ</p>
        <p>ë°ì´í„° ê¸°ë°˜ ë„ì„œê´€ ì„œë¹„ìŠ¤ ê°œì„ ì„ ìœ„í•œ ë¶„ì„ ë„êµ¬</p>
    </div>
    """,
    unsafe_allow_html=True
)
